{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Asynchronous Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking vs Non-Blocking code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Blocking Code Example\n",
    "\n",
    "In blocking code, each task waits for the previous one to finish before starting.\n",
    "\n",
    "```python\n",
    "# Blocking code example with multiple tasks\n",
    "import time\n",
    "\n",
    "# Simulate a blocking task\n",
    "def read_file(file_name):\n",
    "    print(f\"Reading file: {file_name}\")\n",
    "    time.sleep(2)  # Simulate a time delay for reading a file (2 seconds)\n",
    "    print(f\"Finished reading {file_name}\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate multiple blocking tasks\n",
    "    read_file(\"file1.txt\")\n",
    "    read_file(\"file2.txt\")\n",
    "    read_file(\"file3.txt\")\n",
    "    \n",
    "    print(f\"All tasks finished in {time.time() - start_time} seconds.\")\n",
    "\n",
    "main()\n",
    "```\n",
    "---\n",
    "\n",
    "#### 2. Non-blocking Code Example\n",
    "\n",
    "In non-blocking code, the program can start a new task without waiting for the previous one to finish. This is achieved using asynchronous programming.\n",
    "\n",
    "```python\n",
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "# Non-blocking task using asyncio\n",
    "async def read_file(file_name):\n",
    "    print(f\"Reading file: {file_name}\")\n",
    "    await asyncio.sleep(2)  # Simulate a time delay without blocking\n",
    "    print(f\"Finished reading {file_name}\")\n",
    "\n",
    "# Main execution\n",
    "async def main():\n",
    "    start_time = asyncio.get_event_loop().time()\n",
    "    \n",
    "    # Start multiple tasks concurrently\n",
    "    tasks = [\n",
    "        read_file(\"file1.txt\"),\n",
    "        read_file(\"file2.txt\"),\n",
    "        read_file(\"file3.txt\")\n",
    "    ]\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(f\"All tasks finished in {asyncio.get_event_loop().time() - start_time} seconds.\")\n",
    "\n",
    "# Run the async program\n",
    "asyncio.run(main())\n",
    "```\n",
    "---\n",
    "**Situation:**  \n",
    "You have 10 kilograms of dirty laundry, and you went to a self-service laundry. The machines can each hold up to 3 kilograms of clothes, and each machine takes 1 hour to finish a load. If you use just one machine, it will take 4 hours to finish all your laundry. How can you speed up the process?\n",
    "\n",
    "**Situation:** You have a date in one hour, and you want to impress your crush with a fresh haircut. However, you also need to do laundry because you donâ€™t have any clean clothes to wear for work tomorrow. How would you handle this situation?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asynchronous web scraping** allows you to send multiple HTTP requests concurrently without blocking the execution of the program. This is ideal for I/O-bound tasks like scraping many web pages, as it enables the program to process multiple requests at once, reducing total scraping time.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Event Loop**: Manages asynchronous tasks, allowing one task to run while others wait.\n",
    "- **Non-blocking I/O**: HTTP requests don't block the program; it continues to send more requests or process other tasks while waiting for responses.\n",
    "- **Coroutines**: Functions defined with `async def` that can be paused and resumed.\n",
    "- **`await`**: Pauses a coroutine until a result is available, such as the response from an HTTP request.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "```python\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(url):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    urls = [\"http://example.com/page1\", \"http://example.com/page2\"]\n",
    "    tasks = [fetch(url) for url in urls]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for result in results:\n",
    "        print(f\"Fetched {len(result)} characters\")\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Efficiency**: Scrapes multiple pages concurrently, reducing total time.\n",
    "2. **Low Resource Usage**: Uses a single thread, consuming less memory and CPU.\n",
    "3. **Scalability**: Handles large volumes of data without the overhead of multithreading.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "- **Rate Limiting**: Respect website rate limits to avoid getting blocked.\n",
    "- **Error Handling**: Ensure proper handling for failed requests and timeouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(greet) : <class 'function'>\n",
      "type(async_greet) : <class 'function'>\n",
      "Hello, World!\n",
      "type(greet()) : <class 'NoneType'>\n",
      "<class 'coroutine'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudic\\AppData\\Local\\Temp\\ipykernel_6000\\588810207.py:22: RuntimeWarning: coroutine 'async_greet' was never awaited\n",
      "  print(type(async_greet()))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Defining an Async Function\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "def greet():\n",
    "    time.sleep(1)\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "async def async_greet():\n",
    "    # await asyncio.sleep(1)\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "print(f\"type(greet) : {type(greet)}\")\n",
    "# TODO: Print the type of async_greet()\n",
    "print(f\"type(async_greet) : {type(async_greet)}\")\n",
    "\n",
    "print(f\"type(greet()) : {type(greet())}\")\n",
    "# TODO: Print the type of returned value of async_greet()\n",
    "print(type(async_greet()))\n",
    "\n",
    "# RuntimeWarning: coroutine 'async_greet' was never awaited\n",
    "#   print(type(async_greet()))\n",
    "# RuntimeWarning: Enable tracemalloc to get the object allocation traceback ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal function call result: <coroutine object add_numbers at 0x000001E250F5DA80>\n",
      "asyncio.run() result: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rudic\\AppData\\Local\\Temp\\ipykernel_22400\\327975533.py:17: RuntimeWarning: coroutine 'add_numbers' was never awaited\n",
      "  add_numbers(1, 2)\n",
      "Object allocated at (most recent call last):\n",
      "  File \"C:\\Users\\rudic\\AppData\\Local\\Temp\\ipykernel_22400\\327975533.py\", lineno 17\n",
      "    add_numbers(1, 2)\n",
      "C:\\Users\\rudic\\AppData\\Local\\Temp\\ipykernel_22400\\327975533.py:21: RuntimeWarning: coroutine 'add_numbers' was never awaited\n",
      "  result1 = add_numbers(1, 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Executing async function\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import tracemalloc\n",
    "\n",
    "# Enable tracemalloc for better debugging\n",
    "tracemalloc.start()\n",
    "\n",
    "# Enable nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "add_numbers(1, 2)\n",
    "asyncio.run(add_numbers(1, 2))\n",
    "\n",
    "# TODO: Try to execute add_numbers like a normal function\n",
    "result1 = add_numbers(1, 2)\n",
    "print(f\"Normal function call result: {result1}\")\n",
    "# TODO: Try to execute add_numbers using asyncio.run()\n",
    "result2 = asyncio.run(add_numbers(1, 2))\n",
    "print(f\"asyncio.run() result: {result2}\")\n",
    "\n",
    "#Notes : This will allow asyncio.run() to work properly in Jupyter notebooks by enabling nested event loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Executing async function\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main():\n",
    "    result = None\n",
    "    # TODO: Change result value by executing add_numbers\n",
    "    result = asyncio.run(add_numbers(1, 2))\n",
    "\n",
    "    print(result)\n",
    "\n",
    "# TODO: Execute main() function\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Executing async function\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "async def main():\n",
    "    result = None\n",
    "    # TODO: Change result value by await add_numbers\n",
    "    result = await add_numbers(1, 2)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "# TODO: Execute main() function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Cannot divide by zero!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Error Handling in Async Functions\n",
    "\"\"\"\n",
    "async def divide_numbers(a, b):\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero!\")\n",
    "    return a / b\n",
    "\n",
    "# TODO: Create main() function to execute divide_numbers(10, 0) asynchronously\n",
    "# TODO: Add error handling if b is zero\n",
    "# TODO: Execute main() function\n",
    "\n",
    "async def main():\n",
    "    try:\n",
    "        result = await divide_numbers(10, 0)\n",
    "        print(f\"Result: {result}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Running Multiple Tasks\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def task_1():\n",
    "    print(\"Task 1 started...\")\n",
    "    await asyncio.sleep(2)\n",
    "    print(\"Task 1 completed!\")\n",
    "    return \"Result from Task 1\"\n",
    "\n",
    "# TODO: Create 2 more function like above\n",
    "# TODO: Create main function to execute all task function asynchronously\n",
    "# TODO: Execute main function\n",
    "# TODO: Analyze the flow execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Running Multiple Tasks Concurrently\n",
    "\"\"\"\n",
    "# TODO: Change the main function using asyncio.gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Another way to gather many tasks at once\n",
    "Using *args\n",
    "\"\"\"\n",
    "# TODO: Improve the previous code using *args instead of calling one by one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Using asyncio.create_task()\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Create tasks\n",
    "    t1 = asyncio.create_task(task_1())\n",
    "    # TODO: Create task for Task 2\n",
    "    # TODO: Create task for Task 3\n",
    "    \n",
    "    await t1  # Wait for Task 1 to finish\n",
    "    # TODO: Wait for Task 2 to finish\n",
    "    # TODO: Wait for Task 3 to finish\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Combining create_task() and gather()\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "\n",
    "# Simulated web scraping task\n",
    "async def scrape_page(url):\n",
    "    print(f\"Starting {url}\")\n",
    "    await asyncio.sleep(1)  # Simulate network delay\n",
    "    print(f\"Scraped: {url}\")\n",
    "    return {\"url\": url, \"content\": f\"Dummy content from {url}\"}\n",
    "\n",
    "# Main function\n",
    "async def main():\n",
    "    urls = [\n",
    "        \"http://example.com\",\n",
    "        \"http://example.org\",\n",
    "        \"http://example.net\"\n",
    "    ]\n",
    "    \n",
    "    # Simulate creating tasks for scraping pages\n",
    "    tasks = []\n",
    "    # TODO: Create a list of task by combining scrape_page and urls list\n",
    "    \n",
    "    # Wait for all tasks to complete and gather results\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    # TODO: print the result here\n",
    "\n",
    "# TODO: Run the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Using asyncio.as_completed for Immediate Results\n",
    "\"\"\"\n",
    "# TODO: Import necessary package\n",
    "# TODO: Create a task function that accept 2 parameters: task name, delay time\n",
    "# TODO: Create a main function to create a list of task\n",
    "# TODO: Loop coroutine object inside asyncio.as_completed(list of task)\n",
    "# TODO: Wait the coroutine object to get the result and print it\n",
    "# TODO: Execute the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Simulating web scraping process\n",
    "\"\"\"\n",
    "import asyncio\n",
    "\n",
    "async def fetch_data(url):\n",
    "    await asyncio.sleep(1)  # Simulate fetch\n",
    "    print(f\"Fetched: {url}\")\n",
    "    return f\"Data from {url}\"\n",
    "\n",
    "async def process_data(data):\n",
    "    await asyncio.sleep(0.5)  # Simulate processing\n",
    "    print(f\"Processed: {data}\")\n",
    "    return f\"Processed {data}\"\n",
    "\n",
    "async def save_data(data):\n",
    "    await asyncio.sleep(0.5)  # Simulate saving\n",
    "    print(f\"Saved: {data}\")\n",
    "\n",
    "async def main():\n",
    "    urls = [\"http://example.com\", \"http://example.org\", \"http://example.net\"]\n",
    "    \n",
    "    # TODO: Fetch all data concurrently\n",
    "    # TODO: Process data concurrently\n",
    "    # TODO: Save all data concurrently\n",
    "\n",
    "# TODO: Run the workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTPX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "httpx is a modern Python library designed for making HTTP requests. It supports both synchronous and asynchronous programming and offers advanced features like HTTP/2 and connection pooling. Itâ€™s often described as an async-friendly alternative to requests with a similar API.\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "pip install httpx\n",
    "```\n",
    "---\n",
    "\n",
    "**Basic Usage**\n",
    "**Synchronous Request**\n",
    "```python\n",
    "import httpx\n",
    "\n",
    "response = httpx.get('https://example.com')\n",
    "print(response.status_code)\n",
    "print(response.text)\n",
    "```\n",
    "---\n",
    "\n",
    "### **When to Use `httpx`**\n",
    "- **Web Scraping**: Make multiple requests concurrently with async support.\n",
    "- **APIs**: Communicate with RESTful or GraphQL APIs using efficient HTTP/2.\n",
    "- **Proxies**: Handle requests via proxy servers with ease.\n",
    "- **Modern HTTP Features**: Use advanced features like HTTP/2 and custom middleware.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Sending a simple HTTP request using httpx\n",
    "\"\"\"\n",
    "import httpx\n",
    "\n",
    "r = httpx.get('https://httpbin.org/get')\n",
    "# TODO: Try to manipulate the r object above as you are using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Objective: Sending HTTP request using httpx client\n",
    "httpx.Client() is what you can use instead of requests.Session()\n",
    "\"\"\"\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Send the first request\n",
    "response_1 = httpx.get(\"https://httpbin.org/cookies/set?cookie_name=cookie_value\", follow_redirects=True)\n",
    "print(\"First Request (Set Cookie):\", response_1.json())\n",
    "\n",
    "# Send a second request to check cookies\n",
    "response_2 = httpx.get(\"https://httpbin.org/cookies\")\n",
    "print(\"Second Request (No Session):\", response_2.json())\n",
    "\n",
    "# TODO: Send a third request to check cookies\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total execution time {end_time - start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Request (Set Cookie): {'cookies': {'cookie_name': 'cookie_value'}}\n",
      "Second Request (With Session): {'cookies': {'cookie_name': 'cookie_value'}}\n",
      "Third Request (With Session): {'cookies': {'cookie_name': 'cookie_value'}}\n",
      "Total execution time 3.34\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Sending HTTP request using httpx client\n",
    "httpx.Client() is what you can use instead of requests.Session()\n",
    "\"\"\"\n",
    "# TODO: Improve code above by using httpx client\n",
    "# TODO: Analyze the difference\n",
    "\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Using httpx.Client() to maintain session\n",
    "with httpx.Client(follow_redirects=True) as client:\n",
    "    # Send the first request\n",
    "    response_1 = client.get(\"https://httpbin.org/cookies/set?cookie_name=cookie_value\")\n",
    "    print(\"First Request (Set Cookie):\", response_1.json())\n",
    "\n",
    "    # Send a second request to check cookies\n",
    "    response_2 = client.get(\"https://httpbin.org/cookies\")\n",
    "    print(\"Second Request (With Session):\", response_2.json())\n",
    "\n",
    "    # Send a third request to check cookies\n",
    "    response_3 = client.get(\"https://httpbin.org/cookies\")\n",
    "    print(\"Third Request (With Session):\", response_3.json())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time {end_time - start_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 status code: 200\n",
      "Response 2 status code: 200\n",
      "Response 3 status code: 200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Make asynchronous requests, Using AsyncClient.\n",
    "\"\"\"\n",
    "# async with httpx.AsyncClient() as client:\n",
    "#     response_1 = await client.get(\"https://httpbin.org/get\")\n",
    "#     response_2 = await client.get(\"https://httpbin.org/get\")\n",
    "    # TODO: Add another response object from the same site\n",
    "    # TODO: Print all response status code\n",
    "    \n",
    "    \n",
    "    \n",
    "import httpx\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response_1 = await client.get(\"https://httpbin.org/get\")\n",
    "        response_2 = await client.get(\"https://httpbin.org/get\")\n",
    "        response_3 = await client.get(\"https://httpbin.org/get\")  # Added third response\n",
    "        \n",
    "        # Print all response status codes\n",
    "        print(f\"Response 1 status code: {response_1.status_code}\")\n",
    "        print(f\"Response 2 status code: {response_2.status_code}\")\n",
    "        print(f\"Response 3 status code: {response_3.status_code}\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 100 requests\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 200]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Simulating sending a list of URLs\n",
    "\"\"\"\n",
    "import httpx\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# List of URLs to scrape (use a test URL or public API)\n",
    "urls = [\"https://httpbin.org/get\"] * 100  # Sending 100 requests to the same URL\n",
    "\n",
    "# Function to send requests concurrently\n",
    "async def fetch(url, client):\n",
    "    print(f\"Sending request to {url}\")\n",
    "    response = await client.get(url)\n",
    "    return response.status_code  # Return the status code to track success\n",
    "\n",
    "# Main function to send all requests concurrently\n",
    "async def send_requests():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        \n",
    "        # TODO: Use asyncio.gather to send requests concurrently  \n",
    "        tasks = [fetch(url, client) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(f\"Completed {len(results)} requests\")\n",
    "        return results    \n",
    "\n",
    "# TODO: Run the function\n",
    "asyncio.run(send_requests())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 1/100 requests. Status code: 200\n",
      "Completed 2/100 requests. Status code: 200\n",
      "Completed 3/100 requests. Status code: 200\n",
      "Completed 4/100 requests. Status code: 200\n",
      "Completed 5/100 requests. Status code: 200\n",
      "Completed 6/100 requests. Status code: 200\n",
      "Completed 7/100 requests. Status code: 200\n",
      "Completed 8/100 requests. Status code: 200\n",
      "Completed 9/100 requests. Status code: 200\n",
      "Completed 10/100 requests. Status code: 200\n",
      "Completed 11/100 requests. Status code: 200\n",
      "Completed 12/100 requests. Status code: 200\n",
      "Completed 13/100 requests. Status code: 200\n",
      "Completed 14/100 requests. Status code: 200\n",
      "Completed 15/100 requests. Status code: 200\n",
      "Completed 16/100 requests. Status code: 200\n",
      "Completed 17/100 requests. Status code: 200\n",
      "Completed 18/100 requests. Status code: 200\n",
      "Completed 19/100 requests. Status code: 200\n",
      "Completed 20/100 requests. Status code: 200\n",
      "Completed 21/100 requests. Status code: 200\n",
      "Completed 22/100 requests. Status code: 200\n",
      "Completed 23/100 requests. Status code: 200\n",
      "Completed 24/100 requests. Status code: 200\n",
      "Completed 25/100 requests. Status code: 200\n",
      "Completed 26/100 requests. Status code: 200\n",
      "Completed 27/100 requests. Status code: 200\n",
      "Completed 28/100 requests. Status code: 200\n",
      "Completed 29/100 requests. Status code: 200\n",
      "Completed 30/100 requests. Status code: 200\n",
      "Completed 31/100 requests. Status code: 200\n",
      "Completed 32/100 requests. Status code: 200\n",
      "Completed 33/100 requests. Status code: 200\n",
      "Completed 34/100 requests. Status code: 200\n",
      "Completed 35/100 requests. Status code: 200\n",
      "Completed 36/100 requests. Status code: 200\n",
      "Completed 37/100 requests. Status code: 200\n",
      "Completed 38/100 requests. Status code: 200\n",
      "Completed 39/100 requests. Status code: 200\n",
      "Completed 40/100 requests. Status code: 200\n",
      "Completed 41/100 requests. Status code: 200\n",
      "Completed 42/100 requests. Status code: 200\n",
      "Completed 43/100 requests. Status code: 200\n",
      "Completed 44/100 requests. Status code: 200\n",
      "Completed 45/100 requests. Status code: 200\n",
      "Completed 46/100 requests. Status code: 200\n",
      "Completed 47/100 requests. Status code: 200\n",
      "Completed 48/100 requests. Status code: 200\n",
      "Completed 49/100 requests. Status code: 200\n",
      "Completed 50/100 requests. Status code: 200\n",
      "Completed 51/100 requests. Status code: 200\n",
      "Completed 52/100 requests. Status code: 200\n",
      "Completed 53/100 requests. Status code: 200\n",
      "Completed 54/100 requests. Status code: 200\n",
      "Completed 55/100 requests. Status code: 200\n",
      "Completed 56/100 requests. Status code: 200\n",
      "Completed 57/100 requests. Status code: 200\n",
      "Completed 58/100 requests. Status code: 200\n",
      "Completed 59/100 requests. Status code: 200\n",
      "Completed 60/100 requests. Status code: 200\n",
      "Completed 61/100 requests. Status code: 200\n",
      "Completed 62/100 requests. Status code: 200\n",
      "Completed 63/100 requests. Status code: 200\n",
      "Completed 64/100 requests. Status code: 200\n",
      "Completed 65/100 requests. Status code: 200\n",
      "Completed 66/100 requests. Status code: 200\n",
      "Completed 67/100 requests. Status code: 200\n",
      "Completed 68/100 requests. Status code: 200\n",
      "Completed 69/100 requests. Status code: 200\n",
      "Completed 70/100 requests. Status code: 200\n",
      "Completed 71/100 requests. Status code: 200\n",
      "Completed 72/100 requests. Status code: 200\n",
      "Completed 73/100 requests. Status code: 200\n",
      "Completed 74/100 requests. Status code: 200\n",
      "Completed 75/100 requests. Status code: 200\n",
      "Completed 76/100 requests. Status code: 200\n",
      "Completed 77/100 requests. Status code: 200\n",
      "Completed 78/100 requests. Status code: 200\n",
      "Completed 79/100 requests. Status code: 200\n",
      "Completed 80/100 requests. Status code: 200\n",
      "Completed 81/100 requests. Status code: 200\n",
      "Completed 82/100 requests. Status code: 200\n",
      "Completed 83/100 requests. Status code: 200\n",
      "Completed 84/100 requests. Status code: 200\n",
      "Completed 85/100 requests. Status code: 200\n",
      "Completed 86/100 requests. Status code: 200\n",
      "Completed 87/100 requests. Status code: 200\n",
      "Completed 88/100 requests. Status code: 200\n",
      "Completed 89/100 requests. Status code: 200\n",
      "Completed 90/100 requests. Status code: 200\n",
      "Completed 91/100 requests. Status code: 200\n",
      "Completed 92/100 requests. Status code: 200\n",
      "Completed 93/100 requests. Status code: 200\n",
      "Completed 94/100 requests. Status code: 200\n",
      "Completed 95/100 requests. Status code: 200\n",
      "Completed 96/100 requests. Status code: 200\n",
      "Completed 97/100 requests. Status code: 200\n",
      "Completed 98/100 requests. Status code: 200\n",
      "Completed 99/100 requests. Status code: 200\n",
      "Completed 100/100 requests. Status code: 200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Monitoring progress as each task completed\n",
    "\"\"\"\n",
    "# TODO: Improve previous code to monitor progress using asyncio.as_completed()\n",
    "# TODO: Add a counter to count how many request already send\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://httpbin.org/get\"] * 100  # Sending 100 requests to the same URL\n",
    "\n",
    "async def fetch(url, client):\n",
    "    print(f\"Sending request to {url}\")\n",
    "    response = await client.get(url)\n",
    "    return response.status_code\n",
    "\n",
    "async def send_requests():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = [fetch(url, client) for url in urls]\n",
    "        completed = 0\n",
    "        \n",
    "        # Use as_completed to process results as they arrive\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            status_code = await task\n",
    "            completed += 1\n",
    "            print(f\"Completed {completed}/100 requests. Status code: {status_code}\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(send_requests())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 1/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 2/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 3/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 4/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 5/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 6/100 requests. Status code: 200\n",
      "Completed 7/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 8/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 9/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 10/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 11/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 12/100 requests. Status code: 200\n",
      "Completed 13/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 14/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 15/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 16/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 17/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 18/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 19/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 20/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 21/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 22/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 23/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 24/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 25/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 26/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 27/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 28/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 29/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 30/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 31/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 32/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 33/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 34/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 35/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 36/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 37/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 38/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 39/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 40/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 41/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 42/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 43/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 44/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 45/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 46/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 47/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 48/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 49/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 50/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 51/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 52/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 53/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 54/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 55/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 56/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 57/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 58/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 59/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 60/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 61/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 62/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 63/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 64/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 65/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 66/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 67/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 68/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 69/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 70/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 71/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 72/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 73/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 74/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 75/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 76/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 77/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 78/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 79/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 80/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 81/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 82/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 83/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 84/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 85/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 86/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 87/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 88/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 89/100 requests. Status code: 200\n",
      "Sending request to https://httpbin.org/get\n",
      "Completed 90/100 requests. Status code: 200\n",
      "Completed 91/100 requests. Status code: 200\n",
      "Completed 92/100 requests. Status code: 200\n",
      "Completed 93/100 requests. Status code: 200\n",
      "Completed 94/100 requests. Status code: 200\n",
      "Completed 95/100 requests. Status code: 200\n",
      "Completed 96/100 requests. Status code: 200\n",
      "Completed 97/100 requests. Status code: 200\n",
      "Completed 98/100 requests. Status code: 200\n",
      "Completed 99/100 requests. Status code: 200\n",
      "Completed 100/100 requests. Status code: 200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Limiting Concurrent Requests using Semaphore to avoid overloading the server\n",
    "\"\"\"\n",
    "# TODO: improve previous code by limiting to max 10 requests using asyncio.Semaphore()\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\"https://httpbin.org/get\"] * 100  # Sending 100 requests to the same URL\n",
    "\n",
    "# Create a semaphore to limit concurrent requests\n",
    "semaphore = asyncio.Semaphore(10)  # Limit to 10 concurrent requests\n",
    "\n",
    "async def fetch(url, client):\n",
    "    async with semaphore:  # Use semaphore to limit concurrent requests\n",
    "        print(f\"Sending request to {url}\")\n",
    "        response = await client.get(url)\n",
    "        return response.status_code\n",
    "\n",
    "async def send_requests():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        completed = 0\n",
    "        tasks = [fetch(url, client) for url in urls]\n",
    "        \n",
    "        # Process tasks as they complete\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            status_code = await task\n",
    "            completed += 1\n",
    "            print(f\"Completed {completed}/100 requests. Status code: {status_code}\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(send_requests())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2 pages successfully\n",
      "GitHub Link: https://github.com/rudicatsmile/scrpping_tutorial/tree/async-scraping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "Objective: Implement asynchronous in your web scraping\n",
    "\"\"\"\n",
    "# TODO: Create a new branch from your previous web scraping project\n",
    "# TODO: Implement asynchronous using httpx.AsyncClient\n",
    "# TODO: Push and put the github link here for grading\n",
    "\n",
    "\n",
    "import httpx\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Semaphore to limit concurrent requests\n",
    "semaphore = asyncio.Semaphore(10)\n",
    "\n",
    "async def fetch_page(url, client):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            response = await client.get(url)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "async def parse_page(html):\n",
    "    if html is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Add your parsing logic here\n",
    "    # Example:\n",
    "    data = {\n",
    "        'title': soup.title.text if soup.title else '',\n",
    "        # Add more fields based on your scraping needs\n",
    "    }\n",
    "    return data\n",
    "\n",
    "async def scrape_website():\n",
    "    # Your list of URLs to scrape\n",
    "    urls = [\n",
    "        \"https://example.com/page1\",\n",
    "        \"https://example.com/page2\",\n",
    "        # Add more URLs\n",
    "    ]\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Fetch all pages concurrently\n",
    "        html_contents = await asyncio.gather(*[\n",
    "            fetch_page(url, client) for url in urls\n",
    "        ])\n",
    "        \n",
    "        # Parse all pages concurrently\n",
    "        results = await asyncio.gather(*[\n",
    "            parse_page(html) for html in html_contents if html is not None\n",
    "        ])\n",
    "        \n",
    "        # Filter out None results and convert to DataFrame\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        df = pd.DataFrame(valid_results)\n",
    "        return df\n",
    "\n",
    "# Run the scraper\n",
    "df = asyncio.run(scrape_website())\n",
    "print(f\"Scraped {len(df)} pages successfully\")\n",
    "\n",
    "# GitHub repository link for grading\n",
    "github_link = \"https://github.com/rudicatsmile/scrpping_tutorial/tree/async-scraping\"\n",
    "print(f\"GitHub Link: {github_link}\")\n",
    "\n",
    "\n",
    "\n",
    "# git add .\n",
    "# git commit -m \"Implement async web scraping using httpx\"\n",
    "# git push origin async-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "By using asynchronous, we can send multiple request at once. By doing that, what do you think will effect on the server side?\n",
    "\n",
    "(answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using asynchronous requests to send multiple requests simultaneously to a server, there are several potential effects on the server side:\n",
    "\n",
    "1. Increased Server Load\n",
    "   \n",
    "   - The server needs to handle multiple concurrent requests instead of processing them one at a time\n",
    "   - This can lead to higher CPU and memory usage on the server\n",
    "2. Resource Consumption\n",
    "   \n",
    "   - More simultaneous connections need to be maintained\n",
    "   - Database connections may increase if the requests require database access\n",
    "   - Memory usage might spike to handle multiple requests at once\n",
    "3. Rate Limiting Issues\n",
    "   \n",
    "   - Servers might interpret multiple simultaneous requests as a potential DDoS attack\n",
    "   - This could trigger rate limiting or IP blocking mechanisms\n",
    "   - Some servers might return 429 (Too Many Requests) errors\n",
    "4. Bandwidth Usage\n",
    "   \n",
    "   - Network bandwidth consumption increases due to parallel requests\n",
    "   - This could affect other users accessing the same server\n",
    "5. Server Response Time\n",
    "   \n",
    "   - If the server isn't properly configured for concurrent requests, response times might increase\n",
    "   - Other users' requests might be delayed due to increased server load\n",
    "To mitigate these effects, it's important to:\n",
    "\n",
    "- Implement rate limiting in your async code\n",
    "- Use semaphores to control concurrent connections\n",
    "- Respect the server's robots.txt and terms of service\n",
    "- Add delays between requests when necessary\n",
    "- Monitor server responses for rate limiting warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Explore how you can optimize the scraping execution time while still maintaining control over the quantity of request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are several strategies to optimize web scraping execution time while maintaining control over request quantity:\n",
    "1. Batch Processing with Semaphores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "# Configure limits\n",
    "BATCH_SIZE = 10\n",
    "TOTAL_URLS = 100\n",
    "DELAY_BETWEEN_BATCHES = 1  # seconds\n",
    "\n",
    "async def fetch_with_batches():\n",
    "    urls = [\"https://httpbin.org/get\"] * TOTAL_URLS\n",
    "    semaphore = asyncio.Semaphore(BATCH_SIZE)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        async def fetch_url(url):\n",
    "            async with semaphore:\n",
    "                response = await client.get(url)\n",
    "                return response.status_code\n",
    "                \n",
    "        # Process in batches\n",
    "        for i in range(0, len(urls), BATCH_SIZE):\n",
    "            batch = urls[i:i+BATCH_SIZE]\n",
    "            tasks = [fetch_url(url) for url in batch]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            print(f\"Batch {i//BATCH_SIZE + 1} completed: {len(results)} requests\")\n",
    "            await asyncio.sleep(DELAY_BETWEEN_BATCHES)\n",
    "            \n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "asyncio.run(fetch_with_batches())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dynamic Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_requests, time_window):\n",
    "        self.max_requests = max_requests\n",
    "        self.time_window = time_window\n",
    "        self.requests = deque()\n",
    "    \n",
    "    async def acquire(self):\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remove old requests\n",
    "        while self.requests and self.requests[0] <= now - self.time_window:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Wait if at limit\n",
    "        if len(self.requests) >= self.max_requests:\n",
    "            wait_time = self.requests[0] - (now - self.time_window)\n",
    "            if wait_time > 0:\n",
    "                await asyncio.sleep(wait_time)\n",
    "        \n",
    "        self.requests.append(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prioritized Queue System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "from asyncio import PriorityQueue\n",
    "\n",
    "async def priority_scraper():\n",
    "    queue = PriorityQueue()\n",
    "    # Priority 1: Critical pages\n",
    "    # Priority 2: Important pages\n",
    "    # Priority 3: Normal pages\n",
    "    \n",
    "    async def worker():\n",
    "        while True:\n",
    "            priority, url = await queue.get()\n",
    "            try:\n",
    "                async with httpx.AsyncClient() as client:\n",
    "                    response = await client.get(url)\n",
    "                    print(f\"Processed {url} with priority {priority}\")\n",
    "            finally:\n",
    "                queue.task_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Optimization Techniques:\n",
    "\n",
    "1. Use connection pooling with httpx.AsyncClient()\n",
    "2. Implement exponential backoff for retries\n",
    "3. Monitor and adjust concurrency limits based on server response\n",
    "4. Cache responses when appropriate\n",
    "5. Use streaming responses for large payloads\n",
    "6. Implement proper error handling and recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
