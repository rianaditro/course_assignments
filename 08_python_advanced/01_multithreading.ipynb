{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ThreadPoolExecutor for Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ThreadPoolExecutor?\n",
    "`ThreadPoolExecutor` is a Python class in the `concurrent.futures` module that allows you to manage a pool of threads efficiently. It simplifies multithreading by allowing you to run multiple tasks concurrently, making it ideal for I/O-bound tasks like web scraping.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use ThreadPoolExecutor in Web Scraping?\n",
    "When web scraping, most of the time is spent waiting for server responses (I/O). Using `ThreadPoolExecutor` enables you to:\n",
    "- Scrape multiple pages concurrently.\n",
    "- Reduce overall execution time.\n",
    "- Use system resources more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Basic Example\n",
    "Hereâ€™s a simple example of using `ThreadPoolExecutor` to scrape multiple URLs:\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def scrape_page(url):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    time.sleep(2)  # Simulates a delay for I/O-bound tasks\n",
    "    return f\"Data from {url}\"\n",
    "\n",
    "urls = [f\"https://example.com/page{i}\" for i in range(1, 6)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # 3 worker threads\n",
    "    results = list(executor.map(scrape_page, urls))\n",
    "\n",
    "print(\"Scraping completed!\")\n",
    "```\n",
    "- **`max_workers=3`**: Creates 3 threads to scrape URLs concurrently.\n",
    "- **`executor.map()`**: Maps the `scrape_page` function to each URL in the list.\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Using ThreadPoolExecutor\n",
    "- **Concurrency**: Reduces execution time for I/O-bound tasks.\n",
    "- **Simple API**: Easy to use compared to manually managing threads.\n",
    "- **Scalability**: Handles many tasks efficiently by reusing threads.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 started.\n",
      "scraping 1 completed.\n",
      "scraping 2 started.\n",
      "scraping 2 completed.\n",
      "scraping 3 started.\n",
      "scraping 3 completed.\n",
      "scraping 4 started.\n",
      "scraping 4 completed.\n",
      "scraping 5 started.\n",
      "scraping 5 completed.\n",
      "\n",
      "Total execution time: 10.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Compare time execution based on basic loop\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "\n",
    "# Simulating an I/O-bound scraping with time.sleep()\n",
    "def io_bound_scraping(scraping_id):\n",
    "    print(f\"scraping {scraping_id} started.\")\n",
    "    time.sleep(2)  # Simulate I/O operation\n",
    "    print(f\"scraping {scraping_id} completed.\")\n",
    "\n",
    "# Using basic loop\n",
    "def main():\n",
    "    # TODO: Fill main function to run io_bound_scraping 5 times\n",
    "    # TODO: (Optional) Estimate the time execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run scraping 5 times sequentially\n",
    "    for i in range(1, 6):\n",
    "        io_bound_scraping(i)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n",
    "    pass\n",
    "        \n",
    "# Call the main function to run the scrapings\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 started.\n",
      "scraping 1 completed.\n",
      "scraping 2 started.\n",
      "scraping 2 completed.\n",
      "scraping 3 started.\n",
      "scraping 3 completed.\n",
      "scraping 4 started.\n",
      "scraping 4 completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Compare time execution based on the number of workers\n",
    "\"\"\"\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Simulating an I/O-bound scraping with time.sleep()\n",
    "def io_bound_scraping(scraping_id):\n",
    "    print(f\"scraping {scraping_id} started.\")\n",
    "    time.sleep(2)  # Simulate I/O operation\n",
    "    print(f\"scraping {scraping_id} completed.\")\n",
    "\n",
    "# Using ThreadPoolExecutor with map for faster execution\n",
    "def main():\n",
    "    # TODO: Creating a ThreadPoolExecutor with 1 threads\n",
    "    with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        # Use map to run the scrapings concurrently\n",
    "        executor.map(io_bound_scraping, range(1,5))\n",
    "\n",
    "# Call the main function to run the scrapings\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 started.\n",
      "scraping 2 started.\n",
      "scraping 1 completed.\n",
      "scraping 3 started.\n",
      "scraping 2 completed.\n",
      "scraping 4 started.\n",
      "scraping 3 completed.\n",
      "scraping 5 started.\n",
      "scraping 4 completed.\n",
      "scraping 5 completed.\n",
      "\n",
      "Total execution time: 6.01 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Compare time execution based on the number of workers\n",
    "\"\"\"\n",
    "# TODO: Recreate previous code with the 2 workers\n",
    "\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Simulating an I/O-bound scraping with time.sleep()\n",
    "def io_bound_scraping(scraping_id):\n",
    "    print(f\"scraping {scraping_id} started.\")\n",
    "    time.sleep(2)  # Simulate I/O operation\n",
    "    print(f\"scraping {scraping_id} completed.\")\n",
    "\n",
    "# Using ThreadPoolExecutor with map for faster execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creating a ThreadPoolExecutor with 2 workers\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # Use map to run the scrapings concurrently\n",
    "        executor.map(io_bound_scraping, range(1, 6))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Call the main function to run the scrapings\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 started.\n",
      "scraping 2 started.\n",
      "scraping 3 started.\n",
      "scraping 4 started.\n",
      "scraping 1 completed.\n",
      "scraping 5 started.\n",
      "scraping 2 completed.\n",
      "scraping 3 completed.\n",
      "scraping 4 completed.\n",
      "scraping 5 completed.\n",
      "\n",
      "Total execution time: 4.01 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Compare time execution based on the number of workers\n",
    "\"\"\"\n",
    "# TODO: Recreate previous code with the 4 workers\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Simulating an I/O-bound scraping with time.sleep()\n",
    "def io_bound_scraping(scraping_id):\n",
    "    print(f\"scraping {scraping_id} started.\")\n",
    "    time.sleep(2)  # Simulate I/O operation\n",
    "    print(f\"scraping {scraping_id} completed.\")\n",
    "\n",
    "# Using ThreadPoolExecutor with map for faster execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Creating a ThreadPoolExecutor with 4 workers\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Use map to run the scrapings concurrently\n",
    "        executor.map(io_bound_scraping, range(1, 6))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"\\nTotal execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Call the main function to run the scrapings\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 started.\n",
      "scraping 2 started.\n",
      "scraping 3 started.\n",
      "scraping 4 started.\n",
      "scraping 5 started.\n",
      "scraping 6 started.\n",
      "scraping 7 started.\n",
      "scraping 8 started.\n",
      "scraping 9 started.\n",
      "scraping 10 started.\n",
      "scraping 11 started.\n",
      "scraping 12 started.\n",
      "scraping 13 started.\n",
      "scraping 14 started.\n",
      "scraping 15 started.\n",
      "scraping 16 started.\n",
      "scraping 17 started.\n",
      "scraping 18 started.\n",
      "scraping 19 started.\n",
      "scraping 20 started.\n",
      "scraping 21 started.\n",
      "scraping 22 started.\n",
      "scraping 23 started.\n",
      "scraping 24 started.\n",
      "scraping 25 started.\n",
      "scraping 26 started.\n",
      "scraping 27 started.\n",
      "scraping 28 started.\n",
      "scraping 29 started.\n",
      "scraping 30 started.\n",
      "scraping 31 started.\n",
      "scraping 32 started.\n",
      "scraping 33 started.\n",
      "scraping 34 started.\n",
      "scraping 35 started.\n",
      "scraping 36 started.\n",
      "scraping 37 started.\n",
      "scraping 38 started.\n",
      "scraping 39 started.\n",
      "scraping 40 started.\n",
      "scraping 41 started.\n",
      "scraping 42 started.\n",
      "scraping 43 started.\n",
      "scraping 44 started.\n",
      "scraping 45 started.\n",
      "scraping 46 started.\n",
      "scraping 47 started.\n",
      "scraping 48 started.\n",
      "scraping 49 started.\n",
      "scraping 50 started.\n",
      "scraping 51 started.\n",
      "scraping 52 started.\n",
      "scraping 53 started.\n",
      "scraping 54 started.\n",
      "scraping 55 started.\n",
      "scraping 56 started.\n",
      "scraping 57 started.\n",
      "scraping 58 started.\n",
      "scraping 59 started.\n",
      "scraping 60 started.\n",
      "scraping 61 started.\n",
      "scraping 62 started.\n",
      "scraping 63 started.\n",
      "scraping 64 started.\n",
      "scraping 65 started.\n",
      "scraping 66 started.\n",
      "scraping 67 started.\n",
      "scraping 68 started.\n",
      "scraping 69 started.\n",
      "scraping 70 started.\n",
      "scraping 71 started.\n",
      "scraping 72 started.\n",
      "scraping 73 started.\n",
      "scraping 74 started.\n",
      "scraping 75 started.\n",
      "scraping 76 started.\n",
      "scraping 77 started.\n",
      "scraping 78 started.\n",
      "scraping 79 started.\n",
      "scraping 80 started.\n",
      "scraping 81 started.\n",
      "scraping 82 started.\n",
      "scraping 83 started.\n",
      "scraping 84 started.\n",
      "scraping 85 started.\n",
      "scraping 86 started.\n",
      "scraping 87 started.\n",
      "scraping 88 started.\n",
      "scraping 89 started.\n",
      "scraping 90 started.\n",
      "scraping 91 started.\n",
      "scraping 92 started.\n",
      "scraping 93 started.\n",
      "scraping 94 started.\n",
      "scraping 95 started.\n",
      "scraping 96 started.\n",
      "scraping 97 started.\n",
      "scraping 98 started.\n",
      "scraping 99 started.\n",
      "scraping 100 started.\n",
      "scraping 101 started.\n",
      "scraping 102 started.\n",
      "scraping 103 started.\n",
      "scraping 104 started.\n",
      "scraping 105 started.\n",
      "scraping 106 started.\n",
      "scraping 107 started.\n",
      "scraping 108 started.\n",
      "scraping 109 started.\n",
      "scraping 110 started.\n",
      "scraping 111 started.\n",
      "scraping 112 started.\n",
      "scraping 113 started.\n",
      "scraping 114 started.\n",
      "scraping 115 started.\n",
      "scraping 116 started.\n",
      "scraping 117 started.\n",
      "scraping 118 started.\n",
      "scraping 119 started.\n",
      "scraping 120 started.\n",
      "scraping 121 started.\n",
      "scraping 122 started.\n",
      "scraping 123 started.\n",
      "scraping 124 started.\n",
      "scraping 125 started.\n",
      "scraping 126 started.\n",
      "scraping 127 started.\n",
      "scraping 128 started.\n",
      "scraping 129 started.\n",
      "scraping 130 started.\n",
      "scraping 131 started.\n",
      "scraping 132 started.\n",
      "scraping 133 started.\n",
      "scraping 134 started.\n",
      "scraping 135 started.\n",
      "scraping 136 started.\n",
      "scraping 137 started.\n",
      "scraping 138 started.\n",
      "scraping 139 started.\n",
      "scraping 140 started.\n",
      "scraping 141 started.\n",
      "scraping 142 started.\n",
      "scraping 143 started.\n",
      "scraping 144 started.\n",
      "scraping 145 started.\n",
      "scraping 146 started.\n",
      "scraping 147 started.\n",
      "scraping 148 started.\n",
      "scraping 149 started.\n",
      "scraping 150 started.\n",
      "scraping 151 started.\n",
      "scraping 152 started.\n",
      "scraping 153 started.\n",
      "scraping 154 started.\n",
      "scraping 155 started.\n",
      "scraping 156 started.\n",
      "scraping 157 started.\n",
      "scraping 158 started.\n",
      "scraping 159 started.\n",
      "scraping 160 started.\n",
      "scraping 161 started.\n",
      "scraping 162 started.\n",
      "scraping 163 started.\n",
      "scraping 164 started.\n",
      "scraping 165 started.\n",
      "scraping 166 started.\n",
      "scraping 167 started.\n",
      "scraping 168 started.\n",
      "scraping 169 started.\n",
      "scraping 170 started.\n",
      "scraping 171 started.\n",
      "scraping 172 started.\n",
      "scraping 173 started.\n",
      "scraping 174 started.\n",
      "scraping 175 started.\n",
      "scraping 176 started.\n",
      "scraping 177 started.\n",
      "scraping 178 started.\n",
      "scraping 179 started.\n",
      "scraping 180 started.\n",
      "scraping 181 started.\n",
      "scraping 182 started.\n",
      "scraping 183 started.\n",
      "scraping 184 started.\n",
      "scraping 185 started.\n",
      "scraping 186 started.\n",
      "scraping 187 started.\n",
      "scraping 188 started.\n",
      "scraping 189 started.\n",
      "scraping 190 started.\n",
      "scraping 191 started.\n",
      "scraping 192 started.\n",
      "scraping 193 started.\n",
      "scraping 194 started.\n",
      "scraping 195 started.\n",
      "scraping 196 started.\n",
      "scraping 197 started.\n",
      "scraping 198 started.\n",
      "scraping 199 started.\n",
      "scraping 200 started.\n",
      "scraping 201 started.\n",
      "scraping 202 started.\n",
      "scraping 203 started.\n",
      "scraping 204 started.\n",
      "scraping 205 started.\n",
      "scraping 206 started.\n",
      "scraping 207 started.\n",
      "scraping 208 started.\n",
      "scraping 209 started.\n",
      "scraping 210 started.\n",
      "scraping 211 started.\n",
      "scraping 212 started.\n",
      "scraping 213 started.\n",
      "scraping 214 started.\n",
      "scraping 215 started.\n",
      "scraping 216 started.\n",
      "scraping 217 started.\n",
      "scraping 218 started.\n",
      "scraping 219 started.\n",
      "scraping 220 started.\n",
      "scraping 221 started.\n",
      "scraping 222 started.\n",
      "scraping 223 started.\n",
      "scraping 224 started.\n",
      "scraping 225 started.\n",
      "scraping 226 started.\n",
      "scraping 227 started.\n",
      "scraping 228 started.\n",
      "scraping 229 started.\n",
      "scraping 230 started.\n",
      "scraping 231 started.\n",
      "scraping 232 started.\n",
      "scraping 233 started.\n",
      "scraping 234 started.\n",
      "scraping 235 started.\n",
      "scraping 236 started.\n",
      "scraping 237 started.\n",
      "scraping 238 started.\n",
      "scraping 239 started.\n",
      "scraping 240 started.\n",
      "scraping 241 started.\n",
      "scraping 242 started.\n",
      "scraping 243 started.\n",
      "scraping 244 started.\n",
      "scraping 245 started.\n",
      "scraping 246 started.\n",
      "scraping 247 started.\n",
      "scraping 248 started.\n",
      "scraping 249 started.\n",
      "scraping 250 started.\n",
      "scraping 251 started.\n",
      "scraping 252 started.\n",
      "scraping 253 started.\n",
      "scraping 254 started.\n",
      "scraping 255 started.\n",
      "scraping 256 started.\n",
      "scraping 257 started.\n",
      "scraping 258 started.\n",
      "scraping 259 started.\n",
      "scraping 260 started.\n",
      "scraping 261 started.\n",
      "scraping 262 started.\n",
      "scraping 263 started.\n",
      "scraping 264 started.\n",
      "scraping 265 started.\n",
      "scraping 266 started.\n",
      "scraping 267 started.\n",
      "scraping 268 started.\n",
      "scraping 269 started.\n",
      "scraping 270 started.\n",
      "scraping 271 started.\n",
      "scraping 272 started.\n",
      "scraping 273 started.\n",
      "scraping 274 started.\n",
      "scraping 275 started.\n",
      "scraping 276 started.\n",
      "scraping 277 started.\n",
      "scraping 278 started.\n",
      "scraping 279 started.\n",
      "scraping 280 started.\n",
      "scraping 281 started.\n",
      "scraping 282 started.\n",
      "scraping 283 started.\n",
      "scraping 284 started.\n",
      "scraping 285 started.\n",
      "scraping 286 started.\n",
      "scraping 287 started.\n",
      "scraping 288 started.\n",
      "scraping 289 started.\n",
      "scraping 290 started.\n",
      "scraping 291 started.\n",
      "scraping 292 started.\n",
      "scraping 293 started.\n",
      "scraping 294 started.\n",
      "scraping 295 started.\n",
      "scraping 296 started.\n",
      "scraping 297 started.\n",
      "scraping 298 started.\n",
      "scraping 299 started.\n",
      "scraping 300 started.\n",
      "scraping 301 started.\n",
      "scraping 302 started.\n",
      "scraping 303 started.\n",
      "scraping 304 started.\n",
      "scraping 305 started.\n",
      "scraping 306 started.\n",
      "scraping 307 started.\n",
      "scraping 308 started.\n",
      "scraping 309 started.\n",
      "scraping 310 started.\n",
      "scraping 311 started.\n",
      "scraping 312 started.\n",
      "scraping 313 started.\n",
      "scraping 314 started.\n",
      "scraping 315 started.\n",
      "scraping 316 started.\n",
      "scraping 317 started.\n",
      "scraping 318 started.\n",
      "scraping 319 started.\n",
      "scraping 320 started.\n",
      "scraping 321 started.\n",
      "scraping 322 started.\n",
      "scraping 323 started.\n",
      "scraping 324 started.\n",
      "scraping 325 started.\n",
      "scraping 326 started.\n",
      "scraping 327 started.\n",
      "scraping 328 started.\n",
      "scraping 329 started.\n",
      "scraping 330 started.\n",
      "scraping 331 started.\n",
      "scraping 332 started.\n",
      "scraping 333 started.\n",
      "scraping 334 started.\n",
      "scraping 335 started.\n",
      "scraping 336 started.\n",
      "scraping 337 started.\n",
      "scraping 338 started.\n",
      "scraping 339 started.\n",
      "scraping 340 started.\n",
      "scraping 341 started.\n",
      "scraping 342 started.\n",
      "scraping 343 started.\n",
      "scraping 344 started.\n",
      "scraping 345 started.\n",
      "scraping 346 started.\n",
      "scraping 347 started.\n",
      "scraping 348 started.\n",
      "scraping 349 started.\n",
      "scraping 350 started.\n",
      "scraping 351 started.\n",
      "scraping 352 started.\n",
      "scraping 353 started.\n",
      "scraping 354 started.\n",
      "scraping 355 started.\n",
      "scraping 356 started.\n",
      "scraping 357 started.\n",
      "scraping 358 started.\n",
      "scraping 359 started.\n",
      "scraping 360 started.\n",
      "scraping 361 started.\n",
      "scraping 362 started.\n",
      "scraping 363 started.\n",
      "scraping 364 started.\n",
      "scraping 365 started.\n",
      "scraping 366 started.\n",
      "scraping 367 started.\n",
      "scraping 368 started.\n",
      "scraping 369 started.\n",
      "scraping 370 started.\n",
      "scraping 371 started.\n",
      "scraping 372 started.\n",
      "scraping 373 started.\n",
      "scraping 374 started.\n",
      "scraping 375 started.\n",
      "scraping 376 started.\n",
      "scraping 377 started.\n",
      "scraping 378 started.\n",
      "scraping 379 started.\n",
      "scraping 380 started.\n",
      "scraping 381 started.\n",
      "scraping 382 started.\n",
      "scraping 383 started.\n",
      "scraping 384 started.\n",
      "scraping 385 started.\n",
      "scraping 386 started.\n",
      "scraping 387 started.\n",
      "scraping 388 started.\n",
      "scraping 389 started.\n",
      "scraping 390 started.\n",
      "scraping 391 started.\n",
      "scraping 392 started.\n",
      "scraping 393 started.\n",
      "scraping 394 started.\n",
      "scraping 395 started.\n",
      "scraping 396 started.\n",
      "scraping 397 started.\n",
      "scraping 398 started.\n",
      "scraping 399 started.\n",
      "scraping 400 started.\n",
      "scraping 401 started.\n",
      "scraping 402 started.\n",
      "scraping 403 started.\n",
      "scraping 404 started.\n",
      "scraping 405 started.\n",
      "scraping 406 started.\n",
      "scraping 407 started.\n",
      "scraping 408 started.\n",
      "scraping 409 started.\n",
      "scraping 410 started.\n",
      "scraping 411 started.\n",
      "scraping 412 started.\n",
      "scraping 413 started.\n",
      "scraping 414 started.\n",
      "scraping 415 started.\n",
      "scraping 416 started.\n",
      "scraping 417 started.\n",
      "scraping 418 started.\n",
      "scraping 419 started.\n",
      "scraping 420 started.\n",
      "scraping 421 started.\n",
      "scraping 422 started.\n",
      "scraping 423 started.\n",
      "scraping 424 started.\n",
      "scraping 425 started.\n",
      "scraping 426 started.\n",
      "scraping 427 started.\n",
      "scraping 428 started.\n",
      "scraping 429 started.\n",
      "scraping 430 started.\n",
      "scraping 431 started.\n",
      "scraping 432 started.\n",
      "scraping 433 started.\n",
      "scraping 434 started.\n",
      "scraping 435 started.\n",
      "scraping 436 started.\n",
      "scraping 437 started.\n",
      "scraping 438 started.\n",
      "scraping 439 started.\n",
      "scraping 440 started.\n",
      "scraping 441 started.\n",
      "scraping 442 started.\n",
      "scraping 443 started.\n",
      "scraping 444 started.\n",
      "scraping 445 started.\n",
      "scraping 446 started.\n",
      "scraping 447 started.\n",
      "scraping 448 started.\n",
      "scraping 449 started.\n",
      "scraping 450 started.\n",
      "scraping 451 started.\n",
      "scraping 452 started.\n",
      "scraping 453 started.\n",
      "scraping 454 started.\n",
      "scraping 455 started.\n",
      "scraping 456 started.\n",
      "scraping 457 started.\n",
      "scraping 458 started.\n",
      "scraping 459 started.\n",
      "scraping 460 started.\n",
      "scraping 461 started.\n",
      "scraping 462 started.\n",
      "scraping 463 started.\n",
      "scraping 464 started.\n",
      "scraping 465 started.\n",
      "scraping 466 started.\n",
      "scraping 467 started.\n",
      "scraping 468 started.\n",
      "scraping 469 started.\n",
      "scraping 470 started.\n",
      "scraping 471 started.\n",
      "scraping 472 started.\n",
      "scraping 473 started.\n",
      "scraping 474 started.\n",
      "scraping 475 started.\n",
      "scraping 476 started.\n",
      "scraping 477 started.\n",
      "scraping 478 started.\n",
      "scraping 479 started.\n",
      "scraping 480 started.\n",
      "scraping 481 started.\n",
      "scraping 482 started.\n",
      "scraping 483 started.\n",
      "scraping 484 started.\n",
      "scraping 485 started.\n",
      "scraping 486 started.\n",
      "scraping 487 started.\n",
      "scraping 488 started.\n",
      "scraping 489 started.\n",
      "scraping 490 started.\n",
      "scraping 491 started.\n",
      "scraping 492 started.\n",
      "scraping 493 started.\n",
      "scraping 494 started.\n",
      "scraping 495 started.\n",
      "scraping 496 started.\n",
      "scraping 497 started.\n",
      "scraping 498 started.\n",
      "scraping 499 started.\n",
      "scraping 500 started.\n",
      "scraping 1 completed.\n",
      "scraping 501 started.\n",
      "scraping 2 completed.\n",
      "scraping 502 started.\n",
      "scraping 3 completed.\n",
      "scraping 503 started.\n",
      "scraping 4 completed.\n",
      "scraping 504 started.\n",
      "scraping 6 completed.\n",
      "scraping 505 started.\n",
      "scraping 5 completed.\n",
      "scraping 506 started.\n",
      "scraping 7 completed.\n",
      "scraping 507 started.\n",
      "scraping 8 completed.\n",
      "scraping 508 started.\n",
      "scraping 9 completed.\n",
      "scraping 509 started.\n",
      "scraping 10 completed.\n",
      "scraping 510 started.\n",
      "scraping 11 completed.\n",
      "scraping 511 started.\n",
      "scraping 12 completed.\n",
      "scraping 512 started.\n",
      "scraping 14 completed.\n",
      "scraping 513 started.\n",
      "scraping 13 completed.\n",
      "scraping 514 started.\n",
      "scraping 15 completed.\n",
      "scraping 515 started.\n",
      "scraping 16 completed.\n",
      "scraping 516 started.\n",
      "scraping 17 completed.\n",
      "scraping 517 started.\n",
      "scraping 18 completed.\n",
      "scraping 518 started.\n",
      "scraping 19 completed.\n",
      "scraping 519 started.\n",
      "scraping 20 completed.\n",
      "scraping 520 started.\n",
      "scraping 21 completed.\n",
      "scraping 521 started.\n",
      "scraping 22 completed.\n",
      "scraping 522 started.\n",
      "scraping 23 completed.\n",
      "scraping 523 started.\n",
      "scraping 24 completed.\n",
      "scraping 524 started.\n",
      "scraping 25 completed.\n",
      "scraping 525 started.\n",
      "scraping 26 completed.\n",
      "scraping 526 started.\n",
      "scraping 28 completed.\n",
      "scraping 527 started.\n",
      "scraping 27 completed.\n",
      "scraping 528 started.\n",
      "scraping 29 completed.\n",
      "scraping 529 started.\n",
      "scraping 30 completed.\n",
      "scraping 530 started.\n",
      "scraping 31 completed.\n",
      "scraping 531 started.\n",
      "scraping 32 completed.\n",
      "scraping 532 started.\n",
      "scraping 33 completed.\n",
      "scraping 533 started.\n",
      "scraping 34 completed.\n",
      "scraping 534 started.\n",
      "scraping 35 completed.\n",
      "scraping 535 started.\n",
      "scraping 37 completed.\n",
      "scraping 536 started.\n",
      "scraping 36 completed.\n",
      "scraping 537 started.\n",
      "scraping 38 completed.\n",
      "scraping 538 started.\n",
      "scraping 40 completed.\n",
      "scraping 539 started.\n",
      "scraping 39 completed.\n",
      "scraping 540 started.\n",
      "scraping 41 completed.\n",
      "scraping 541 started.\n",
      "scraping 42 completed.\n",
      "scraping 542 started.\n",
      "scraping 44 completed.\n",
      "scraping 543 started.\n",
      "scraping 43 completed.\n",
      "scraping 544 started.\n",
      "scraping 45 completed.\n",
      "scraping 545 started.\n",
      "scraping 46 completed.\n",
      "scraping 546 started.\n",
      "scraping 47 completed.\n",
      "scraping 547 started.\n",
      "scraping 48 completed.\n",
      "scraping 548 started.\n",
      "scraping 49 completed.\n",
      "scraping 549 started.\n",
      "scraping 50 completed.\n",
      "scraping 550 started.\n",
      "scraping 51 completed.\n",
      "scraping 551 started.\n",
      "scraping 52 completed.\n",
      "scraping 552 started.\n",
      "scraping 53 completed.\n",
      "scraping 553 started.\n",
      "scraping 54 completed.\n",
      "scraping 554 started.\n",
      "scraping 55 completed.\n",
      "scraping 555 started.\n",
      "scraping 56 completed.\n",
      "scraping 556 started.\n",
      "scraping 57 completed.\n",
      "scraping 557 started.\n",
      "scraping 58 completed.\n",
      "scraping 558 started.\n",
      "scraping 59 completed.\n",
      "scraping 559 started.\n",
      "scraping 61 completed.\n",
      "scraping 560 started.\n",
      "scraping 60 completed.\n",
      "scraping 561 started.\n",
      "scraping 62 completed.\n",
      "scraping 562 started.\n",
      "scraping 64 completed.\n",
      "scraping 563 started.\n",
      "scraping 63 completed.\n",
      "scraping 564 started.\n",
      "scraping 65 completed.\n",
      "scraping 565 started.\n",
      "scraping 66 completed.\n",
      "scraping 566 started.\n",
      "scraping 67 completed.\n",
      "scraping 567 started.\n",
      "scraping 68 completed.\n",
      "scraping 568 started.\n",
      "scraping 69 completed.\n",
      "scraping 569 started.\n",
      "scraping 70 completed.\n",
      "scraping 570 started.\n",
      "scraping 71 completed.\n",
      "scraping 571 started.\n",
      "scraping 72 completed.\n",
      "scraping 572 started.\n",
      "scraping 73 completed.\n",
      "scraping 573 started.\n",
      "scraping 74 completed.\n",
      "scraping 574 started.\n",
      "scraping 75 completed.\n",
      "scraping 575 started.\n",
      "scraping 76 completed.\n",
      "scraping 576 started.\n",
      "scraping 77 completed.\n",
      "scraping 577 started.\n",
      "scraping 78 completed.\n",
      "scraping 578 started.\n",
      "scraping 79 completed.\n",
      "scraping 579 started.\n",
      "scraping 80 completed.\n",
      "scraping 580 started.\n",
      "scraping 81 completed.\n",
      "scraping 581 started.\n",
      "scraping 82 completed.\n",
      "scraping 582 started.\n",
      "scraping 83 completed.\n",
      "scraping 583 started.\n",
      "scraping 84 completed.\n",
      "scraping 584 started.\n",
      "scraping 85 completed.\n",
      "scraping 585 started.\n",
      "scraping 86 completed.\n",
      "scraping 586 started.\n",
      "scraping 87 completed.\n",
      "scraping 587 started.\n",
      "scraping 88 completed.\n",
      "scraping 588 started.\n",
      "scraping 89 completed.\n",
      "scraping 589 started.\n",
      "scraping 91 completed.\n",
      "scraping 590 started.\n",
      "scraping 90 completed.\n",
      "scraping 591 started.\n",
      "scraping 92 completed.\n",
      "scraping 592 started.\n",
      "scraping 93 completed.\n",
      "scraping 593 started.\n",
      "scraping 94 completed.\n",
      "scraping 594 started.\n",
      "scraping 95 completed.\n",
      "scraping 595 started.\n",
      "scraping 96 completed.\n",
      "scraping 596 started.\n",
      "scraping 98 completed.\n",
      "scraping 597 started.\n",
      "scraping 97 completed.\n",
      "scraping 598 started.\n",
      "scraping 99 completed.\n",
      "scraping 599 started.\n",
      "scraping 101 completed.\n",
      "scraping 600 started.\n",
      "scraping 100 completed.\n",
      "scraping 601 started.\n",
      "scraping 102 completed.\n",
      "scraping 602 started.\n",
      "scraping 103 completed.\n",
      "scraping 603 started.\n",
      "scraping 104 completed.\n",
      "scraping 604 started.\n",
      "scraping 105 completed.\n",
      "scraping 605 started.\n",
      "scraping 106 completed.\n",
      "scraping 606 started.\n",
      "scraping 107 completed.\n",
      "scraping 607 started.\n",
      "scraping 108 completed.\n",
      "scraping 608 started.\n",
      "scraping 109 completed.\n",
      "scraping 609 started.\n",
      "scraping 111 completed.\n",
      "scraping 610 started.\n",
      "scraping 110 completed.\n",
      "scraping 611 started.\n",
      "scraping 112 completed.\n",
      "scraping 612 started.\n",
      "scraping 113 completed.\n",
      "scraping 613 started.\n",
      "scraping 114 completed.\n",
      "scraping 614 started.\n",
      "scraping 116 completed.\n",
      "scraping 615 started.\n",
      "scraping 115 completed.\n",
      "scraping 616 started.\n",
      "scraping 117 completed.\n",
      "scraping 617 started.\n",
      "scraping 120 completed.\n",
      "scraping 618 started.\n",
      "scraping 118 completed.\n",
      "scraping 619 started.\n",
      "scraping 119 completed.\n",
      "scraping 620 started.\n",
      "scraping 121 completed.\n",
      "scraping 621 started.\n",
      "scraping 123 completed.\n",
      "scraping 622 started.\n",
      "scraping 122 completed.\n",
      "scraping 623 started.\n",
      "scraping 124 completed.\n",
      "scraping 624 started.\n",
      "scraping 125 completed.\n",
      "scraping 625 started.\n",
      "scraping 126 completed.\n",
      "scraping 626 started.\n",
      "scraping 127 completed.\n",
      "scraping 627 started.\n",
      "scraping 128 completed.\n",
      "scraping 628 started.\n",
      "scraping 129 completed.\n",
      "scraping 629 started.\n",
      "scraping 131 completed.\n",
      "scraping 630 started.\n",
      "scraping 130 completed.\n",
      "scraping 631 started.\n",
      "scraping 133 completed.\n",
      "scraping 632 started.\n",
      "scraping 132 completed.\n",
      "scraping 633 started.\n",
      "scraping 134 completed.\n",
      "scraping 634 started.\n",
      "scraping 135 completed.\n",
      "scraping 635 started.\n",
      "scraping 136 completed.\n",
      "scraping 636 started.\n",
      "scraping 137 completed.\n",
      "scraping 637 started.\n",
      "scraping 139 completed.\n",
      "scraping 638 started.\n",
      "scraping 138 completed.\n",
      "scraping 639 started.\n",
      "scraping 141 completed.\n",
      "scraping 640 started.\n",
      "scraping 140 completed.\n",
      "scraping 641 started.\n",
      "scraping 142 completed.\n",
      "scraping 642 started.\n",
      "scraping 143 completed.\n",
      "scraping 643 started.\n",
      "scraping 144 completed.\n",
      "scraping 644 started.\n",
      "scraping 145 completed.\n",
      "scraping 645 started.\n",
      "scraping 146 completed.\n",
      "scraping 646 started.\n",
      "scraping 147 completed.\n",
      "scraping 647 started.\n",
      "scraping 148 completed.\n",
      "scraping 648 started.\n",
      "scraping 149 completed.\n",
      "scraping 649 started.\n",
      "scraping 150 completed.\n",
      "scraping 650 started.\n",
      "scraping 151 completed.\n",
      "scraping 651 started.\n",
      "scraping 152 completed.\n",
      "scraping 652 started.\n",
      "scraping 153 completed.\n",
      "scraping 653 started.\n",
      "scraping 154 completed.\n",
      "scraping 654 started.\n",
      "scraping 155 completed.\n",
      "scraping 655 started.\n",
      "scraping 157 completed.\n",
      "scraping 656 started.\n",
      "scraping 156 completed.\n",
      "scraping 657 started.\n",
      "scraping 158 completed.\n",
      "scraping 658 started.\n",
      "scraping 159 completed.\n",
      "scraping 659 started.\n",
      "scraping 161 completed.\n",
      "scraping 660 started.\n",
      "scraping 160 completed.\n",
      "scraping 661 started.\n",
      "scraping 163 completed.\n",
      "scraping 662 started.\n",
      "scraping 164 completed.\n",
      "scraping 663 started.\n",
      "scraping 162 completed.\n",
      "scraping 664 started.\n",
      "scraping 165 completed.\n",
      "scraping 665 started.\n",
      "scraping 167 completed.\n",
      "scraping 666 started.\n",
      "scraping 166 completed.\n",
      "scraping 667 started.\n",
      "scraping 168 completed.\n",
      "scraping 668 started.\n",
      "scraping 169 completed.\n",
      "scraping 669 started.\n",
      "scraping 171 completed.\n",
      "scraping 670 started.\n",
      "scraping 170 completed.\n",
      "scraping 671 started.\n",
      "scraping 172 completed.\n",
      "scraping 672 started.\n",
      "scraping 174 completed.\n",
      "scraping 673 started.\n",
      "scraping 173 completed.\n",
      "scraping 674 started.\n",
      "scraping 175 completed.\n",
      "scraping 675 started.\n",
      "scraping 176 completed.\n",
      "scraping 676 started.\n",
      "scraping 177 completed.\n",
      "scraping 677 started.\n",
      "scraping 178 completed.\n",
      "scraping 678 started.\n",
      "scraping 179 completed.\n",
      "scraping 679 started.\n",
      "scraping 180 completed.\n",
      "scraping 680 started.\n",
      "scraping 181 completed.\n",
      "scraping 681 started.\n",
      "scraping 182 completed.\n",
      "scraping 682 started.\n",
      "scraping 183 completed.\n",
      "scraping 683 started.\n",
      "scraping 184 completed.\n",
      "scraping 684 started.\n",
      "scraping 185 completed.\n",
      "scraping 685 started.\n",
      "scraping 186 completed.\n",
      "scraping 686 started.\n",
      "scraping 188 completed.\n",
      "scraping 687 started.\n",
      "scraping 187 completed.\n",
      "scraping 688 started.\n",
      "scraping 189 completed.\n",
      "scraping 689 started.\n",
      "scraping 191 completed.\n",
      "scraping 690 started.\n",
      "scraping 190 completed.\n",
      "scraping 691 started.\n",
      "scraping 192 completed.\n",
      "scraping 692 started.\n",
      "scraping 193 completed.\n",
      "scraping 693 started.\n",
      "scraping 194 completed.\n",
      "scraping 694 started.\n",
      "scraping 197 completed.\n",
      "scraping 695 started.\n",
      "scraping 195 completed.\n",
      "scraping 696 started.\n",
      "scraping 196 completed.\n",
      "scraping 697 started.\n",
      "scraping 198 completed.\n",
      "scraping 698 started.\n",
      "scraping 200 completed.\n",
      "scraping 699 started.\n",
      "scraping 199 completed.\n",
      "scraping 700 started.\n",
      "scraping 201 completed.\n",
      "scraping 701 started.\n",
      "scraping 202 completed.\n",
      "scraping 702 started.\n",
      "scraping 204 completed.\n",
      "scraping 703 started.\n",
      "scraping 203 completed.\n",
      "scraping 704 started.\n",
      "scraping 205 completed.\n",
      "scraping 705 started.\n",
      "scraping 206 completed.\n",
      "scraping 706 started.\n",
      "scraping 207 completed.\n",
      "scraping 707 started.\n",
      "scraping 209 completed.\n",
      "scraping 708 started.\n",
      "scraping 208 completed.\n",
      "scraping 709 started.\n",
      "scraping 210 completed.\n",
      "scraping 710 started.\n",
      "scraping 212 completed.\n",
      "scraping 711 started.\n",
      "scraping 211 completed.\n",
      "scraping 712 started.\n",
      "scraping 214 completed.\n",
      "scraping 713 started.\n",
      "scraping 213 completed.\n",
      "scraping 714 started.\n",
      "scraping 215 completed.\n",
      "scraping 715 started.\n",
      "scraping 216 completed.\n",
      "scraping 716 started.\n",
      "scraping 217 completed.\n",
      "scraping 717 started.\n",
      "scraping 218 completed.\n",
      "scraping 718 started.\n",
      "scraping 219 completed.\n",
      "scraping 719 started.\n",
      "scraping 220 completed.\n",
      "scraping 720 started.\n",
      "scraping 221 completed.\n",
      "scraping 721 started.\n",
      "scraping 223 completed.\n",
      "scraping 722 started.\n",
      "scraping 222 completed.\n",
      "scraping 723 started.\n",
      "scraping 224 completed.\n",
      "scraping 724 started.\n",
      "scraping 225 completed.\n",
      "scraping 725 started.\n",
      "scraping 226 completed.\n",
      "scraping 726 started.\n",
      "scraping 227 completed.\n",
      "scraping 727 started.\n",
      "scraping 228 completed.\n",
      "scraping 728 started.\n",
      "scraping 229 completed.\n",
      "scraping 729 started.\n",
      "scraping 230 completed.\n",
      "scraping 730 started.\n",
      "scraping 231 completed.\n",
      "scraping 731 started.\n",
      "scraping 232 completed.\n",
      "scraping 732 started.\n",
      "scraping 233 completed.\n",
      "scraping 733 started.\n",
      "scraping 234 completed.\n",
      "scraping 734 started.\n",
      "scraping 235 completed.\n",
      "scraping 735 started.\n",
      "scraping 236 completed.\n",
      "scraping 736 started.\n",
      "scraping 237 completed.\n",
      "scraping 737 started.\n",
      "scraping 240 completed.\n",
      "scraping 738 started.\n",
      "scraping 239 completed.\n",
      "scraping 739 started.\n",
      "scraping 238 completed.\n",
      "scraping 740 started.\n",
      "scraping 241 completed.\n",
      "scraping 741 started.\n",
      "scraping 242 completed.\n",
      "scraping 742 started.\n",
      "scraping 243 completed.\n",
      "scraping 743 started.\n",
      "scraping 244 completed.\n",
      "scraping 744 started.\n",
      "scraping 246 completed.\n",
      "scraping 745 started.\n",
      "scraping 245 completed.\n",
      "scraping 746 started.\n",
      "scraping 247 completed.\n",
      "scraping 747 started.\n",
      "scraping 249 completed.\n",
      "scraping 748 started.\n",
      "scraping 248 completed.\n",
      "scraping 749 started.\n",
      "scraping 250 completed.\n",
      "scraping 750 started.\n",
      "scraping 251 completed.\n",
      "scraping 751 started.\n",
      "scraping 252 completed.\n",
      "scraping 752 started.\n",
      "scraping 253 completed.\n",
      "scraping 753 started.\n",
      "scraping 254 completed.\n",
      "scraping 754 started.\n",
      "scraping 255 completed.\n",
      "scraping 755 started.\n",
      "scraping 256 completed.\n",
      "scraping 756 started.\n",
      "scraping 257 completed.\n",
      "scraping 757 started.\n",
      "scraping 259 completed.\n",
      "scraping 758 started.\n",
      "scraping 258 completed.\n",
      "scraping 759 started.\n",
      "scraping 260 completed.\n",
      "scraping 760 started.\n",
      "scraping 261 completed.\n",
      "scraping 761 started.\n",
      "scraping 263 completed.\n",
      "scraping 762 started.\n",
      "scraping 262 completed.\n",
      "scraping 763 started.\n",
      "scraping 264 completed.\n",
      "scraping 764 started.\n",
      "scraping 266 completed.\n",
      "scraping 765 started.\n",
      "scraping 265 completed.\n",
      "scraping 766 started.\n",
      "scraping 268 completed.\n",
      "scraping 767 started.\n",
      "scraping 267 completed.\n",
      "scraping 768 started.\n",
      "scraping 269 completed.\n",
      "scraping 769 started.\n",
      "scraping 270 completed.\n",
      "scraping 770 started.\n",
      "scraping 271 completed.\n",
      "scraping 771 started.\n",
      "scraping 273 completed.\n",
      "scraping 772 started.\n",
      "scraping 274 completed.\n",
      "scraping 773 started.\n",
      "scraping 272 completed.\n",
      "scraping 774 started.\n",
      "scraping 275 completed.\n",
      "scraping 775 started.\n",
      "scraping 276 completed.\n",
      "scraping 776 started.\n",
      "scraping 279 completed.\n",
      "scraping 777 started.\n",
      "scraping 278 completed.\n",
      "scraping 778 started.\n",
      "scraping 277 completed.\n",
      "scraping 779 started.\n",
      "scraping 280 completed.\n",
      "scraping 780 started.\n",
      "scraping 282 completed.\n",
      "scraping 781 started.\n",
      "scraping 283 completed.\n",
      "scraping 782 started.\n",
      "scraping 281 completed.\n",
      "scraping 783 started.\n",
      "scraping 284 completed.\n",
      "scraping 784 started.\n",
      "scraping 285 completed.\n",
      "scraping 785 started.\n",
      "scraping 286 completed.\n",
      "scraping 786 started.\n",
      "scraping 287 completed.\n",
      "scraping 787 started.\n",
      "scraping 288 completed.\n",
      "scraping 788 started.\n",
      "scraping 289 completed.\n",
      "scraping 789 started.\n",
      "scraping 290 completed.\n",
      "scraping 790 started.\n",
      "scraping 291 completed.\n",
      "scraping 791 started.\n",
      "scraping 292 completed.\n",
      "scraping 792 started.\n",
      "scraping 294 completed.\n",
      "scraping 793 started.\n",
      "scraping 293 completed.\n",
      "scraping 794 started.\n",
      "scraping 295 completed.\n",
      "scraping 795 started.\n",
      "scraping 297 completed.\n",
      "scraping 796 started.\n",
      "scraping 296 completed.\n",
      "scraping 797 started.\n",
      "scraping 298 completed.\n",
      "scraping 798 started.\n",
      "scraping 299 completed.\n",
      "scraping 799 started.\n",
      "scraping 301 completed.\n",
      "scraping 800 started.\n",
      "scraping 300 completed.\n",
      "scraping 801 started.\n",
      "scraping 303 completed.\n",
      "scraping 802 started.\n",
      "scraping 302 completed.\n",
      "scraping 803 started.\n",
      "scraping 305 completed.\n",
      "scraping 804 started.\n",
      "scraping 304 completed.\n",
      "scraping 805 started.\n",
      "scraping 306 completed.\n",
      "scraping 806 started.\n",
      "scraping 307 completed.\n",
      "scraping 807 started.\n",
      "scraping 308 completed.\n",
      "scraping 808 started.\n",
      "scraping 309 completed.\n",
      "scraping 809 started.\n",
      "scraping 312 completed.\n",
      "scraping 810 started.\n",
      "scraping 311 completed.\n",
      "scraping 811 started.\n",
      "scraping 310 completed.\n",
      "scraping 812 started.\n",
      "scraping 313 completed.\n",
      "scraping 813 started.\n",
      "scraping 314 completed.\n",
      "scraping 814 started.\n",
      "scraping 315 completed.\n",
      "scraping 815 started.\n",
      "scraping 316 completed.\n",
      "scraping 816 started.\n",
      "scraping 317 completed.\n",
      "scraping 817 started.\n",
      "scraping 318 completed.\n",
      "scraping 818 started.\n",
      "scraping 319 completed.\n",
      "scraping 819 started.\n",
      "scraping 320 completed.\n",
      "scraping 820 started.\n",
      "scraping 321 completed.\n",
      "scraping 821 started.\n",
      "scraping 322 completed.\n",
      "scraping 822 started.\n",
      "scraping 323 completed.\n",
      "scraping 823 started.\n",
      "scraping 324 completed.\n",
      "scraping 824 started.\n",
      "scraping 325 completed.\n",
      "scraping 825 started.\n",
      "scraping 327 completed.\n",
      "scraping 826 started.\n",
      "scraping 326 completed.\n",
      "scraping 827 started.\n",
      "scraping 329 completed.\n",
      "scraping 828 started.\n",
      "scraping 328 completed.\n",
      "scraping 829 started.\n",
      "scraping 332 completed.\n",
      "scraping 830 started.\n",
      "scraping 331 completed.\n",
      "scraping 831 started.\n",
      "scraping 330 completed.\n",
      "scraping 832 started.\n",
      "scraping 333 completed.\n",
      "scraping 833 started.\n",
      "scraping 335 completed.\n",
      "scraping 834 started.\n",
      "scraping 334 completed.\n",
      "scraping 835 started.\n",
      "scraping 336 completed.\n",
      "scraping 836 started.\n",
      "scraping 337 completed.\n",
      "scraping 837 started.\n",
      "scraping 338 completed.\n",
      "scraping 838 started.\n",
      "scraping 339 completed.\n",
      "scraping 839 started.\n",
      "scraping 340 completed.\n",
      "scraping 840 started.\n",
      "scraping 341 completed.\n",
      "scraping 841 started.\n",
      "scraping 342 completed.\n",
      "scraping 842 started.\n",
      "scraping 344 completed.\n",
      "scraping 843 started.\n",
      "scraping 343 completed.\n",
      "scraping 844 started.\n",
      "scraping 346 completed.\n",
      "scraping 845 started.\n",
      "scraping 345 completed.\n",
      "scraping 846 started.\n",
      "scraping 347 completed.\n",
      "scraping 847 started.\n",
      "scraping 349 completed.\n",
      "scraping 848 started.\n",
      "scraping 348 completed.\n",
      "scraping 849 started.\n",
      "scraping 351 completed.\n",
      "scraping 850 started.\n",
      "scraping 350 completed.\n",
      "scraping 851 started.\n",
      "scraping 353 completed.\n",
      "scraping 852 started.\n",
      "scraping 352 completed.\n",
      "scraping 853 started.\n",
      "scraping 354 completed.\n",
      "scraping 854 started.\n",
      "scraping 355 completed.\n",
      "scraping 855 started.\n",
      "scraping 356 completed.\n",
      "scraping 856 started.\n",
      "scraping 357 completed.\n",
      "scraping 857 started.\n",
      "scraping 358 completed.\n",
      "scraping 858 started.\n",
      "scraping 359 completed.\n",
      "scraping 859 started.\n",
      "scraping 361 completed.\n",
      "scraping 860 started.\n",
      "scraping 360 completed.\n",
      "scraping 861 started.\n",
      "scraping 362 completed.\n",
      "scraping 862 started.\n",
      "scraping 363 completed.\n",
      "scraping 863 started.\n",
      "scraping 365 completed.\n",
      "scraping 864 started.\n",
      "scraping 366 completed.\n",
      "scraping 865 started.\n",
      "scraping 364 completed.\n",
      "scraping 866 started.\n",
      "scraping 367 completed.\n",
      "scraping 867 started.\n",
      "scraping 368 completed.\n",
      "scraping 868 started.\n",
      "scraping 369 completed.\n",
      "scraping 869 started.\n",
      "scraping 370 completed.\n",
      "scraping 870 started.\n",
      "scraping 371 completed.\n",
      "scraping 871 started.\n",
      "scraping 373 completed.\n",
      "scraping 872 started.\n",
      "scraping 372 completed.\n",
      "scraping 873 started.\n",
      "scraping 374 completed.\n",
      "scraping 874 started.\n",
      "scraping 375 completed.\n",
      "scraping 875 started.\n",
      "scraping 377 completed.\n",
      "scraping 876 started.\n",
      "scraping 376 completed.\n",
      "scraping 877 started.\n",
      "scraping 379 completed.\n",
      "scraping 878 started.\n",
      "scraping 378 completed.\n",
      "scraping 879 started.\n",
      "scraping 382 completed.\n",
      "scraping 880 started.\n",
      "scraping 381 completed.\n",
      "scraping 881 started.\n",
      "scraping 380 completed.\n",
      "scraping 882 started.\n",
      "scraping 383 completed.\n",
      "scraping 883 started.\n",
      "scraping 385 completed.\n",
      "scraping 884 started.\n",
      "scraping 384 completed.\n",
      "scraping 885 started.\n",
      "scraping 386 completed.\n",
      "scraping 886 started.\n",
      "scraping 387 completed.\n",
      "scraping 887 started.\n",
      "scraping 388 completed.\n",
      "scraping 888 started.\n",
      "scraping 389 completed.\n",
      "scraping 889 started.\n",
      "scraping 390 completed.\n",
      "scraping 890 started.\n",
      "scraping 391 completed.\n",
      "scraping 891 started.\n",
      "scraping 392 completed.\n",
      "scraping 892 started.\n",
      "scraping 393 completed.\n",
      "scraping 893 started.\n",
      "scraping 394 completed.\n",
      "scraping 894 started.\n",
      "scraping 395 completed.\n",
      "scraping 895 started.\n",
      "scraping 396 completed.\n",
      "scraping 896 started.\n",
      "scraping 397 completed.\n",
      "scraping 897 started.\n",
      "scraping 399 completed.\n",
      "scraping 898 started.\n",
      "scraping 398 completed.\n",
      "scraping 899 started.\n",
      "scraping 401 completed.\n",
      "scraping 900 started.\n",
      "scraping 400 completed.\n",
      "scraping 901 started.\n",
      "scraping 403 completed.\n",
      "scraping 902 started.\n",
      "scraping 402 completed.\n",
      "scraping 903 started.\n",
      "scraping 405 completed.\n",
      "scraping 904 started.\n",
      "scraping 406 completed.\n",
      "scraping 905 started.\n",
      "scraping 404 completed.\n",
      "scraping 906 started.\n",
      "scraping 407 completed.\n",
      "scraping 907 started.\n",
      "scraping 409 completed.\n",
      "scraping 908 started.\n",
      "scraping 408 completed.\n",
      "scraping 909 started.\n",
      "scraping 411 completed.\n",
      "scraping 910 started.\n",
      "scraping 410 completed.\n",
      "scraping 911 started.\n",
      "scraping 413 completed.\n",
      "scraping 912 started.\n",
      "scraping 412 completed.\n",
      "scraping 913 started.\n",
      "scraping 415 completed.\n",
      "scraping 914 started.\n",
      "scraping 414 completed.\n",
      "scraping 915 started.\n",
      "scraping 416 completed.\n",
      "scraping 916 started.\n",
      "scraping 418 completed.\n",
      "scraping 917 started.\n",
      "scraping 417 completed.\n",
      "scraping 918 started.\n",
      "scraping 420 completed.\n",
      "scraping 919 started.\n",
      "scraping 419 completed.\n",
      "scraping 920 started.\n",
      "scraping 422 completed.\n",
      "scraping 921 started.\n",
      "scraping 421 completed.\n",
      "scraping 922 started.\n",
      "scraping 424 completed.\n",
      "scraping 923 started.\n",
      "scraping 423 completed.\n",
      "scraping 924 started.\n",
      "scraping 425 completed.\n",
      "scraping 925 started.\n",
      "scraping 426 completed.\n",
      "scraping 926 started.\n",
      "scraping 428 completed.\n",
      "scraping 927 started.\n",
      "scraping 427 completed.\n",
      "scraping 928 started.\n",
      "scraping 429 completed.\n",
      "scraping 929 started.\n",
      "scraping 430 completed.\n",
      "scraping 930 started.\n",
      "scraping 432 completed.\n",
      "scraping 931 started.\n",
      "scraping 433 completed.\n",
      "scraping 932 started.\n",
      "scraping 431 completed.\n",
      "scraping 933 started.\n",
      "scraping 434 completed.\n",
      "scraping 934 started.\n",
      "scraping 436 completed.\n",
      "scraping 935 started.\n",
      "scraping 435 completed.\n",
      "scraping 936 started.\n",
      "scraping 437 completed.\n",
      "scraping 937 started.\n",
      "scraping 438 completed.\n",
      "scraping 938 started.\n",
      "scraping 439 completed.\n",
      "scraping 939 started.\n",
      "scraping 440 completed.\n",
      "scraping 940 started.\n",
      "scraping 442 completed.\n",
      "scraping 941 started.\n",
      "scraping 441 completed.\n",
      "scraping 942 started.\n",
      "scraping 443 completed.\n",
      "scraping 943 started.\n",
      "scraping 444 completed.\n",
      "scraping 944 started.\n",
      "scraping 445 completed.\n",
      "scraping 945 started.\n",
      "scraping 447 completed.\n",
      "scraping 946 started.\n",
      "scraping 446 completed.\n",
      "scraping 947 started.\n",
      "scraping 449 completed.\n",
      "scraping 948 started.\n",
      "scraping 448 completed.\n",
      "scraping 949 started.\n",
      "scraping 451 completed.\n",
      "scraping 950 started.\n",
      "scraping 450 completed.\n",
      "scraping 951 started.\n",
      "scraping 453 completed.\n",
      "scraping 952 started.\n",
      "scraping 454 completed.\n",
      "scraping 953 started.\n",
      "scraping 452 completed.\n",
      "scraping 954 started.\n",
      "scraping 456 completed.\n",
      "scraping 955 started.\n",
      "scraping 455 completed.\n",
      "scraping 956 started.\n",
      "scraping 458 completed.\n",
      "scraping 957 started.\n",
      "scraping 457 completed.\n",
      "scraping 958 started.\n",
      "scraping 459 completed.\n",
      "scraping 959 started.\n",
      "scraping 460 completed.\n",
      "scraping 960 started.\n",
      "scraping 461 completed.\n",
      "scraping 961 started.\n",
      "scraping 463 completed.\n",
      "scraping 962 started.\n",
      "scraping 462 completed.\n",
      "scraping 963 started.\n",
      "scraping 464 completed.\n",
      "scraping 964 started.\n",
      "scraping 465 completed.\n",
      "scraping 965 started.\n",
      "scraping 467 completed.\n",
      "scraping 966 started.\n",
      "scraping 466 completed.\n",
      "scraping 967 started.\n",
      "scraping 468 completed.\n",
      "scraping 968 started.\n",
      "scraping 469 completed.\n",
      "scraping 969 started.\n",
      "scraping 470 completed.\n",
      "scraping 970 started.\n",
      "scraping 471 completed.\n",
      "scraping 971 started.\n",
      "scraping 472 completed.\n",
      "scraping 972 started.\n",
      "scraping 473 completed.\n",
      "scraping 973 started.\n",
      "scraping 475 completed.\n",
      "scraping 974 started.\n",
      "scraping 474 completed.\n",
      "scraping 975 started.\n",
      "scraping 476 completed.\n",
      "scraping 976 started.\n",
      "scraping 477 completed.\n",
      "scraping 977 started.\n",
      "scraping 478 completed.\n",
      "scraping 978 started.\n",
      "scraping 480 completed.\n",
      "scraping 979 started.\n",
      "scraping 479 completed.\n",
      "scraping 980 started.\n",
      "scraping 481 completed.\n",
      "scraping 981 started.\n",
      "scraping 483 completed.\n",
      "scraping 982 started.\n",
      "scraping 482 completed.\n",
      "scraping 983 started.\n",
      "scraping 484 completed.\n",
      "scraping 984 started.\n",
      "scraping 485 completed.\n",
      "scraping 985 started.\n",
      "scraping 487 completed.\n",
      "scraping 986 started.\n",
      "scraping 486 completed.\n",
      "scraping 987 started.\n",
      "scraping 488 completed.\n",
      "scraping 988 started.\n",
      "scraping 489 completed.\n",
      "scraping 989 started.\n",
      "scraping 490 completed.\n",
      "scraping 990 started.\n",
      "scraping 492 completed.\n",
      "scraping 991 started.\n",
      "scraping 491 completed.\n",
      "scraping 992 started.\n",
      "scraping 493 completed.\n",
      "scraping 993 started.\n",
      "scraping 494 completed.\n",
      "scraping 994 started.\n",
      "scraping 495 completed.\n",
      "scraping 995 started.\n",
      "scraping 496 completed.\n",
      "scraping 996 started.\n",
      "scraping 497 completed.\n",
      "scraping 997 started.\n",
      "scraping 498 completed.\n",
      "scraping 998 started.\n",
      "scraping 499 completed.\n",
      "scraping 999 started.\n",
      "scraping 500 completed.\n",
      "scraping 1000 started.\n",
      "scraping 501 completed.\n",
      "scraping 502 completed.\n",
      "scraping 503 completed.\n",
      "scraping 504 completed.\n",
      "scraping 506 completed.\n",
      "scraping 505 completed.\n",
      "scraping 508 completed.\n",
      "scraping 507 completed.\n",
      "scraping 509 completed.\n",
      "scraping 510 completed.\n",
      "scraping 514 completed.\n",
      "scraping 513 completed.\n",
      "scraping 511 completed.\n",
      "scraping 512 completed.\n",
      "scraping 515 completed.\n",
      "scraping 516 completed.\n",
      "scraping 519 completed.\n",
      "scraping 517 completed.\n",
      "scraping 518 completed.\n",
      "scraping 520 completed.\n",
      "scraping 521 completed.\n",
      "scraping 523 completed.\n",
      "scraping 522 completed.\n",
      "scraping 526 completed.\n",
      "scraping 525 completed.\n",
      "scraping 524 completed.\n",
      "scraping 528 completed.\n",
      "scraping 527 completed.\n",
      "scraping 529 completed.\n",
      "scraping 530 completed.\n",
      "scraping 531 completed.\n",
      "scraping 532 completed.\n",
      "scraping 534 completed.\n",
      "scraping 533 completed.\n",
      "scraping 535 completed.\n",
      "scraping 536 completed.\n",
      "scraping 537 completed.\n",
      "scraping 538 completed.\n",
      "scraping 539 completed.\n",
      "scraping 542 completed.\n",
      "scraping 541 completed.\n",
      "scraping 540 completed.\n",
      "scraping 544 completed.\n",
      "scraping 543 completed.\n",
      "scraping 546 completed.\n",
      "scraping 545 completed.\n",
      "scraping 548 completed.\n",
      "scraping 547 completed.\n",
      "scraping 549 completed.\n",
      "scraping 550 completed.\n",
      "scraping 552 completed.\n",
      "scraping 551 completed.\n",
      "scraping 553 completed.\n",
      "scraping 556 completed.\n",
      "scraping 555 completed.\n",
      "scraping 554 completed.\n",
      "scraping 557 completed.\n",
      "scraping 558 completed.\n",
      "scraping 559 completed.\n",
      "scraping 561 completed.\n",
      "scraping 560 completed.\n",
      "scraping 562 completed.\n",
      "scraping 563 completed.\n",
      "scraping 564 completed.\n",
      "scraping 565 completed.\n",
      "scraping 566 completed.\n",
      "scraping 567 completed.\n",
      "scraping 568 completed.\n",
      "scraping 569 completed.\n",
      "scraping 570 completed.\n",
      "scraping 571 completed.\n",
      "scraping 572 completed.\n",
      "scraping 574 completed.\n",
      "scraping 573 completed.\n",
      "scraping 575 completed.\n",
      "scraping 576 completed.\n",
      "scraping 577 completed.\n",
      "scraping 579 completed.\n",
      "scraping 578 completed.\n",
      "scraping 580 completed.\n",
      "scraping 582 completed.\n",
      "scraping 581 completed.\n",
      "scraping 583 completed.\n",
      "scraping 584 completed.\n",
      "scraping 585 completed.\n",
      "scraping 586 completed.\n",
      "scraping 588 completed.\n",
      "scraping 587 completed.\n",
      "scraping 589 completed.\n",
      "scraping 590 completed.\n",
      "scraping 592 completed.\n",
      "scraping 591 completed.\n",
      "scraping 593 completed.\n",
      "scraping 594 completed.\n",
      "scraping 595 completed.\n",
      "scraping 596 completed.\n",
      "scraping 598 completed.\n",
      "scraping 597 completed.\n",
      "scraping 599 completed.\n",
      "scraping 601 completed.\n",
      "scraping 600 completed.\n",
      "scraping 602 completed.\n",
      "scraping 603 completed.\n",
      "scraping 604 completed.\n",
      "scraping 605 completed.\n",
      "scraping 606 completed.\n",
      "scraping 609 completed.\n",
      "scraping 607 completed.\n",
      "scraping 610 completed.\n",
      "scraping 611 completed.\n",
      "scraping 613 completed.\n",
      "scraping 612 completed.\n",
      "scraping 614 completed.\n",
      "scraping 608 completed.\n",
      "scraping 616 completed.\n",
      "scraping 615 completed.\n",
      "scraping 617 completed.\n",
      "scraping 621 completed.\n",
      "scraping 619 completed.\n",
      "scraping 618 completed.\n",
      "scraping 620 completed.\n",
      "scraping 622 completed.\n",
      "scraping 623 completed.\n",
      "scraping 625 completed.\n",
      "scraping 624 completed.\n",
      "scraping 626 completed.\n",
      "scraping 627 completed.\n",
      "scraping 628 completed.\n",
      "scraping 629 completed.\n",
      "scraping 630 completed.\n",
      "scraping 631 completed.\n",
      "scraping 632 completed.\n",
      "scraping 633 completed.\n",
      "scraping 634 completed.\n",
      "scraping 635 completed.\n",
      "scraping 636 completed.\n",
      "scraping 637 completed.\n",
      "scraping 638 completed.\n",
      "scraping 639 completed.\n",
      "scraping 641 completed.\n",
      "scraping 643 completed.\n",
      "scraping 642 completed.\n",
      "scraping 640 completed.\n",
      "scraping 644 completed.\n",
      "scraping 645 completed.\n",
      "scraping 646 completed.\n",
      "scraping 647 completed.\n",
      "scraping 649 completed.\n",
      "scraping 648 completed.\n",
      "scraping 650 completed.\n",
      "scraping 651 completed.\n",
      "scraping 652 completed.\n",
      "scraping 653 completed.\n",
      "scraping 654 completed.\n",
      "scraping 655 completed.\n",
      "scraping 657 completed.\n",
      "scraping 656 completed.\n",
      "scraping 660 completed.\n",
      "scraping 658 completed.\n",
      "scraping 659 completed.\n",
      "scraping 661 completed.\n",
      "scraping 662 completed.\n",
      "scraping 663 completed.\n",
      "scraping 664 completed.\n",
      "scraping 665 completed.\n",
      "scraping 667 completed.\n",
      "scraping 666 completed.\n",
      "scraping 669 completed.\n",
      "scraping 668 completed.\n",
      "scraping 671 completed.\n",
      "scraping 670 completed.\n",
      "scraping 672 completed.\n",
      "scraping 673 completed.\n",
      "scraping 674 completed.\n",
      "scraping 675 completed.\n",
      "scraping 677 completed.\n",
      "scraping 676 completed.\n",
      "scraping 678 completed.\n",
      "scraping 679 completed.\n",
      "scraping 681 completed.\n",
      "scraping 680 completed.\n",
      "scraping 682 completed.\n",
      "scraping 683 completed.\n",
      "scraping 684 completed.\n",
      "scraping 685 completed.\n",
      "scraping 686 completed.\n",
      "scraping 687 completed.\n",
      "scraping 688 completed.\n",
      "scraping 689 completed.\n",
      "scraping 691 completed.\n",
      "scraping 690 completed.\n",
      "scraping 692 completed.\n",
      "scraping 694 completed.\n",
      "scraping 693 completed.\n",
      "scraping 697 completed.\n",
      "scraping 696 completed.\n",
      "scraping 695 completed.\n",
      "scraping 698 completed.\n",
      "scraping 699 completed.\n",
      "scraping 700 completed.\n",
      "scraping 701 completed.\n",
      "scraping 702 completed.\n",
      "scraping 703 completed.\n",
      "scraping 705 completed.\n",
      "scraping 706 completed.\n",
      "scraping 704 completed.\n",
      "scraping 707 completed.\n",
      "scraping 710 completed.\n",
      "scraping 708 completed.\n",
      "scraping 709 completed.\n",
      "scraping 712 completed.\n",
      "scraping 711 completed.\n",
      "scraping 713 completed.\n",
      "scraping 714 completed.\n",
      "scraping 715 completed.\n",
      "scraping 716 completed.\n",
      "scraping 717 completed.\n",
      "scraping 718 completed.\n",
      "scraping 719 completed.\n",
      "scraping 721 completed.\n",
      "scraping 720 completed.\n",
      "scraping 722 completed.\n",
      "scraping 723 completed.\n",
      "scraping 724 completed.\n",
      "scraping 725 completed.\n",
      "scraping 727 completed.\n",
      "scraping 726 completed.\n",
      "scraping 728 completed.\n",
      "scraping 729 completed.\n",
      "scraping 730 completed.\n",
      "scraping 731 completed.\n",
      "scraping 732 completed.\n",
      "scraping 733 completed.\n",
      "scraping 735 completed.\n",
      "scraping 734 completed.\n",
      "scraping 737 completed.\n",
      "scraping 736 completed.\n",
      "scraping 739 completed.\n",
      "scraping 738 completed.\n",
      "scraping 740 completed.\n",
      "scraping 742 completed.\n",
      "scraping 741 completed.\n",
      "scraping 744 completed.\n",
      "scraping 743 completed.\n",
      "scraping 745 completed.\n",
      "scraping 746 completed.\n",
      "scraping 747 completed.\n",
      "scraping 748 completed.\n",
      "scraping 749 completed.\n",
      "scraping 750 completed.\n",
      "scraping 751 completed.\n",
      "scraping 752 completed.\n",
      "scraping 753 completed.\n",
      "scraping 754 completed.\n",
      "scraping 756 completed.\n",
      "scraping 755 completed.\n",
      "scraping 757 completed.\n",
      "scraping 758 completed.\n",
      "scraping 759 completed.\n",
      "scraping 761 completed.\n",
      "scraping 760 completed.\n",
      "scraping 764 completed.\n",
      "scraping 762 completed.\n",
      "scraping 763 completed.\n",
      "scraping 765 completed.\n",
      "scraping 766 completed.\n",
      "scraping 768 completed.\n",
      "scraping 767 completed.\n",
      "scraping 770 completed.\n",
      "scraping 769 completed.\n",
      "scraping 771 completed.\n",
      "scraping 772 completed.\n",
      "scraping 773 completed.\n",
      "scraping 775 completed.\n",
      "scraping 774 completed.\n",
      "scraping 776 completed.\n",
      "scraping 777 completed.\n",
      "scraping 780 completed.\n",
      "scraping 778 completed.\n",
      "scraping 779 completed.\n",
      "scraping 782 completed.\n",
      "scraping 781 completed.\n",
      "scraping 784 completed.\n",
      "scraping 783 completed.\n",
      "scraping 788 completed.\n",
      "scraping 786 completed.\n",
      "scraping 785 completed.\n",
      "scraping 787 completed.\n",
      "scraping 790 completed.\n",
      "scraping 789 completed.\n",
      "scraping 792 completed.\n",
      "scraping 791 completed.\n",
      "scraping 793 completed.\n",
      "scraping 794 completed.\n",
      "scraping 795 completed.\n",
      "scraping 797 completed.\n",
      "scraping 796 completed.\n",
      "scraping 798 completed.\n",
      "scraping 799 completed.\n",
      "scraping 800 completed.\n",
      "scraping 801 completed.\n",
      "scraping 802 completed.\n",
      "scraping 803 completed.\n",
      "scraping 804 completed.\n",
      "scraping 806 completed.\n",
      "scraping 805 completed.\n",
      "scraping 808 completed.\n",
      "scraping 807 completed.\n",
      "scraping 809 completed.\n",
      "scraping 810 completed.\n",
      "scraping 811 completed.\n",
      "scraping 812 completed.\n",
      "scraping 813 completed.\n",
      "scraping 814 completed.\n",
      "scraping 815 completed.\n",
      "scraping 818 completed.\n",
      "scraping 816 completed.\n",
      "scraping 817 completed.\n",
      "scraping 819 completed.\n",
      "scraping 821 completed.\n",
      "scraping 820 completed.\n",
      "scraping 823 completed.\n",
      "scraping 822 completed.\n",
      "scraping 824 completed.\n",
      "scraping 826 completed.\n",
      "scraping 825 completed.\n",
      "scraping 827 completed.\n",
      "scraping 829 completed.\n",
      "scraping 828 completed.\n",
      "scraping 830 completed.\n",
      "scraping 831 completed.\n",
      "scraping 833 completed.\n",
      "scraping 832 completed.\n",
      "scraping 835 completed.\n",
      "scraping 834 completed.\n",
      "scraping 836 completed.\n",
      "scraping 838 completed.\n",
      "scraping 837 completed.\n",
      "scraping 839 completed.\n",
      "scraping 840 completed.\n",
      "scraping 841 completed.\n",
      "scraping 842 completed.\n",
      "scraping 845 completed.\n",
      "scraping 848 completed.\n",
      "scraping 846 completed.\n",
      "scraping 844 completed.\n",
      "scraping 843 completed.\n",
      "scraping 849 completed.\n",
      "scraping 847 completed.\n",
      "scraping 850 completed.\n",
      "scraping 851 completed.\n",
      "scraping 852 completed.\n",
      "scraping 853 completed.\n",
      "scraping 855 completed.\n",
      "scraping 854 completed.\n",
      "scraping 856 completed.\n",
      "scraping 857 completed.\n",
      "scraping 858 completed.\n",
      "scraping 859 completed.\n",
      "scraping 861 completed.\n",
      "scraping 863 completed.\n",
      "scraping 862 completed.\n",
      "scraping 860 completed.\n",
      "scraping 865 completed.\n",
      "scraping 864 completed.\n",
      "scraping 866 completed.\n",
      "scraping 867 completed.\n",
      "scraping 868 completed.\n",
      "scraping 869 completed.\n",
      "scraping 870 completed.\n",
      "scraping 872 completed.\n",
      "scraping 871 completed.\n",
      "scraping 875 completed.\n",
      "scraping 874 completed.\n",
      "scraping 873 completed.\n",
      "scraping 876 completed.\n",
      "scraping 877 completed.\n",
      "scraping 878 completed.\n",
      "scraping 879 completed.\n",
      "scraping 882 completed.\n",
      "scraping 881 completed.\n",
      "scraping 880 completed.\n",
      "scraping 883 completed.\n",
      "scraping 885 completed.\n",
      "scraping 884 completed.\n",
      "scraping 886 completed.\n",
      "scraping 887 completed.\n",
      "scraping 888 completed.\n",
      "scraping 889 completed.\n",
      "scraping 890 completed.\n",
      "scraping 891 completed.\n",
      "scraping 892 completed.\n",
      "scraping 895 completed.\n",
      "scraping 893 completed.\n",
      "scraping 894 completed.\n",
      "scraping 898 completed.\n",
      "scraping 896 completed.\n",
      "scraping 897 completed.\n",
      "scraping 900 completed.\n",
      "scraping 899 completed.\n",
      "scraping 901 completed.\n",
      "scraping 902 completed.\n",
      "scraping 903 completed.\n",
      "scraping 905 completed.\n",
      "scraping 904 completed.\n",
      "scraping 908 completed.\n",
      "scraping 906 completed.\n",
      "scraping 907 completed.\n",
      "scraping 909 completed.\n",
      "scraping 910 completed.\n",
      "scraping 911 completed.\n",
      "scraping 912 completed.\n",
      "scraping 913 completed.\n",
      "scraping 915 completed.\n",
      "scraping 914 completed.\n",
      "scraping 917 completed.\n",
      "scraping 916 completed.\n",
      "scraping 920 completed.\n",
      "scraping 919 completed.\n",
      "scraping 918 completed.\n",
      "scraping 921 completed.\n",
      "scraping 922 completed.\n",
      "scraping 923 completed.\n",
      "scraping 924 completed.\n",
      "scraping 925 completed.\n",
      "scraping 926 completed.\n",
      "scraping 929 completed.\n",
      "scraping 927 completed.\n",
      "scraping 930 completed.\n",
      "scraping 928 completed.\n",
      "scraping 932 completed.\n",
      "scraping 931 completed.\n",
      "scraping 934 completed.\n",
      "scraping 935 completed.\n",
      "scraping 936 completed.\n",
      "scraping 938 completed.\n",
      "scraping 937 completed.\n",
      "scraping 933 completed.\n",
      "scraping 939 completed.\n",
      "scraping 940 completed.\n",
      "scraping 941 completed.\n",
      "scraping 943 completed.\n",
      "scraping 942 completed.\n",
      "scraping 944 completed.\n",
      "scraping 947 completed.\n",
      "scraping 945 completed.\n",
      "scraping 946 completed.\n",
      "scraping 950 completed.\n",
      "scraping 949 completed.\n",
      "scraping 948 completed.\n",
      "scraping 954 completed.\n",
      "scraping 956 completed.\n",
      "scraping 952 completed.\n",
      "scraping 951 completed.\n",
      "scraping 953 completed.\n",
      "scraping 955 completed.\n",
      "scraping 957 completed.\n",
      "scraping 960 completed.\n",
      "scraping 958 completed.\n",
      "scraping 959 completed.\n",
      "scraping 961 completed.\n",
      "scraping 964 completed.\n",
      "scraping 962 completed.\n",
      "scraping 963 completed.\n",
      "scraping 966 completed.\n",
      "scraping 965 completed.\n",
      "scraping 970 completed.\n",
      "scraping 969 completed.\n",
      "scraping 968 completed.\n",
      "scraping 967 completed.\n",
      "scraping 971 completed.\n",
      "scraping 972 completed.\n",
      "scraping 973 completed.\n",
      "scraping 975 completed.\n",
      "scraping 974 completed.\n",
      "scraping 976 completed.\n",
      "scraping 978 completed.\n",
      "scraping 977 completed.\n",
      "scraping 980 completed.\n",
      "scraping 979 completed.\n",
      "scraping 981 completed.\n",
      "scraping 983 completed.\n",
      "scraping 982 completed.\n",
      "scraping 984 completed.\n",
      "scraping 985 completed.\n",
      "scraping 986 completed.\n",
      "scraping 987 completed.\n",
      "scraping 988 completed.\n",
      "scraping 989 completed.\n",
      "scraping 990 completed.\n",
      "scraping 992 completed.\n",
      "scraping 991 completed.\n",
      "scraping 994 completed.\n",
      "scraping 993 completed.\n",
      "scraping 995 completed.\n",
      "scraping 997 completed.\n",
      "scraping 998 completed.\n",
      "scraping 996 completed.\n",
      "scraping 999 completed.\n",
      "scraping 1000 completed.\n",
      "\n",
      "Performance Analysis:\n",
      "Total execution time: 4.20 seconds\n",
      "CPU Usage: 5.9% increase\n",
      "Memory Usage: 4.0MB increase\n",
      "\n",
      "Thread Pool Analysis:\n",
      "- Number of workers: 500\n",
      "- Number of tasks: 1000\n",
      "- Average time per task: 0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Compare time execution based on the number of workers\n",
    "\"\"\"\n",
    "# TODO: Recreate previous code with the 500 workers for 1000\n",
    "# TODO: Analyze how your program manage to execute 500 workers at once\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Simulating an I/O-bound scraping with time.sleep()\n",
    "def io_bound_scraping(scraping_id):\n",
    "    print(f\"scraping {scraping_id} started.\")\n",
    "    time.sleep(2)  # Simulate I/O operation\n",
    "    print(f\"scraping {scraping_id} completed.\")\n",
    "\n",
    "# Using ThreadPoolExecutor with map for faster execution\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get initial CPU and memory usage\n",
    "    initial_cpu = psutil.cpu_percent()\n",
    "    initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Creating a ThreadPoolExecutor with 500 workers\n",
    "    with ThreadPoolExecutor(max_workers=500) as executor:\n",
    "        # Use map to run 1000 scraping tasks concurrently\n",
    "        executor.map(io_bound_scraping, range(1, 1001))\n",
    "    \n",
    "    # Get final CPU and memory usage\n",
    "    final_cpu = psutil.cpu_percent()\n",
    "    final_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance Analysis:\")\n",
    "    print(f\"Total execution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"CPU Usage: {final_cpu - initial_cpu:.1f}% increase\")\n",
    "    print(f\"Memory Usage: {final_memory - initial_memory:.1f}MB increase\")\n",
    "    print(f\"\\nThread Pool Analysis:\")\n",
    "    print(f\"- Number of workers: 500\")\n",
    "    print(f\"- Number of tasks: 1000\")\n",
    "    print(f\"- Average time per task: {execution_time/1000:.3f} seconds\")\n",
    "\n",
    "# Call the main function to run the scrapings\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 started with 1s delay\n",
      "Task 1 started with 2s delay\n",
      "Task 1 started with 3s delay\n",
      "Task 1 completed\n",
      "Task 2 started with 1s delay\n",
      "Task 2 completed\n",
      "Task 2 started with 2s delay\n",
      "Task 1 completed\n",
      "Task 2 started with 3s delay\n",
      "Task 1 completed\n",
      "Task 3 started with 1s delay\n",
      "Task 2 completed\n",
      "Task 3 started with 2s delay\n",
      "Task 3 completed\n",
      "Task 3 started with 3s delay\n",
      "Task 2 completed\n",
      "Task 4 started with 1s delay\n",
      "Task 3 completed\n",
      "Task 4 started with 2s delay\n",
      "Task 4 completed\n",
      "Task 4 started with 3s delay\n",
      "Task 3 completed\n",
      "Task 5 started with 1s delay\n",
      "Task 4 completed\n",
      "Task 5 started with 2s delay\n",
      "Task 5 completed\n",
      "Task 5 started with 3s delay\n",
      "Task 4 completed\n",
      "Task 5 completed\n",
      "Task 5 completed\n",
      "\n",
      "Total execution time: 11.06 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Concurrently run function with 2 parameters or more\n",
    "\"\"\"\n",
    "# TODO: Import necessary package\n",
    "# TODO: Create a function to simulate I/O bound task with 2 parameters: task_id and delay time\n",
    "# TODO: Create list of task_id and delay time\n",
    "# TODO: Run your function with multi-threading by mapping your function with all the parameters\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "\n",
    "# Function with multiple parameters\n",
    "def io_bound_task(task_id, delay):\n",
    "    print(f\"Task {task_id} started with {delay}s delay\")\n",
    "    time.sleep(delay)  # Simulate I/O operation\n",
    "    print(f\"Task {task_id} completed\")\n",
    "    return f\"Result from task {task_id}\"\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create lists of parameters\n",
    "    task_ids = range(1, 6)  # Tasks 1-5\n",
    "    delay_times = [1, 2, 3]  # Different delay times\n",
    "    \n",
    "    # Create all combinations of parameters\n",
    "    task_params = list(product(task_ids, delay_times))\n",
    "    \n",
    "    # Run tasks concurrently with ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # Use starmap to pass multiple parameters\n",
    "        results = list(executor.map(lambda p: io_bound_task(*p), task_params))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 started with 2s delay\n",
      "Task 2 started with 2s delay\n",
      "Task 3 started with 2s delay\n",
      "Task 1 completed\n",
      "Task 4 started with 2s delay\n",
      "Task 3 completed\n",
      "Task 5 started with 2s delay\n",
      "Task 2 completed\n",
      "Task 4 completed\n",
      "Task 5 completed\n",
      "\n",
      "Total execution time: 4.01 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Homework Assignment: Improve previous code. \n",
    "Instead of creating a list of delay time, combine the list of task_id with a constant value of delay time\n",
    "using lambda\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function with multiple parameters\n",
    "def io_bound_task(task_id, delay):\n",
    "    print(f\"Task {task_id} started with {delay}s delay\")\n",
    "    time.sleep(delay)  # Simulate I/O operation\n",
    "    print(f\"Task {task_id} completed\")\n",
    "    return f\"Result from task {task_id}\"\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create list of task IDs and constant delay\n",
    "    task_ids = range(1, 6)  # Tasks 1-5\n",
    "    DELAY_TIME = 2  # Constant delay time\n",
    "    \n",
    "    # Run tasks concurrently with ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # Use lambda to combine task_id with constant delay\n",
    "        results = list(executor.map(\n",
    "            lambda x: io_bound_task(x, DELAY_TIME), \n",
    "            task_ids\n",
    "        ))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping http://books.toscrape.com/catalogue/catalogue/sapiens-a-brief-history-of-humankind_996/index.html: 'NoneType' object has no attribute 'text'Error scraping http://books.toscrape.com/catalogue/catalogue/the-black-maria_991/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/tipping-the-velvet_999/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/the-requiem-red_995/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/soumission_998/index.html: 'NoneType' object has no attribute 'text'\n",
      "\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/sharp-objects_997/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/a-light-in-the-attic_1000/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/shakespeares-sonnets_989/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/libertarianism-for-beginners_982/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/olio_984/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/rip-it-up-and-start-again_986/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/set-me-free_988/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/its-only-the-himalayas_981/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html: 'NoneType' object has no attribute 'text'\n",
      "Error scraping http://books.toscrape.com/catalogue/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html: 'NoneType' object has no attribute 'text'\n",
      "\n",
      "Total execution time: 2.87 seconds\n",
      "Successfully scraped 0 books\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Implement multi-threading in web scraping\n",
    "\"\"\"\n",
    "# TODO: Implement multi-threading on your bookstoscrape project inside new branch\n",
    "# TODO: Put github url here for grading\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def scrape_book(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        price = soup.find('p', class_='price_color').text\n",
    "        availability = soup.find('p', class_='availability').text.strip()\n",
    "        rating = soup.find('p', class_='star-rating')['class'][1]\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'availability': availability,\n",
    "            'rating': rating,\n",
    "            'url': url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    base_url = 'http://books.toscrape.com/catalogue/'\n",
    "    \n",
    "    # Get all book URLs from the main page\n",
    "    response = requests.get('http://books.toscrape.com')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_links = soup.select('h3 a')\n",
    "    book_urls = [urljoin(base_url, link['href']) for link in book_links]\n",
    "    \n",
    "    # Use ThreadPoolExecutor to scrape books concurrently\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(executor.map(scrape_book, book_urls))\n",
    "    \n",
    "    # Filter out None results and create DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('books_data.csv', index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully scraped {len(results)} books\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# GitHub Repository URL: [Your GitHub URL here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 articles to scrape\n",
      "Error scraping https://www.bbc.com/news/topics/c2vdnvdg6xxt: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/war-in-ukraine: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/us-canada: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/uk: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/world/africa: 'NoneType' object is not subscriptable\n",
      "\n",
      "Total execution time: 3.42 seconds\n",
      "Successfully scraped 0 articles\n",
      "Data saved to bbc_news_20250313_073718.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Implement multi-threading in web scraping\n",
    "\"\"\"\n",
    "# TODO: Find any news site that you like: Tribun, Detik, BBC, nytimes, etc\n",
    "# TODO: Extract data from the site in CSV\n",
    "# TODO: Push on github and put the link here\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # BBC News specific selectors\n",
    "        title = soup.find('h1').text.strip()\n",
    "        timestamp = soup.find('time')['datetime']\n",
    "        content = ' '.join([p.text.strip() for p in soup.select('article p')])\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'timestamp': timestamp,\n",
    "            'content': content,\n",
    "            'url': url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_article_urls(base_url, num_pages=5):\n",
    "    article_urls = []\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # BBC News specific selector for article links\n",
    "        links = soup.select('a[href*=\"/news/\"]')\n",
    "        for link in links:\n",
    "            url = link.get('href')\n",
    "            if url and url.startswith('/'):\n",
    "                full_url = 'https://www.bbc.com' + url\n",
    "                if full_url not in article_urls:\n",
    "                    article_urls.append(full_url)\n",
    "                    if len(article_urls) >= num_pages:\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting article URLs: {e}\")\n",
    "    \n",
    "    return article_urls\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get article URLs from BBC News\n",
    "    base_url = 'https://www.bbc.com/news'\n",
    "    article_urls = get_article_urls(base_url)\n",
    "    \n",
    "    print(f\"Found {len(article_urls)} articles to scrape\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor to scrape articles concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(scrape_article, article_urls))\n",
    "    \n",
    "    # Filter out None results and create DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_filename = f'bbc_news_{timestamp}.csv'\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully scraped {len(results)} articles\")\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# GitHub Repository URL: [Your GitHub URL here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "Monitor your resources usage while executing multi-threading, what do you think?\n",
    "\n",
    "(answer here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 articles to scrape\n",
      "Error scraping https://www.bbc.com/news/war-in-ukraine: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/topics/c2vdnvdg6xxt: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/uk: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/us-canada: 'NoneType' object is not subscriptable\n",
      "Error scraping https://www.bbc.com/news/world/africa: 'NoneType' object is not subscriptable\n",
      "\n",
      "Total execution time: 1.02 seconds\n",
      "Successfully scraped 0 articles\n",
      "Data saved to bbc_news_20250313_075135.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Implement multi-threading in web scraping\n",
    "\"\"\"\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # BBC News specific selectors\n",
    "        title = soup.find('h1').text.strip()\n",
    "        timestamp = soup.find('time')['datetime']\n",
    "        content = ' '.join([p.text.strip() for p in soup.select('article p')])\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'timestamp': timestamp,\n",
    "            'content': content,\n",
    "            'url': url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_article_urls(base_url, num_pages=5):\n",
    "    article_urls = []\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # BBC News specific selector for article links\n",
    "        links = soup.select('a[href*=\"/news/\"]')\n",
    "        for link in links:\n",
    "            url = link.get('href')\n",
    "            if url and url.startswith('/'):\n",
    "                full_url = 'https://www.bbc.com' + url\n",
    "                if full_url not in article_urls:\n",
    "                    article_urls.append(full_url)\n",
    "                    if len(article_urls) >= num_pages:\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting article URLs: {e}\")\n",
    "    \n",
    "    return article_urls\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get article URLs from BBC News\n",
    "    base_url = 'https://www.bbc.com/news'\n",
    "    article_urls = get_article_urls(base_url)\n",
    "    \n",
    "    print(f\"Found {len(article_urls)} articles to scrape\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor to scrape articles concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(scrape_article, article_urls))\n",
    "    \n",
    "    # Filter out None results and create DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV with timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_filename = f'bbc_news_{timestamp}.csv'\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully scraped {len(results)} articles\")\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# GitHub Repository URL: [Your GitHub URL here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "While Multi-threading is like adding \"more engine\", there is a better approach for improve scraping time. Find out about asynchronous concept and be prepared for the next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 1.44 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Asynchronous Programming Example\n",
    "\"\"\"\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def fetch_url(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def main():\n",
    "    start_time = time.time()\n",
    "    urls = [f'http://example.com/page{i}' for i in range(1, 6)]\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_url(session, url) for url in urls]\n",
    "        await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Run the async code\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
