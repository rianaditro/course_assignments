{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Transformations**\n",
    "Web scraping often results in raw, messy data that can be inconsistent, incomplete, or improperly formatted. This unrefined data typically includes issues like missing values, typographical errors, varied date formats, and numeric values stored as text. Data transformation is the process of cleaning and standardizing this data using tools such as Pandas, which converts the raw output into a structured DataFrame. Through transformation, we can trim unnecessary whitespace, correct inconsistent casing, fill in missing values, and convert data types appropriately, ensuring that the data is accurate and ready for further analysis.\n",
    "\n",
    "In summary, mastering data transformation techniques is essential for web scrapers to unlock the full potential of their collected data and to facilitate a seamless transition from raw data to actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Execute this cell to get a CSV file to work with\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "scraping_data = [\n",
    "    {\"name\": \"Alice\", \"age\": \"25\", \"email\": \"alice@example.com\", \"rate\": \"$20.5\", \"join_date\": \"2004-01-10\"}, # rate is a string\n",
    "    {\"name\": \"bob\", \"age\": \"30\", \"email\": \"my email is bob30@example.com\", \"rate\": \"$35\", \"join_date\": \"2020/01/12\"},   # email typo, different date format\n",
    "    {\"name\": \"Charlie\", \"age\": \"20\", \"email\": \"charlie@example.com\", \"rate\": \"$20\", \"join_date\": \"2004-01-15\"},  # age missing, non-numeric price\n",
    "    {\"name\": \"David\", \"age\": \"40\", \"email\": \"\", \"rate\": \"45.0\", \"join_date\": \"2004-01-15\"},  # missing email\n",
    "    {\"name\": None, \"age\": None, \"email\": None, \"rate\": None, \"join_date\": None},  # invalid data row\n",
    "    {\"name\": \"Frank\", \"age\": \"35\", \"email\": \"frank@example.com\", \"rate\": 20, \"join_date\": \"2004-01-25\"},\n",
    "    {\"name\": \"Grace\", \"age\": \"28\", \"email\": \"grace@example.com\", \"rate\": 30, \"join_date\": \"2004-12-01\"} \n",
    "]\n",
    "\n",
    "df = pd.DataFrame(scraping_data)\n",
    "df.to_csv(\"scraping_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>rate</th>\n",
       "      <th>join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25.0</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>$20.5</td>\n",
       "      <td>2004-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>30.0</td>\n",
       "      <td>my email is bob30@example.com</td>\n",
       "      <td>$35</td>\n",
       "      <td>2020/01/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>20.0</td>\n",
       "      <td>charlie@example.com</td>\n",
       "      <td>$20</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>35.0</td>\n",
       "      <td>frank@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>28.0</td>\n",
       "      <td>grace@example.com</td>\n",
       "      <td>30</td>\n",
       "      <td>2004-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age                          email   rate   join_date\n",
       "0    Alice  25.0              alice@example.com  $20.5  2004-01-10\n",
       "1      bob  30.0  my email is bob30@example.com    $35  2020/01/12\n",
       "2  Charlie  20.0            charlie@example.com    $20  2004-01-15\n",
       "3    David  40.0                            NaN   45.0  2004-01-15\n",
       "4      NaN   NaN                            NaN    NaN         NaN\n",
       "5    Frank  35.0              frank@example.com     20  2004-01-25\n",
       "6    Grace  28.0              grace@example.com     30  2004-12-01"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Reading data from a CSV file\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Read the data from the CSV file into a DataFrame\n",
    "# TODO: Apart from the read_csv() method, what else can pandas read from?\n",
    "\n",
    "df = pd.read_csv(\"scraping_data.csv\")\n",
    "\n",
    "\"\"\"\n",
    "1. Excel files:\n",
    "df = pd.read_excel('file.xlsx')\n",
    "\n",
    "2. JSON files:\n",
    "df = pd.read_json('file.json')\n",
    "\n",
    "3. SQL databases:\n",
    "df = pd.read_sql('query', connection)\n",
    "\n",
    "4. HTML tables:\n",
    "df = pd.read_html('url_or_file.html')\n",
    "\n",
    "5. XML files:\n",
    "df = pd.read_xml('file.xml')\n",
    "\n",
    "6. Parquet files:\n",
    "df = pd.read_parquet('file.parquet')\n",
    "\n",
    "7. HDF5 files:\n",
    "df = pd.read_hdf('file.h5', 'key')\n",
    "\n",
    "8. Pickle files:\n",
    "df = pd.read_pickle('file.pkl')\n",
    "\n",
    "9. SAS files:\n",
    "df = pd.read_sas('file.sas7bdat')\n",
    "\n",
    "10. STATA files:\n",
    "df = pd.read_stata('file.dta')\n",
    "\"\"\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   name       6 non-null      object \n",
      " 1   age        6 non-null      float64\n",
      " 2   email      5 non-null      object \n",
      " 3   rate       6 non-null      object \n",
      " 4   join_date  6 non-null      object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 412.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understanding the data\n",
    "\"\"\"\n",
    "# TODO: Use the info() and describe(include=\"all\") methods to understand the data\n",
    "# TODO: The info() method returns different non-null values, why is that?\n",
    "# The info() method shows different non-null values because:\n",
    "# - Some columns have missing values (NaN)\n",
    "# - Row 4 has all null values\n",
    "# - 'email' column has an empty string which is converted to NaN\n",
    "\n",
    "# TODO: The describe() method returns 7 count name and 7 unique name, what is that means?\n",
    "# The describe() method shows:\n",
    "# - count: number of non-null values\n",
    "# - unique: number of unique values\n",
    "# - top: most frequent value\n",
    "# - freq: frequency of the most common value\n",
    "# - mean, std, min, 25%, 50%, 75%, max: numerical statistics (only for numeric columns)\n",
    "\n",
    "print(df.info())\n",
    "# prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.\n",
    "\n",
    "#print(df.describe(include=\"all\"))\n",
    "# display summary statistics of numerical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name   age                          email   rate   join_date\n",
      "0    Alice  25.0              alice@example.com  $20.5  2004-01-10\n",
      "1      bob  30.0  my email is bob30@example.com    $35  2020/01/12\n",
      "2  Charlie  20.0            charlie@example.com    $20  2004-01-15\n",
      "3    David  40.0                            NaN   45.0  2004-01-15\n",
      "4      NaN   NaN                            NaN    NaN         NaN\n",
      "5    Frank  35.0              frank@example.com     20  2004-01-25\n",
      "6    Grace  28.0              grace@example.com     30  2004-12-01\n",
      "======================================\n",
      "      name   age                          email   rate   join_date\n",
      "0    Alice  25.0              alice@example.com  $20.5  2004-01-10\n",
      "1      bob  30.0  my email is bob30@example.com    $35  2020/01/12\n",
      "2  Charlie  20.0            charlie@example.com    $20  2004-01-15\n",
      "3    David  40.0                            NaN   45.0  2004-01-15\n",
      "5    Frank  35.0              frank@example.com     20  2004-01-25\n",
      "6    Grace  28.0              grace@example.com     30  2004-12-01\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Handling missing data\n",
    "Resource: https://www.kaggle.com/code/rtatman/data-cleaning-challenge-handling-missing-values\n",
    "\"\"\"\n",
    "# TODO: Print the dataframe and notice which columns have missing values\n",
    "# TODO: Add a separator print(\"======================================\")\n",
    "# TODO: Remove all the rows that contain a missing value using the dropna() method\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(\"======================================\")\n",
    "\n",
    "df = df.dropna(how=\"all\")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>rate</th>\n",
       "      <th>join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25.0</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2004-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>30.0</td>\n",
       "      <td>my email is bob30@example.com</td>\n",
       "      <td>35</td>\n",
       "      <td>2020/01/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>20.0</td>\n",
       "      <td>charlie@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>35.0</td>\n",
       "      <td>frank@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>28.0</td>\n",
       "      <td>grace@example.com</td>\n",
       "      <td>30</td>\n",
       "      <td>2004-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age                          email  rate   join_date\n",
       "0    Alice  25.0              alice@example.com  20.5  2004-01-10\n",
       "1      bob  30.0  my email is bob30@example.com    35  2020/01/12\n",
       "2  Charlie  20.0            charlie@example.com    20  2004-01-15\n",
       "3    David  40.0                            NaN  45.0  2004-01-15\n",
       "4      NaN   NaN                            NaN   NaN         NaN\n",
       "5    Frank  35.0              frank@example.com    20  2004-01-25\n",
       "6    Grace  28.0              grace@example.com    30  2004-12-01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Manipulating Textual Data\n",
    "\"\"\"\n",
    "# TODO: Replace all the $ signs with nothing using str.replace\n",
    "# TODO: Shows the result\n",
    "\n",
    "# df[\"rate\"] = df[\"rate\"].str.replace(\"$\", \"\")\n",
    "df.loc[:, \"rate\"] = df[\"rate\"].str.replace(\"$\", \"\")\n",
    "df\n",
    "\n",
    "# 1. Uses str.replace() to remove all \"$\" symbols from the rate column\n",
    "# 2. Uses df.loc[:, \"rate\"] for explicit column access (safer than direct indexing)\n",
    "# 3. The result shows the rate column with clean numeric values without dollar signs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>rate</th>\n",
       "      <th>join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25.0</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2004-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>30.0</td>\n",
       "      <td>my email is bob30@example.com</td>\n",
       "      <td>35</td>\n",
       "      <td>2020-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>20.0</td>\n",
       "      <td>charlie@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>35.0</td>\n",
       "      <td>frank@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>28.0</td>\n",
       "      <td>grace@example.com</td>\n",
       "      <td>30</td>\n",
       "      <td>2004-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age                          email  rate   join_date\n",
       "0    Alice  25.0              alice@example.com  20.5  2004-01-10\n",
       "1      bob  30.0  my email is bob30@example.com    35  2020-01-12\n",
       "2  Charlie  20.0            charlie@example.com    20  2004-01-15\n",
       "3    David  40.0                            NaN  45.0  2004-01-15\n",
       "4      NaN   NaN                            NaN   NaN         NaN\n",
       "5    Frank  35.0              frank@example.com    20  2004-01-25\n",
       "6    Grace  28.0              grace@example.com    30  2004-12-01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Manipulating Textual Data\n",
    "\"\"\"\n",
    "# TODO: Replace all the / signs with - using str.replace\n",
    "# TODO: Shows the result\n",
    "\n",
    "df.loc[:, \"join_date\"] = df[\"join_date\"].str.replace(\"/\", \"-\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective: Manipulating Textual Data\n",
    "\"\"\"\n",
    "# TODO: Validate emails from the email column\n",
    "# TODO: Shows the result\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_copy[\"email\"] = df_copy[\"email\"].str.extract(r'([\\w\\-]+@[\\w\\.-]+\\.\\w+)')\n",
    "df_copy\n",
    "df = df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>rate</th>\n",
       "      <th>join_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25.0</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2004-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bob</td>\n",
       "      <td>30.0</td>\n",
       "      <td>bob30@example.com</td>\n",
       "      <td>35</td>\n",
       "      <td>2020-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>20.0</td>\n",
       "      <td>charlie@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>40.0</td>\n",
       "      <td>-</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2004-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Frank</td>\n",
       "      <td>35.0</td>\n",
       "      <td>frank@example.com</td>\n",
       "      <td>20</td>\n",
       "      <td>2004-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grace</td>\n",
       "      <td>28.0</td>\n",
       "      <td>grace@example.com</td>\n",
       "      <td>30</td>\n",
       "      <td>2004-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age                email  rate   join_date\n",
       "0    Alice  25.0    alice@example.com  20.5  2004-01-10\n",
       "1      bob  30.0    bob30@example.com    35  2020-01-12\n",
       "2  Charlie  20.0  charlie@example.com    20  2004-01-15\n",
       "3    David  40.0                    -  45.0  2004-01-15\n",
       "4      NaN   NaN                    -   NaN         NaN\n",
       "5    Frank  35.0    frank@example.com    20  2004-01-25\n",
       "6    Grace  28.0    grace@example.com    30  2004-12-01"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Filling Missing Data\n",
    "\"\"\"\n",
    "# TODO: Fill empty value in the email column with \"-\"\n",
    "# TODO: Shows the result\n",
    "\n",
    "# df[\"email\"] = df[\"email\"].fillna(\"-\")\n",
    "df.loc[:, \"email\"] = df[\"email\"].fillna(\"-\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   name       6 non-null      object \n",
      " 1   age        6 non-null      float64\n",
      " 2   email      7 non-null      object \n",
      " 3   rate       6 non-null      object \n",
      " 4   join_date  6 non-null      object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 412.0+ bytes\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m df_copy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      9\u001b[0m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# this will convert the name column to string\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# this will convert the age column to integer\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_copy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;66;03m# this will convert the join_date column to datetime\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# TODO: Convert email column to string\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# TODO: Convert rate column to float\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# TODO: Check the data types of each column using the info() method to make sure the conversion was successful\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# dtypes: datetime64[ns](1), float64(1), int64(1), string(2)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# memory usage: 288.0 bytes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:101\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mensure_string_array(\n\u001b[0;32m     97\u001b[0m         arr, skipna\u001b[38;5;241m=\u001b[39mskipna, convert_na_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(shape)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _astype_float_to_int_nansafe(arr, dtype, copy)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[0m, in \u001b[0;36m_astype_float_to_int_nansafe\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(values)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Convert Data Types\n",
    "\"\"\"\n",
    "# TODO: Check the data types of each column using the info() method\n",
    "df.info()\n",
    "\n",
    "df_copy = df.copy()\n",
    "\n",
    "df_copy[\"name\"] = df_copy[\"name\"].astype(\"string\") # this will convert the name column to string\n",
    "df_copy[\"age\"] = df_copy[\"age\"].astype(\"int\") # this will convert the age column to integer\n",
    "df_copy[\"join_date\"] = pd.to_datetime(df_copy[\"join_date\"]) # this will convert the join_date column to datetime\n",
    "\n",
    "# TODO: Convert email column to string\n",
    "# TODO: Convert rate column to float\n",
    "# TODO: Check the data types of each column using the info() method to make sure the conversion was successful\n",
    "\n",
    "# Expected Output\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 6 entries, 0 to 6\n",
    "# Data columns (total 5 columns):\n",
    "#  #   Column     Non-Null Count  Dtype         \n",
    "# ---  ------     --------------  -----         \n",
    "#  0   name       6 non-null      string        \n",
    "#  1   age        6 non-null      int64         \n",
    "#  2   email      6 non-null      string        \n",
    "#  3   rate       6 non-null      float64       \n",
    "#  4   join_date  6 non-null      datetime64[ns]\n",
    "# dtypes: datetime64[ns](1), float64(1), int64(1), string(2)\n",
    "# memory usage: 288.0 bytes\n",
    "\n",
    "df_copy[\"email\"] = df_copy[\"email\"].astype(\"string\")\n",
    "df_copy[\"rate\"] = df_copy[\"rate\"].astype(\"float\")\n",
    "\n",
    "\n",
    "\n",
    "df_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Timestamp' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mObjective: Add new columns based on existing columns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m today \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoday\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# get the current date\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoin_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (today \u001b[38;5;241m-\u001b[39m x)\u001b[38;5;241m.\u001b[39mdays) \u001b[38;5;66;03m# this will calculate the period of employment in days\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TODO: Re-assign apply() to calculate the period of employment in years\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# TODO: Re-assign apply() to round up the period of employment\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m365\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\rudic\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mObjective: Add new columns based on existing columns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m today \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoday\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# get the current date\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoin_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (today \u001b[38;5;241m-\u001b[39m x)\u001b[38;5;241m.\u001b[39mdays) \u001b[38;5;66;03m# this will calculate the period of employment in days\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# TODO: Re-assign apply() to calculate the period of employment in years\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# TODO: Re-assign apply() to round up the period of employment\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiod of employment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m365\u001b[39m))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Timestamp' and 'str'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Add new columns based on existing columns\n",
    "\"\"\"\n",
    "\n",
    "today = pd.to_datetime(\"today\") # get the current date\n",
    "df['period of employment'] = df['join_date'].apply(lambda x: (today - x).days) # this will calculate the period of employment in days\n",
    "\n",
    "# TODO: Re-assign apply() to calculate the period of employment in years\n",
    "# TODO: Re-assign apply() to round up the period of employment\n",
    "\n",
    "df['period of employment'] = df['period of employment'].apply(lambda x: (x/365))\n",
    "df['period of employment'] = df['period of employment'].apply(lambda x: round(x, 0))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Microsoft</th>\n",
       "      <th>Google</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Price</td>\n",
       "      <td>150.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Change</td>\n",
       "      <td>+2%</td>\n",
       "      <td>-1%</td>\n",
       "      <td>+1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Volume</td>\n",
       "      <td>1M</td>\n",
       "      <td>500K</td>\n",
       "      <td>2M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Attribute  Apple Microsoft  Google\n",
       "0     Price  150.0     250.0  2800.0\n",
       "1    Change    +2%       -1%     +1%\n",
       "2    Volume     1M      500K      2M"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Transpose Rows and Columns\n",
    "\"\"\"\n",
    "# TODO: Execute this cell before continue\n",
    "\n",
    "# Sample data (as rows)\n",
    "data = {\n",
    "    'Attribute': ['Price', 'Change', 'Volume'],\n",
    "    'Apple': [150.00, '+2%', '1M'],\n",
    "    'Microsoft': [250.00, '-1%', '500K'],\n",
    "    'Google': [2800.00, '+1%', '2M']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# # Set 'Attribute' as index (attributes as rows)\n",
    "# df.set_index('Attribute', inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute   Price  Change  Volume\n",
      "Price                            \n",
      "Price       Price  Change  Volume\n",
      "150.0       150.0     +2%      1M\n",
      "250.0       250.0     -1%    500K\n",
      "2800.0     2800.0     +1%      2M\n",
      "Price    Price  Change  Volume\n",
      "Price                         \n",
      "Price    Price  Change  Volume\n",
      "150.0    150.0     +2%      1M\n",
      "250.0    250.0     -1%    500K\n",
      "2800.0  2800.0     +1%      2M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Price</th>\n",
       "      <th>Change</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150.0</td>\n",
       "      <td>+2%</td>\n",
       "      <td>1M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250.0</td>\n",
       "      <td>-1%</td>\n",
       "      <td>500K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>+1%</td>\n",
       "      <td>2M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price   Price Change Volume\n",
       "0       150.0    +2%     1M\n",
       "1       250.0    -1%   500K\n",
       "2      2800.0    +1%     2M"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Transpose Rows and Columns\n",
    "\"\"\"\n",
    "# TODO: Transpose the DataFrame using the transpose() method\n",
    "# TODO: Shows the result\n",
    "# TODO: Rename the column headers to be the values from the first row\n",
    "# TODO: Drop the first row, reset the index before dropping\n",
    "\n",
    "df = df.transpose()\n",
    "print(df)\n",
    "\n",
    "# # Set the first row as column headers and drop it from the DataFrame\n",
    "df.columns = df.iloc[0]  # Set the first row as column headers\n",
    "print(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = df.drop(index=0)  # Drop the first row\n",
    "\n",
    "# # Reset the index for clean output\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name\n",
      "0   1    Alice\n",
      "1   2      Bob\n",
      "2   3  Charlie\n",
      "===============================\n",
      "   ID   Name\n",
      "0   4  David\n",
      "1   5    Eva\n",
      "2   6  Frank\n",
      "===============================\n",
      "   ID     Name\n",
      "0   1    Alice\n",
      "1   2      Bob\n",
      "2   3  Charlie\n",
      "3   4    David\n",
      "4   5      Eva\n",
      "5   6    Frank\n",
      "===============================\n",
      "   Age         City\n",
      "0   25     New York\n",
      "1   30  Los Angeles\n",
      "2   35      Chicago\n",
      "3   25     New York\n",
      "4   30  Los Angeles\n",
      "5   35      Chicago\n",
      "===============================\n",
      "    ID     Name   Age         City\n",
      "0  1.0    Alice   NaN          NaN\n",
      "1  2.0      Bob   NaN          NaN\n",
      "2  3.0  Charlie   NaN          NaN\n",
      "3  4.0    David   NaN          NaN\n",
      "4  5.0      Eva   NaN          NaN\n",
      "5  6.0    Frank   NaN          NaN\n",
      "0  NaN      NaN  25.0     New York\n",
      "1  NaN      NaN  30.0  Los Angeles\n",
      "2  NaN      NaN  35.0      Chicago\n",
      "3  NaN      NaN  25.0     New York\n",
      "4  NaN      NaN  30.0  Los Angeles\n",
      "5  NaN      NaN  35.0      Chicago\n",
      "Adding new columns horizontally (axis=1):\n",
      "   Age         City\n",
      "0   25     New York\n",
      "1   30  Los Angeles\n",
      "2   35      Chicago\n",
      "3   25     New York\n",
      "4   30  Los Angeles\n",
      "5   35      Chicago\n",
      "===============================\n",
      "   ID     Name  Age         City\n",
      "0   1    Alice   25     New York\n",
      "1   2      Bob   30  Los Angeles\n",
      "2   3  Charlie   35      Chicago\n",
      "3   4    David   25     New York\n",
      "4   5      Eva   30  Los Angeles\n",
      "5   6    Frank   35      Chicago\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understanding Concatenation (pd.concat): Stack DataFrames vertically (rows) or horizontally (columns).\n",
    "\"\"\"\n",
    "# Create first DataFrame\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "# Create second DataFrame\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': [4, 5, 6],\n",
    "    'Name': ['David', 'Eva', 'Frank']\n",
    "})\n",
    "\n",
    "# Concatenate vertically (row-wise)\n",
    "df_concat = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(df1)\n",
    "print(\"===============================\")\n",
    "print(df2)\n",
    "print(\"===============================\")\n",
    "print(df_concat)\n",
    "print(\"===============================\")\n",
    "\n",
    "# TODO: Execute this cell to understand the before and after concatenation\n",
    "# TODO: Re-apply concatenation vertically to add Age and City columns from df3 below by using axis=1 and set ignore_index=False\n",
    "\n",
    "# Create third DataFrame\n",
    "df3 = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 25, 30, 35],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Los Angeles', 'Chicago']\n",
    "})\n",
    "\n",
    "df_concat_vertical = pd.concat([df_concat, df3], axis=0, ignore_index=False)\n",
    "print(df3)\n",
    "print(\"===============================\")\n",
    "print(df_concat_vertical)\n",
    "\n",
    "df_concat_horizontal = pd.concat([df_concat, df3], axis=1)\n",
    "print(\"Adding new columns horizontally (axis=1):\")\n",
    "print(df3)\n",
    "print(\"===============================\")\n",
    "print(df_concat_horizontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID     Name\n",
      "0   1    Alice\n",
      "1   2      Bob\n",
      "2   3  Charlie\n",
      "===============================\n",
      "   ID  Age\n",
      "0   2   30\n",
      "1   3   35\n",
      "2   4   40\n",
      "===============================\n",
      "   ID     Name  Age\n",
      "0   2      Bob   30\n",
      "1   3  Charlie   35\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understanding Merging (pd.merge): Join DataFrames based on common column values (like SQL joins).\n",
    "\"\"\"\n",
    "# Create first DataFrame\n",
    "df1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "# Create second DataFrame\n",
    "df2 = pd.DataFrame({\n",
    "    'ID': [2, 3, 4],\n",
    "    'Age': [30, 35, 40]\n",
    "})\n",
    "\n",
    "# Merge DataFrames on 'ID' (inner join by default)\n",
    "df_merged = pd.merge(df1, df2, on='ID', how='inner')\n",
    "\n",
    "print(df1)\n",
    "print(\"===============================\")\n",
    "print(df2)\n",
    "print(\"===============================\")\n",
    "print(df_merged)\n",
    "\n",
    "# TODO: Execute this cell to understand the inner join in pandas\n",
    "# TODO: After that change how='inner' to how='outer' to understand the outer join\n",
    "# TODO: What is the difference between inner join and outer join?\n",
    "\n",
    "# First, inner join\n",
    "print(\"Inner Join Result:\")\n",
    "df_merged_inner = pd.merge(df1, df2, on='ID', how='inner')\n",
    "print(df1)\n",
    "print(\"===============================\")\n",
    "print(df2)\n",
    "print(\"===============================\")\n",
    "print(df_merged_inner)\n",
    "\n",
    "print(\"\\nOuter Join Result:\")\n",
    "# Now  outer join\n",
    "df_merged_outer = pd.merge(df1, df2, on='ID', how='outer')\n",
    "print(df_merged_outer)\n",
    "\n",
    "# The key differences between inner and outer joins:\n",
    "\n",
    "# 1. Inner Join ( how='inner' ):\n",
    "   \n",
    "#    - Only keeps rows where the 'ID' exists in both DataFrames\n",
    "#    - Result only contains IDs 2 and 3\n",
    "#    - Rows with ID 1 and 4 are dropped because they don't exist in both DataFrames\n",
    "# 2. Outer Join ( how='outer' ):\n",
    "   \n",
    "#    - Keeps all rows from both DataFrames\n",
    "#    - Result contains all IDs (1, 2, 3, and 4)\n",
    "#    - Missing values are filled with NaN\n",
    "#    - ID 1 will have NaN for Age\n",
    "#    - ID 4 will have NaN for Name\n",
    "# The output will show how inner join creates a more restricted result while outer join preserves all data from both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name\n",
      "1    Alice\n",
      "2      Bob\n",
      "3  Charlie\n",
      "===============================\n",
      "   Age\n",
      "2   25\n",
      "3   30\n",
      "4   35\n",
      "===============================\n",
      "      Name   Age\n",
      "1    Alice   NaN\n",
      "2      Bob  25.0\n",
      "3  Charlie  30.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understanding Joining (df.join): Join DataFrames based on index.\n",
    "\"\"\"\n",
    "# Create first DataFrame with custom index\n",
    "df1 = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie']\n",
    "}, index=[1, 2, 3])\n",
    "\n",
    "# Create second DataFrame with matching index\n",
    "df2 = pd.DataFrame({\n",
    "    'Age': [25, 30, 35]\n",
    "}, index=[2, 3, 4])\n",
    "\n",
    "# Join DataFrames on index\n",
    "df_joined = df1.join(df2)\n",
    "\n",
    "print(df1)\n",
    "print(\"===============================\")\n",
    "print(df2)\n",
    "print(\"===============================\")\n",
    "print(df_joined)\n",
    "\n",
    "# TODO: Execute this cell to understand the join in pandas\n",
    "# TODO: Create third dataframe with different index\n",
    "# TODO: Join DataFrames using join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 Sales Report:\n",
      "  Product  Sales  Revenue  Expenses  Profit\n",
      "0       A   1200    24000     15000    9000\n",
      "1       B   1500    30000     18000   12000\n",
      "2       C   1300    26000     16000   10000\n",
      "\n",
      "2022 Sales Report:\n",
      "  Product  Sales  Revenue  Expenses  Profit\n",
      "0       A   1250    25000     16000    9000\n",
      "1       B   1600    32000     19000   13000\n",
      "2       C   1350    27000     17000   10000\n",
      "\n",
      "2023 Sales Report:\n",
      "  Product  Sales  Revenue  Expenses  Profit\n",
      "0       A   1300    26000     17000    9000\n",
      "1       B   1700    34000     20000   14000\n",
      "2       C   1400    28000     18000   10000\n",
      "\n",
      "Combined Sales Report (Vertical Concatenation):\n",
      "  Product  Sales  Revenue  Expenses  Profit  Year\n",
      "0       A   1200    24000     15000    9000  2021\n",
      "1       B   1500    30000     18000   12000  2021\n",
      "2       C   1300    26000     16000   10000  2021\n",
      "3       A   1250    25000     16000    9000  2022\n",
      "4       B   1600    32000     19000   13000  2022\n",
      "5       C   1350    27000     17000   10000  2022\n",
      "6       A   1300    26000     17000    9000  2023\n",
      "7       B   1700    34000     20000   14000  2023\n",
      "8       C   1400    28000     18000   10000  2023\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Choosing between concat, merge, and join\n",
    "\"\"\"\n",
    "# Create first DataFrame (2021 Sales Report)\n",
    "df_2021 = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C'],\n",
    "    'Sales': [1200, 1500, 1300],\n",
    "    'Revenue': [24000, 30000, 26000],\n",
    "    'Expenses': [15000, 18000, 16000],\n",
    "    'Profit': [9000, 12000, 10000]\n",
    "})\n",
    "\n",
    "# Create second DataFrame (2022 Sales Report)\n",
    "df_2022 = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C'],\n",
    "    'Sales': [1250, 1600, 1350],\n",
    "    'Revenue': [25000, 32000, 27000],\n",
    "    'Expenses': [16000, 19000, 17000],\n",
    "    'Profit': [9000, 13000, 10000]\n",
    "})\n",
    "\n",
    "# Create third DataFrame (2023 Sales Report)\n",
    "df_2023 = pd.DataFrame({\n",
    "    'Product': ['A', 'B', 'C'],\n",
    "    'Sales': [1300, 1700, 1400],\n",
    "    'Revenue': [26000, 34000, 28000],\n",
    "    'Expenses': [17000, 20000, 18000],\n",
    "    'Profit': [9000, 14000, 10000]\n",
    "})\n",
    "\n",
    "# Display DataFrames\n",
    "print(\"2021 Sales Report:\")\n",
    "print(df_2021)\n",
    "print(\"\\n2022 Sales Report:\")\n",
    "print(df_2022)\n",
    "print(\"\\n2023 Sales Report:\")\n",
    "print(df_2023)\n",
    "\n",
    "# TODO: There are 3 difference dataframe, this are representing sales report in a year. Each year they have different file.\n",
    "# TODO: Your task is to combine them into single dataframe for further analysis\n",
    "# TODO: Determine how you combine them. You can choose between concat, merge, and join.\n",
    "# Concatenate DataFrames vertically, with year as a new column to identify the year\n",
    "df_2021['Year'] = 2021\n",
    "df_2022['Year'] = 2022\n",
    "df_2023['Year'] = 2023\n",
    "\n",
    "# Concatenate vertically (adding rows)\n",
    "df_combined = pd.concat([df_2021, df_2022, df_2023], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"\\nCombined Sales Report:\")\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "When would you choose to use pd.concat() instead of pd.merge() or df.join()? And how do the performance and functionality of these methods differ when dealing with large datasets?\n",
    "\n",
    "(answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pd.concat()\n",
    "Best for:\n",
    "\n",
    "1. Simple stacking of DataFrames (vertically or horizontally)\n",
    "2. When you don't need to match on specific columns\n",
    "3. When dealing with multiple DataFrames at once\n",
    "Performance:\n",
    "\n",
    "- Fast for simple concatenations\n",
    "- Memory efficient for vertical stacking\n",
    "- Can be memory-intensive for horizontal stacking with large datasets\n",
    "### pd.merge()\n",
    "Best for:\n",
    "\n",
    "1. Combining DataFrames based on common column values\n",
    "2. When you need SQL-like join operations (inner, outer, left, right)\n",
    "3. When matching on multiple columns\n",
    "Performance:\n",
    "\n",
    "- Optimized for column-based joins\n",
    "- Can be slower with large datasets due to key matching\n",
    "- Memory usage depends on join type and data size\n",
    "### df.join()\n",
    "Best for:\n",
    "\n",
    "1. Index-based joining\n",
    "2. Simple left joins\n",
    "3. When DataFrames share a common index\n",
    "Performance:\n",
    "\n",
    "- Fastest for index-based operations\n",
    "- Memory efficient when indexes are aligned\n",
    "- Less flexible than merge()\n",
    "### Large Dataset Considerations:\n",
    "1. pd.concat() :\n",
    "   \n",
    "   - Use for simple vertical stacking\n",
    "   - Avoid for wide horizontal concatenations\n",
    "2. pd.merge() :\n",
    "   \n",
    "   - Use when you need complex joins\n",
    "   - Consider chunking data for very large datasets\n",
    "3. df.join() :\n",
    "   \n",
    "   - Best choice when data is already indexed properly\n",
    "   - Most memory efficient for simple joins\n",
    "Choose pd.concat() when you need simple combining of DataFrames without matching values. Use pd.merge() for complex joins based on column values, and df.join() for simple index-based operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Learn how to store web-scraped data or any Pandas DataFrame into a Google Spreadsheet programmatically using Google Sheets API and gspread library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, install the required packages:\n",
    "\n",
    "        #pip install gspread oauth2client pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set up Google Sheets API:\n",
    "\n",
    "- Go to Google Cloud Console\n",
    "- Create a new project\n",
    "- Enable Google Sheets API\n",
    "- Create credentials (Service Account)\n",
    "- Download the JSON credentials file\n",
    "- Share your Google Sheet with the client_email from your credentials\n",
    "3. Here's the code to store DataFrame in Google Sheets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "\n",
    "def upload_to_sheets(df, spreadsheet_name, worksheet_name):\n",
    "    # Define the scope\n",
    "    scope = ['https://spreadsheets.google.com/feeds',\n",
    "             'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    # Load credentials from JSON file\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        'path/to/your/credentials.json', scope)\n",
    "\n",
    "    # Authorize with Google\n",
    "    client = gspread.authorize(credentials)\n",
    "\n",
    "    try:\n",
    "        # Open the spreadsheet\n",
    "        spreadsheet = client.open(spreadsheet_name)\n",
    "        \n",
    "        # Create or open worksheet\n",
    "        try:\n",
    "            worksheet = spreadsheet.worksheet(worksheet_name)\n",
    "        except:\n",
    "            worksheet = spreadsheet.add_worksheet(worksheet_name, \n",
    "                                               rows=df.shape[0]+1, \n",
    "                                               cols=df.shape[1])\n",
    "\n",
    "        # Clear existing content\n",
    "        worksheet.clear()\n",
    "\n",
    "        # Update with new data\n",
    "        worksheet.update([df.columns.values.tolist()] + df.values.tolist())\n",
    "        \n",
    "        print(f\"Data successfully uploaded to {spreadsheet_name}/{worksheet_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample DataFrame\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 30, 35],\n",
    "        'City': ['New York', 'London', 'Paris']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Upload to Google Sheets\n",
    "    upload_to_sheets(df, \n",
    "                    spreadsheet_name='Your Spreadsheet Name',\n",
    "                    worksheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To use this script:\n",
    "   - Replace 'path/to/your/credentials.json' with the actual path to your credentials file\n",
    "   - Replace 'Your Spreadsheet Name' with your actual Google Sheets document name\n",
    "   - Modify the sample DataFrame with your actual data\n",
    "Key Features of this implementation:\n",
    "\n",
    "- Automatically creates new worksheet if it doesn't exist\n",
    "- Clears existing content before uploading new data\n",
    "- Preserves column headers\n",
    "- Handles errors gracefully\n",
    "- Can be easily modified to append data instead of overwriting\n",
    "Remember to keep your credentials file secure and never share it publicly. Also, make sure the Google Sheet is shared with the service account email address from your credentials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
