{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parsing HTML**\n",
    "\n",
    "BeautifulSoup is a Python library that simplifies the process of web scraping by allowing developers to extract data from HTML documents easily. It transforms complicated HTML documents into a tree of Python objects, such as tags, navigable strings, and comments. This makes it straightforward to locate and manipulate the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPractice Exercise: BeautifulSoup Basics\\n\\nComplete each function below by following the TODO instructions. \\nEach function includes the objective of the task and the expected output.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs, Comment\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Practice Exercise: BeautifulSoup Basics\n",
    "\n",
    "Complete each function below by following the TODO instructions. \n",
    "Each function includes the objective of the task and the expected output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of text before soup: <class 'str'>\n",
      "Type of text after soup: <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_soup():\n",
    "    \"\"\"\n",
    "    Objective: Convert the provided text (HTML content) into a BeautifulSoup object.\n",
    "    Expected Output:\n",
    "    Type of text before soup: <class 'str'>\n",
    "    Type of text after soup: <class 'bs4.BeautifulSoup'>\n",
    "    \"\"\"\n",
    "    text = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p>Hello, world!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Use BeautifulSoup to convert text object type to soup object type\n",
    "    # from bs4 import BeautifulSoup\n",
    "    soup = bs(text, \"html.parser\")\n",
    "    # TODO: Use print() to print the type of text before and after conversion\n",
    "    print(f\"Type of text before soup: {type(text)}\")\n",
    "    print(f\"Type of text after soup: {type(soup)}\")\n",
    "convert_text_to_soup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><body><div><p>Hello, world!</p></div></body></html>\n",
      "<html><body><div><p>Hello, world!</p></div></body></html>\n",
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <p>\n",
      "    Hello, world!\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_pretty():\n",
    "    \"\"\"\n",
    "    Objective: Compare the print output with and without BeautifulSoup.prettify() method.\n",
    "    Expected Output:\n",
    "    Type of text before soup: <class 'str'>\n",
    "    Type of text after soup: <class 'bs4.BeautifulSoup'>\n",
    "    \"\"\"\n",
    "    text = \"\"\"<html><body><div><p>Hello, world!</p></div></body></html>\"\"\"\n",
    "    # TODO: Use BeautifulSoup to convert text object type to soup object type\n",
    "    soup = bs(text, \"html.parser\")\n",
    "    # TODO: Print text\n",
    "    print(text)\n",
    "    # TODO: Print soup directly\n",
    "    print(soup)\n",
    "    # TODO: Print using prettify method\n",
    "    print(soup.prettify())\n",
    "print_pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<div>\n",
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n",
      "</div>\n",
      "</body>\n",
      "<div>\n",
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n",
      "</div>\n",
      "[<p class=\"my-class\">Hello, my class!</p>, <p id=\"my-id\">Hello, my id!</p>]\n"
     ]
    }
   ],
   "source": [
    "def find_going_down():\n",
    "    \"\"\"\n",
    "    Objective: Demonstrate how to traverse downward in the HTML structure using `.find()` method.\n",
    "    Expected Output:\n",
    "    The <body>, <div>, and <p> tags in sequence as they are traversed.\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p id=\"my-id\">Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Navigate soup to get <body> and print it\n",
    "    body = soup.find(\"body\")\n",
    "    print(body)\n",
    "    # TODO: Navigate body to get <div> and print it\n",
    "    div = body.find(\"div\")\n",
    "    print(div)\n",
    "    # TODO: Navigate div to get <p> and print it \n",
    "    print(div.find_all(\"p\"))\n",
    "\n",
    "find_going_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p>Hello, my id!</p>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_next_to():\n",
    "    \"\"\"\n",
    "    Objective: Extract the text of the <p> element that comes immediately after a specific <p>.\n",
    "    Expected Output:\n",
    "    <p>Hello, my id!</p>\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p>Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find()` to locate the <p> element with class=\"my-class\"\n",
    "    my_class = soup.find(\"p\", \"my-class\")\n",
    "    # TODO: Use `.find_next()` to locate the next <p> element\n",
    "    return my_class.find_next()\n",
    "find_next_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n"
     ]
    }
   ],
   "source": [
    "def use_css_selectors():\n",
    "    \"\"\"\n",
    "    Objective: Locate elements using CSS selectors.\n",
    "    Expected Output:\n",
    "    <p class=\"my-class\">Hello, my class!</p>\n",
    "    <p id=\"my-id\">Hello, my id!</p>\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p id=\"my-id\">Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.select_one()` to locate elements using class, then print them.\n",
    "    print(soup.select_one(\".my-class\"))\n",
    "    # TODO: Use `.select_one()` to locate elements using ID selectors, then print them.\n",
    "    print(soup.select_one(\"#my-id\"))\n",
    "    \n",
    "use_css_selectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "def extract_text():\n",
    "    \"\"\"\n",
    "    Objective: Extract and print the text content of a <p> element.\n",
    "    Expected Output:\n",
    "    Hello, world!\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p>Hello, world!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find()` to locate the <p> element\n",
    "    p = soup.find(\"p\")\n",
    "    # TODO: Extract text from the <p> element and print it\n",
    "    print(p.string)\n",
    "extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['class', 'href', 'target'])\n",
      "https://www.google.com\n"
     ]
    }
   ],
   "source": [
    "def extract_attributes():\n",
    "    \"\"\"\n",
    "    Objective: Extract and print the text content of a <p> element.\n",
    "    Expected Output:\n",
    "    dict_keys(['href', 'class'])\n",
    "    https://www.google.com\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <a class=\"my-link\" href=\"https://www.google.com\" target=\"_blank\">Google</a>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find()` to locate the <a> element\n",
    "    a = soup.find(\"a\")\n",
    "    # TODO: Print all available attributes from the <a> element\n",
    "    attrs = a.attrs\n",
    "    print(attrs.keys())\n",
    "    # TODO: Print the href attribute from the <a> element\n",
    "    print(attrs['href'])\n",
    "extract_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item 1', 'Item 2', 'Item 3']\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_list():\n",
    "    \"\"\"\n",
    "    Objective: Extract text from all <li> elements and return them as a list of strings.\n",
    "    Expected Output:\n",
    "    ['Item 1', 'Item 2', 'Item 3']\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <ul>\n",
    "            <li>Item 1</li>\n",
    "            <li>Item 2</li>\n",
    "            <li>Item 3</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find_all()` to locate all <li> elements\n",
    "    li = soup.find_all(\"li\")\n",
    "    # TODO: Iterate over the <li> elements and extract their text into a list.\n",
    "    result = [item.string for item in li]\n",
    "    # TODO: Print the list\n",
    "    print(result)\n",
    "extract_text_from_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'John Doe', 'age': '25'}, {'name': 'Nadia', 'age': '31'}, {'name': 'Serena', 'age': '23'}, {'name': 'Tessa', 'age': '17'}, {'name': 'Una', 'age': '23'}]\n"
     ]
    }
   ],
   "source": [
    "def find_all_going_down():\n",
    "    \"\"\"\n",
    "    Objective: Extract text from all <p> elements and return them as a list of dictionaries.\n",
    "    Expected Output:\n",
    "    [{'name': 'John Doe', 'age': '25'}, {'name': 'Nadia', 'age': '31'}, {'name': 'Serena', 'age': '23'}, {'name': 'Tessa', 'age': '17'}, {'name': 'Una', 'age': '23'}]\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <section>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">John Doe</p>\n",
    "                <p class=\"my-age\">25</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Nadia</p>\n",
    "                <p class=\"my-age\">31</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Serena</p>\n",
    "                <p class=\"my-age\">23</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Tessa</p>\n",
    "                <p class=\"my-age\">17</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Una</p>\n",
    "                <p class=\"my-age\">23</p>\n",
    "            </div>\n",
    "        </section>\"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find_all()` to locate all <p> elements\n",
    "    result = []\n",
    "    p_names = soup.find_all(\"p\", \"my-name\")\n",
    "    p_ages = soup.find_all(\"p\", \"my-age\")\n",
    "    # TODO: Iterate over the <p> elements and extract their text into a list of dictionaries\n",
    "    for i in range(len(p_names)):\n",
    "        data = {\n",
    "            'name': p_names[i].string,\n",
    "            'age': p_ages[i].string\n",
    "        }\n",
    "        result.append(data)\n",
    "    # TODO: Print the list of dictionaries\n",
    "    print(result)\n",
    "    \n",
    "find_all_going_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n",
      "[{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_tables():\n",
    "    \"\"\"\n",
    "    Objective: Extract data from an HTML table and return it as a list of dictionaries.\n",
    "    Expected Output:\n",
    "    [{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Name</th>\n",
    "                <th>Age</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Alice</td>\n",
    "                <td>30</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Bob</td>\n",
    "                <td>25</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Extract <tr> elements from <tbody>\n",
    "    tr = soup.tbody.find_all(\"tr\")\n",
    "    # TODO: Iterate over the <td> elements, to construct dictionaries for each row.\n",
    "    result = []\n",
    "    for item in tr:\n",
    "        td = item.find_all(\"td\")\n",
    "        data = {\n",
    "            'name': td[0].string,\n",
    "            'age': td[1].string\n",
    "        }\n",
    "    # TODO: Append each dictionary to a list\n",
    "        result.append(data)\n",
    "    # TODO: Print the list\n",
    "    print(result)\n",
    "    # Challenges: Can you extract it directly from the <tr> elements?\n",
    "    tr = soup.find_all(\"tr\")[1:]\n",
    "    result = []\n",
    "    for item in tr:\n",
    "        td = item.find_all(\"td\")\n",
    "        data = {\n",
    "            'name': td[0].string,\n",
    "            'age': td[1].string\n",
    "        }\n",
    "        result.append(data)\n",
    "    print(result)\n",
    "\n",
    "extract_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 123, 'name': 'Alice'}\n"
     ]
    }
   ],
   "source": [
    "def extract_scripts():\n",
    "    \"\"\"\n",
    "    Objective: Extract JSON-like data embedded in a <script> tag and return it as a Python dictionary.\n",
    "    Expected Output:\n",
    "    {\"id\": 123, \"name\": \"Alice\"}\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            var userInfo = { \"id\": 123, \"name\": \"Alice\" };\n",
    "        </script>\n",
    "        \"\"\"\n",
    "\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Extract the JSON-like content from the <script> tag\n",
    "    text = soup.script.string\n",
    "    # TODO: Remove the \"var userInfo = \" and \";\" from the JSON-like content\n",
    "    text = text.replace(\"var userInfo = \", \"\").replace(\";\", \"\").strip()\n",
    "    # TODO: Convert the JSON-like content to a Python dictionary\n",
    "    import json\n",
    "    result = json.loads(text)\n",
    "    # TODO: Print the dictionary\n",
    "    print(result)\n",
    "extract_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " User ID: 67890 \n"
     ]
    }
   ],
   "source": [
    "def extract_comments():\n",
    "    \"\"\"\n",
    "    Objective: Extract a comment from the HTML and return it as a string.\n",
    "    Expected Output:\n",
    "    ' User ID: 67890 '\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <!-- User ID: 67890 -->\n",
    "        <div class=\"user-info\">Name: John Doe</div>\n",
    "        \"\"\"\n",
    "\n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use BeautifulSoup to locate and extract the comment.\n",
    "    comment = soup.contents[1] \n",
    "    # TODO: Print the comment\n",
    "    print(comment)\n",
    "extract_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item 1', 'Item 2', 'Item 3']\n"
     ]
    }
   ],
   "source": [
    "def extract_dynamic_classes():\n",
    "    \"\"\"\n",
    "    Objective: Extract text content from all <div> elements with class names starting with 'content-'.\n",
    "    Expected Output:\n",
    "    ['Item 1', 'Item 2', 'Item 3']\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <div class=\"content-1\">Item 1</div>\n",
    "        <div class=\"content-2\">Item 2</div>\n",
    "        <div class=\"content-3\">Item 3</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # TODO: Convert html to soup\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    # TODO: Use `.find_all()` with a custom filter to locate the elements\n",
    "    import re\n",
    "    data = soup.find_all(\"div\", attrs={'class': re.compile(\"content\")})\n",
    "    # TODO: Iterate over the elements and extract text from the specified <div> elements.\n",
    "    result = [item.string for item in data]\n",
    "    # TODO: Print the list\n",
    "    print(result)\n",
    "\n",
    "extract_dynamic_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "Which one method in BeautifulSoup you prefer? .find() or .select_one() ?\n",
    "\n",
    "I prefer using .find() when I know the element I need to find is a simple element that doesn't require CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Automate the process of getting HTML content by using Requests library. Read the official documentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
