{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parsing HTML**\n",
    "\n",
    "BeautifulSoup is a Python library that simplifies the process of web scraping by allowing developers to extract data from HTML documents easily. It transforms complicated HTML documents into a tree of Python objects, such as tags, navigable strings, and comments. This makes it straightforward to locate and manipulate the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs, Comment\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Practice Exercise: BeautifulSoup Basics\n",
    "\n",
    "Complete each function below by following the TODO instructions. \n",
    "Each function includes the objective of the task and the expected output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of text before soup: <class 'str'>\n",
      "Type of text after soup: <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_text_to_soup():\n",
    "    \"\"\"\n",
    "    Objective: Convert the provided text (HTML content) into a BeautifulSoup object.\n",
    "    Expected Output:\n",
    "    Type of text before soup: <class 'str'>\n",
    "    Type of text after soup: <class 'bs4.BeautifulSoup'>\n",
    "    \"\"\"\n",
    "    text = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p>Hello, world!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Use BeautifulSoup to convert text object type to soup object type\n",
    "    print(f\"Type of text before soup: {type(text)}\")\n",
    "    # TODO: Use print() to print the type of text before and after conversion\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    print(f\"Type of text after soup: {type(soup)}\")\n",
    "\n",
    "convert_text_to_soup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "<html><body><div><p>Hello, world!</p></div></body></html>\n",
      "\n",
      "Soup direct print:\n",
      "<html><body><div><p>Hello, world!</p></div></body></html>\n",
      "\n",
      "Prettified output:\n",
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <p>\n",
      "    Hello, world!\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_pretty():\n",
    "    \"\"\"\n",
    "    Objective: Compare the print output with and without BeautifulSoup.prettify() method.\n",
    "    Expected Output:\n",
    "    Type of text before soup: <class 'str'>\n",
    "    Type of text after soup: <class 'bs4.BeautifulSoup'>\n",
    "    \"\"\"\n",
    "    text = \"\"\"<html><body><div><p>Hello, world!</p></div></body></html>\"\"\"\n",
    "    # TODO: Use BeautifulSoup to convert text object type to soup object type\n",
    "    # TODO: Print text\n",
    "    # TODO: Print soup directly\n",
    "    # TODO: Print using prettify method\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Print original text\n",
    "    print(\"Original text:\")\n",
    "    print(text)\n",
    "    print(\"\\nSoup direct print:\")\n",
    "    print(soup)\n",
    "    print(\"\\nPrettified output:\")\n",
    "    print(soup.prettify())\n",
    "\n",
    "print_pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body tag:\n",
      "<body>\n",
      "<div>\n",
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n",
      "</div>\n",
      "</body>\n",
      "\n",
      "Div tag:\n",
      "<div>\n",
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n",
      "</div>\n",
      "\n",
      "First p tag:\n",
      "<p class=\"my-class\">Hello, my class!</p>\n"
     ]
    }
   ],
   "source": [
    "def find_going_down():\n",
    "    \"\"\"\n",
    "    Objective: Demonstrate how to traverse downward in the HTML structure using `.find()` method.\n",
    "    Expected Output:\n",
    "    The <body>, <div>, and <p> tags in sequence as they are traversed.\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p id=\"my-id\">Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Navigate soup to get <body> and print it\n",
    "    body = soup.find('body')\n",
    "    print(\"Body tag:\")\n",
    "    print(body)\n",
    "    # TODO: Navigate body to get <div> and print it\n",
    "    div = body.find('div')\n",
    "    print(\"\\nDiv tag:\")\n",
    "    print(div)\n",
    "    # TODO: Navigate div to get <p> and print it   \n",
    "    p = div.find('p')\n",
    "    print(\"\\nFirst p tag:\")\n",
    "    print(p)\n",
    "\n",
    "\n",
    "find_going_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Hello, my id!</p>\n"
     ]
    }
   ],
   "source": [
    "def find_next_to():\n",
    "    \"\"\"\n",
    "    Objective: Extract the text of the <p> element that comes immediately after a specific <p>.\n",
    "    Expected Output:\n",
    "    <p>Hello, my id!</p>\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p>Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.find()` to locate the <p> element with class=\"my-class\"\n",
    "    first_p = soup.find('p', class_='my-class')\n",
    "    # TODO: Use `.find_next()` to locate the next <p> element\n",
    "    next_p = first_p.find_next('p')\n",
    "    print(next_p)\n",
    "    \n",
    "find_next_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"my-class\">Hello, my class!</p>\n",
      "<p id=\"my-id\">Hello, my id!</p>\n"
     ]
    }
   ],
   "source": [
    "def use_css_selectors():\n",
    "    \"\"\"\n",
    "    Objective: Locate elements using CSS selectors.\n",
    "    Expected Output:\n",
    "    <p class=\"my-class\">Hello, my class!</p>\n",
    "    <p id=\"my-id\">Hello, my id!</p>\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p class=\"my-class\">Hello, my class!</p>\n",
    "                    <p id=\"my-id\">Hello, my id!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.select_one()` to locate elements using class, then print them.\n",
    "    class_element = soup.select_one('p.my-class')\n",
    "    print(class_element)\n",
    "    # TODO: Use `.select_one()` to locate elements using ID selectors, then print them.\n",
    "    id_element = soup.select_one('p#my-id')\n",
    "    print(id_element)\n",
    "\n",
    "use_css_selectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "def extract_text():\n",
    "    \"\"\"\n",
    "    Objective: Extract and print the text content of a <p> element.\n",
    "    Expected Output:\n",
    "    Hello, world!\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <p>Hello, world!</p>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.find()` to locate the <p> element\n",
    "    p_element = soup.find('p')\n",
    "    # TODO: Extract text from the <p> element and print it\n",
    "    print(p_element.text)\n",
    "\n",
    "extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['class', 'href', 'target'])\n",
      "https://www.google.com\n"
     ]
    }
   ],
   "source": [
    "def extract_attributes():\n",
    "    \"\"\"\n",
    "    Objective: Extract and print the text content of a <p> element.\n",
    "    Expected Output:\n",
    "    dict_keys(['href', 'class'])\n",
    "    https://www.google.com\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <div>\n",
    "                    <a class=\"my-link\" href=\"https://www.google.com\" target=\"_blank\">Google</a>\n",
    "                </div>\n",
    "            </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.find()` to locate the <a> element\n",
    "    a_element = soup.find('a')\n",
    "    # TODO: Print all available attributes from the <a> element\n",
    "    print(a_element.attrs.keys())\n",
    "    # TODO: Print the href attribute from the <a> element\n",
    "    print(a_element['href'])\n",
    "\n",
    "extract_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item 1', 'Item 2', 'Item 3']\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_list():\n",
    "    \"\"\"\n",
    "    Objective: Extract text from all <li> elements and return them as a list of strings.\n",
    "    Expected Output:\n",
    "    ['Item 1', 'Item 2', 'Item 3']\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <ul>\n",
    "            <li>Item 1</li>\n",
    "            <li>Item 2</li>\n",
    "            <li>Item 3</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.find_all()` to locate all <li> elements \n",
    "    li_elements = soup.find_all('li')\n",
    "    # TODO: Iterate over the <li> elements and extract their text into a list.\n",
    "    items = [li.text for li in li_elements]\n",
    "    # TODO: Print the list\n",
    "    print(items)\n",
    "\n",
    "extract_text_from_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'John Doe', 'age': '25'}, {'name': 'Nadia', 'age': '31'}, {'name': 'Serena', 'age': '23'}, {'name': 'Tessa', 'age': '17'}, {'name': 'Una', 'age': '23'}]\n"
     ]
    }
   ],
   "source": [
    "def find_all_going_down():\n",
    "    \"\"\"\n",
    "    Objective: Extract text from all <p> elements and return them as a list of dictionaries.\n",
    "    Expected Output:\n",
    "    [{'name': 'John Doe', 'age': '25'}, {'name': 'Nadia', 'age': '31'}, {'name': 'Serena', 'age': '23'}, {'name': 'Tessa', 'age': '17'}, {'name': 'Una', 'age': '23'}]\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <section>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">John Doe</p>\n",
    "                <p class=\"my-age\">25</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Nadia</p>\n",
    "                <p class=\"my-age\">31</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Serena</p>\n",
    "                <p class=\"my-age\">23</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Tessa</p>\n",
    "                <p class=\"my-age\">17</p>\n",
    "            </div>\n",
    "            <div class=\"user-info\">\n",
    "                <p class=\"my-name\">Una</p>\n",
    "                <p class=\"my-age\">23</p>\n",
    "            </div>\n",
    "        </section>\"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Use `.find_all()` to locate all <p> elements\n",
    "    user_divs = soup.find_all('div', class_='user-info')\n",
    "    # TODO: Iterate over the <p> elements and extract their text into a list of dictionaries\n",
    "    users = []\n",
    "    for div in user_divs:\n",
    "        name = div.find('p', class_='my-name').text\n",
    "        age = div.find('p', class_='my-age').text\n",
    "        users.append({'name': name, 'age': age})\n",
    "    \n",
    "    # TODO: Print the list of dictionaries\n",
    "    print(users)\n",
    "    \n",
    "find_all_going_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_tables():\n",
    "    \"\"\"\n",
    "    Objective: Extract data from an HTML table and return it as a list of dictionaries.\n",
    "    Expected Output:\n",
    "    [{'name': 'Alice', 'age': '30'}, {'name': 'Bob', 'age': '25'}]\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "    <table>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Name</th>\n",
    "                <th>Age</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Alice</td>\n",
    "                <td>30</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Bob</td>\n",
    "                <td>25</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # TODO: Extract <tr> elements from <tbody>\n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "    # TODO: Iterate over the <td> elements, to construct dictionaries for each row.\n",
    "    table_data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        data = {\n",
    "            'name': cols[0].text,\n",
    "            'age': cols[1].text\n",
    "        }\n",
    "    # TODO: Append each dictionary to a list\n",
    "        table_data.append(data)    \n",
    "    # TODO: Print the list\n",
    "    print(table_data)\n",
    "    \n",
    "    # Challenges: Can you extract it directly from the <tr> elements?\n",
    "\n",
    "extract_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 123, 'name': 'Alice'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_scripts():\n",
    "    \"\"\"\n",
    "    Objective: Extract JSON-like data embedded in a <script> tag and return it as a Python dictionary.\n",
    "    Expected Output:\n",
    "    {\"id\": 123, \"name\": \"Alice\"}\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <script>\n",
    "            var userInfo = { \"id\": 123, \"name\": \"Alice\" };\n",
    "        </script>\n",
    "        \"\"\"\n",
    "\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # TODO: Extract the JSON-like content from the <script> tag        \n",
    "    # TODO: Remove the \"var userInfo = \" and \";\" from the JSON-like content\n",
    "    # TODO: Convert the JSON-like content to a Python dictionary\n",
    "    # TODO: Print the dictionary\n",
    "    script_content = soup.find('script').string\n",
    "    json_str = script_content.strip().replace('var userInfo = ', '').rstrip(';')\n",
    "    data = json.loads(json_str)\n",
    "    print(data)\n",
    "    \n",
    "\n",
    "extract_scripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " User ID: 67890 \n"
     ]
    }
   ],
   "source": [
    "def extract_comments():\n",
    "    from bs4 import Comment\n",
    "\n",
    "    \"\"\"\n",
    "    Objective: Extract a comment from the HTML and return it as a string.\n",
    "    Expected Output:\n",
    "    ' User ID: 67890 '\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <!-- User ID: 67890 -->\n",
    "        <div class=\"user-info\">Name: John Doe</div>\n",
    "        \"\"\"\n",
    "\n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # TODO: Use BeautifulSoup to locate and extract the comment.\n",
    "    comment = soup.find(string=lambda text: isinstance(text, Comment))\n",
    "    \n",
    "    # TODO: Print the comment\n",
    "    print(comment)\n",
    "    \n",
    "\n",
    "extract_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Item 1', 'Item 2', 'Item 3']\n"
     ]
    }
   ],
   "source": [
    "def extract_dynamic_classes():\n",
    "    \"\"\"\n",
    "    Objective: Extract text content from all <div> elements with class names starting with 'content-'.\n",
    "    Expected Output:\n",
    "    ['Item 1', 'Item 2', 'Item 3']\n",
    "    \"\"\"\n",
    "    html = \"\"\"\n",
    "        <div class=\"content-1\">Item 1</div>\n",
    "        <div class=\"content-2\">Item 2</div>\n",
    "        <div class=\"content-3\">Item 3</div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # TODO: Convert html to soup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # TODO: Use `.find_all()` with a custom filter to locate the elements\n",
    "    divs = soup.find_all('div', class_=lambda x: x and x.startswith('content-'))\n",
    "     \n",
    "    # TODO: Iterate over the elements and extract text from the specified <div> elements.\n",
    "    items = [div.text for div in divs]\n",
    "    \n",
    "    # TODO: Print the list\n",
    "    print(items)\n",
    "    \n",
    "\n",
    "extract_dynamic_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "Which one method in BeautifulSoup you prefer? .find() or .select_one() ?\n",
    "\n",
    "(answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I prefer .find() over .select_one() for several reasons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Readability : .find() has a more straightforward syntax that clearly shows what you're looking for. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# More readable\u001b[39;00m\n\u001b[0;32m      2\u001b[0m soup\u001b[38;5;241m.\u001b[39mselect_one(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv.content\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "soup.find('div', class_='content')  # More readable\n",
    "soup.select_one('div.content')      # Less intuitive for beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Flexibility : .find() offers more flexible filtering options through its parameters:\n",
    "\n",
    "- Can use dictionaries for multiple attributes\n",
    "- Supports custom filtering functions\n",
    "- Easier to combine multiple conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Performance : .find() is generally faster for simple searches because it doesn't need to parse CSS selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, .select_one() is better when:\n",
    "\n",
    "- You need to use complex CSS selectors\n",
    "- You're working with nested structures where CSS paths are clearer\n",
    "- You're already familiar with CSS selector syntax\n",
    "\n",
    "For most basic web scraping tasks, .find() is sufficient and more maintainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Automate the process of getting HTML content by using Requests library. Read the official documentations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install :\n",
    "#pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key features demonstrated:\n",
    "\n",
    "1. Making HTTP GET requests\n",
    "2. Error handling for network issues\n",
    "3. Status code checking\n",
    "4. Converting response to BeautifulSoup object\n",
    "5. Basic error handling\n",
    "The code shows how to:\n",
    "\n",
    "- Send HTTP requests to web pages\n",
    "- Handle potential network errors\n",
    "- Parse the received HTML content\n",
    "- Extract specific information from the parsed content\n",
    "Remember to respect websites' robots.txt and implement appropriate delays between requests when scraping multiple pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_webpage(url):\n",
    "    try:\n",
    "        # Send GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with a real website\n",
    "    url = \"https://jakarta.go.id\"\n",
    "    soup = fetch_webpage(url)\n",
    "    \n",
    "    if soup:\n",
    "        # Find and print all h2 headings\n",
    "        headings = soup.find_all('h2')\n",
    "        print(\"Main headings on Python.org:\")\n",
    "        for heading in headings:\n",
    "            print(f\"- {heading.text.strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
