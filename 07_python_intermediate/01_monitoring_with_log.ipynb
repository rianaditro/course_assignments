{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Python Logging**\n",
    "\n",
    "The logging module in Python provides a flexible framework for emitting log messages from your code. Logs are essential for understanding and debugging your program, especially in production environments or when you're working with complex systems like web scraping.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why Use Logging?**\n",
    "1. **Debugging:** Helps in tracking program execution without cluttering the code with `print()` statements.\n",
    "2. **Persistence:** Logs can be saved to a file, enabling analysis after the program finishes.\n",
    "3. **Control:** You can set logging levels to filter messages based on their importance.\n",
    "4. **Structured Output:** With proper configuration, logs can include timestamps, severity levels, and more.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Basic Concepts in Logging**\n",
    "1. **Loggers:** The main entry point for logging. You can think of them as entities that emit log messages.\n",
    "2. **Handlers:** Define where the log messages go (console, file, etc.).\n",
    "3. **Levels:** Determine the severity of a log message. Common levels are:\n",
    "   - `DEBUG`: Detailed information for diagnosing problems.\n",
    "   - `INFO`: Confirmation that things are working as expected.\n",
    "   - `WARNING`: An indication of something unexpected or an issue that isn’t critical yet.\n",
    "   - `ERROR`: A serious problem that prevents the program from continuing.\n",
    "   - `CRITICAL`: A very serious error, often indicating a program crash.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Basic Logging Example**\n",
    "\n",
    "Here’s how to get started with Python's logging module:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "\n",
    "# Set up a basic logger\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the minimum logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Define the log message format\n",
    ")\n",
    "\n",
    "# Example log messages\n",
    "logging.debug(\"This is a debug message. Used for detailed diagnostic output.\")\n",
    "logging.info(\"This is an info message. Indicates the program is running as expected.\")\n",
    "logging.warning(\"This is a warning message. Something unexpected happened.\")\n",
    "logging.error(\"This is an error message. A problem occurred.\")\n",
    "logging.critical(\"This is a critical message. A serious error happened.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Output Explanation**\n",
    "When you run the code, you'll see output like this:\n",
    "\n",
    "```\n",
    "2024-12-23 14:23:01,123 - DEBUG - This is a debug message. Used for detailed diagnostic output.\n",
    "2024-12-23 14:23:01,124 - INFO - This is an info message. Indicates the program is running as expected.\n",
    "2024-12-23 14:23:01,125 - WARNING - This is a warning message. Something unexpected happened.\n",
    "2024-12-23 14:23:01,126 - ERROR - This is an error message. A problem occurred.\n",
    "2024-12-23 14:23:01,127 - CRITICAL - This is a critical message. A serious error happened.\n",
    "```\n",
    "\n",
    "- **Timestamp:** Indicates when the log was recorded.\n",
    "- **Log Level:** Shows the severity of the log message.\n",
    "- **Message:** The custom message provided.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Functions**\n",
    "1. **`logging.basicConfig()`**: Sets up the configuration for logging.\n",
    "2. **Logging methods:** These emit messages with a severity level:\n",
    "   - `logging.debug()`\n",
    "   - `logging.info()`\n",
    "   - `logging.warning()`\n",
    "   - `logging.error()`\n",
    "   - `logging.critical()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping completed successfully.\n",
      "Scraping page 2\n",
      "Scraping completed successfully.\n",
      "Scraping page 3\n",
      "Scraping completed successfully.\n",
      "Scraping page 4\n",
      "Scraping completed successfully.\n",
      "Scraping page 5\n",
      "Scraping completed successfully.\n",
      "Scraping page 6\n",
      "Scraping completed successfully.\n",
      "Scraping page 7\n",
      "Scraping completed successfully.\n",
      "Scraping page 8\n",
      "Scraping completed successfully.\n",
      "Scraping page 9\n",
      "Scraping completed successfully.\n",
      "Scraping page 10\n",
      "Scraping completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Example of scraping process\n",
    "\"\"\"\n",
    "def start_scraping():\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Scraping page {i}\")\n",
    "        time.sleep(1)\n",
    "        # Getting page response\n",
    "        print(\"Scraping completed successfully.\")\n",
    "\n",
    "start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 15:57:02,297 - INFO - Scraping page 1\n",
      "2025-03-09 15:57:03,300 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:03,301 - INFO - Scraping page 2\n",
      "2025-03-09 15:57:04,302 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:04,305 - INFO - Scraping page 3\n",
      "2025-03-09 15:57:05,308 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:05,310 - INFO - Scraping page 4\n",
      "2025-03-09 15:57:06,314 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:06,315 - INFO - Scraping page 5\n",
      "2025-03-09 15:57:07,317 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:07,323 - INFO - Scraping page 6\n",
      "2025-03-09 15:57:08,327 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:08,328 - INFO - Scraping page 7\n",
      "2025-03-09 15:57:09,330 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:09,334 - INFO - Scraping page 8\n",
      "2025-03-09 15:57:10,342 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:10,346 - INFO - Scraping page 9\n",
      "2025-03-09 15:57:11,349 - INFO - Scraping completed successfully.\n",
      "2025-03-09 15:57:11,350 - INFO - Scraping page 10\n",
      "2025-03-09 15:57:12,352 - INFO - Scraping completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Understand the basics of Python's logging module and why it's important. \n",
    "Logging helps you monitor your program's behavior and debug issues without relying on print statements.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Set up a basic logger that logs messages at the INFO level.\n",
    "# 2. Replace the start_scraping print statement with logging message.\n",
    "# 3. Log the following messages with info level:\n",
    "#    - \"Scraping page \" following by the page number\n",
    "#    - \"Scraping completed successfully.\"\n",
    "\n",
    "# Set up basic logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def start_scraping():\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1, 11):\n",
    "        logging.info(f\"Scraping page {i}\")\n",
    "        time.sleep(1)\n",
    "        # Getting page response\n",
    "        logging.info(\"Scraping completed successfully.\")\n",
    "\n",
    "start_scraping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 16:04:05,462 - INFO - Page 1 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:06,465 - INFO - Page 2 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:07,470 - INFO - Page 3 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:08,472 - ERROR - Failed to scrape page 4. Status code: 404\n",
      "2025-03-09 16:04:09,477 - INFO - Page 5 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:10,481 - INFO - Page 6 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:11,483 - INFO - Page 7 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:12,488 - INFO - Page 8 scraped successfully. Status code: 200\n",
      "2025-03-09 16:04:13,492 - ERROR - Failed to scrape page 9. Status code: 503\n",
      "2025-03-09 16:04:14,497 - ERROR - Failed to scrape page 10. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Setup different logs level\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Setup a logger that only log error messages\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "def scraping_with_error_response():\n",
    "    response_code = [200, 200, 200, 200, 200, 404, 503]\n",
    "\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1,11):\n",
    "        # TODO: Add log message for tracking page number\n",
    "        logging.debug(f\"Scraping page {i}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting page response\n",
    "        response = random.choice(response_code)\n",
    "        \n",
    "        if response == 200:   \n",
    "            # TODO: Add log message for valid response\n",
    "            logging.info(f\"Page {i} scraped successfully. Status code: {response}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            # TODO: Add log message for invalid response\n",
    "            logging.error(f\"Failed to scrape page {i}. Status code: {response}\")\n",
    "\n",
    "\n",
    "\n",
    "scraping_with_error_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 16:06:02,414 - INFO - Page 1 scraped successfully\n",
      "2025-03-09 16:06:03,416 - INFO - Page 2 scraped successfully\n",
      "2025-03-09 16:06:04,418 - ERROR - Failed to scrape page 3. Status code: 404\n",
      "2025-03-09 16:06:05,423 - ERROR - Failed to scrape page 4. Status code: 503\n",
      "2025-03-09 16:06:06,427 - INFO - Page 5 scraped successfully\n",
      "2025-03-09 16:06:07,430 - ERROR - Failed to scrape page 6. Status code: 503\n",
      "2025-03-09 16:06:08,433 - INFO - Page 7 scraped successfully\n",
      "2025-03-09 16:06:09,434 - INFO - Page 8 scraped successfully\n",
      "2025-03-09 16:06:10,436 - INFO - Page 9 scraped successfully\n",
      "2025-03-09 16:06:11,439 - INFO - Page 10 scraped successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Learn to configure logging to log messages to a file for persistent records. \n",
    "This is useful for analyzing scraping sessions or debugging after the program runs.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Configure logging to log messages at the DEBUG level to a file named `scraper.log`.\n",
    "# 2. Add timestamps to the log messages.\n",
    "# 3. Use previous function for this task\n",
    "\n",
    "# Configure logging to write to file with DEBUG level and timestamps\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='scraper.log',\n",
    "    filemode='w'\n",
    ")\n",
    "\n",
    "def scraping_with_error_response():\n",
    "    response_code = [200, 200, 200, 200, 200, 404, 503]\n",
    "\n",
    "    # Scraping page 1 to 10\n",
    "    for i in range(1,11):\n",
    "        logging.debug(f\"Scraping page {i}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting page response\n",
    "        response = random.choice(response_code)\n",
    "        \n",
    "        if response == 200:   \n",
    "            logging.info(f\"Page {i} scraped successfully\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to scrape page {i}. Status code: {response}\")\n",
    "\n",
    "scraping_with_error_response()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 16:07:49,950 - INFO - Starting web scraping process\n",
      "2025-03-09 16:07:50,034 - INFO - Successfully scraped https://www.python.org\n",
      "2025-03-09 16:07:50,285 - INFO - Successfully scraped https://www.github.com\n",
      "2025-03-09 16:07:51,031 - ERROR - Failed to scrape https://this-is-invalid-url.com: HTTPSConnectionPool(host='this-is-invalid-url.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F941921370>: Failed to resolve 'this-is-invalid-url.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "2025-03-09 16:07:51,138 - INFO - Successfully scraped https://www.wikipedia.org\n",
      "2025-03-09 16:07:51,140 - INFO - Web scraping process completed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Objective: Apply logging to a full scraping workflow and use different logging levels for various stages.\n",
    "This will help you monitor and troubleshoot scraping operations more effectively.\n",
    "\"\"\"\n",
    "\n",
    "# TODO:\n",
    "# 1. Write a script that:\n",
    "#    - Logs INFO when scraping starts.\n",
    "#    - Logs DEBUG for each URL being processed.\n",
    "#    - Logs ERROR if a request fails.\n",
    "#    - Logs INFO when scraping ends.\n",
    "# 2. Scrape data from multiple URLs, including one invalid URL to test the error logging.\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def scrape_urls(urls: List[str]) -> None:\n",
    "    logging.info(\"Starting web scraping process\")\n",
    "    \n",
    "    for url in urls:\n",
    "        logging.debug(f\"Processing URL: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            logging.info(f\"Successfully scraped {url}\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Failed to scrape {url}: {str(e)}\")\n",
    "            \n",
    "    logging.info(\"Web scraping process completed\")\n",
    "\n",
    "# Test URLs (including one invalid)\n",
    "urls_to_scrape = [\n",
    "    \"https://www.python.org\",\n",
    "    \"https://www.github.com\",\n",
    "    \"https://this-is-invalid-url.com\",  # Invalid URL\n",
    "    \"https://www.wikipedia.org\"\n",
    "]\n",
    "\n",
    "# Run the scraper\n",
    "scrape_urls(urls_to_scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective: Explore advanced logging by using custom handlers to log messages to multiple destinations. \n",
    "This technique improves flexibility in handling log output.\n",
    "\"\"\"\n",
    "# Create handlers\n",
    "console_handler = logging.StreamHandler() # This will shows log message in the console\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: \n",
    "# 1. Create another handler for storing log in a file using logging.FileHandler('error.log')\n",
    "# 2. Set the level to DEBUG\n",
    "file_handler = logging.FileHandler('error.log')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Attach formatter to handlers\n",
    "console_handler.setFormatter(formatter)\n",
    "# TODO: Add formatter to the file handler\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "\n",
    "# Create logger and attach handlers\n",
    "logger = logging.getLogger('ScraperLogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(console_handler) # Attach stream handler into the logger object\n",
    "# TODO: Attach the file handler into the logger object\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 16:12:52,213 - INFO - Attempting to scrape https://example1.com\n",
      "2025-03-09 16:12:52,213 - INFO - Attempting to scrape https://example1.com\n",
      "2025-03-09 16:12:52,217 - ERROR - Failed to scrape https://example1.com (Status: 404, Attempt: 1)\n",
      "2025-03-09 16:12:52,217 - ERROR - Failed to scrape https://example1.com (Status: 404, Attempt: 1)\n",
      "2025-03-09 16:12:53,224 - INFO - Attempting to scrape https://example1.com\n",
      "2025-03-09 16:12:53,224 - INFO - Attempting to scrape https://example1.com\n",
      "2025-03-09 16:12:53,228 - INFO - Successfully scraped https://example1.com\n",
      "2025-03-09 16:12:53,228 - INFO - Successfully scraped https://example1.com\n",
      "2025-03-09 16:12:53,233 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:53,233 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:53,237 - ERROR - Failed to scrape https://example2.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:53,237 - ERROR - Failed to scrape https://example2.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:54,246 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:54,246 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:54,252 - ERROR - Failed to scrape https://example2.com (Status: 503, Attempt: 2)\n",
      "2025-03-09 16:12:54,252 - ERROR - Failed to scrape https://example2.com (Status: 503, Attempt: 2)\n",
      "2025-03-09 16:12:55,260 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:55,260 - INFO - Attempting to scrape https://example2.com\n",
      "2025-03-09 16:12:55,263 - INFO - Successfully scraped https://example2.com\n",
      "2025-03-09 16:12:55,263 - INFO - Successfully scraped https://example2.com\n",
      "2025-03-09 16:12:55,266 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:55,266 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:55,268 - ERROR - Failed to scrape https://example3.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:55,268 - ERROR - Failed to scrape https://example3.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:56,273 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:56,273 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:56,276 - ERROR - Failed to scrape https://example3.com (Status: 503, Attempt: 2)\n",
      "2025-03-09 16:12:56,276 - ERROR - Failed to scrape https://example3.com (Status: 503, Attempt: 2)\n",
      "2025-03-09 16:12:57,280 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:57,280 - INFO - Attempting to scrape https://example3.com\n",
      "2025-03-09 16:12:57,285 - ERROR - Failed to scrape https://example3.com (Status: 404, Attempt: 3)\n",
      "2025-03-09 16:12:57,285 - ERROR - Failed to scrape https://example3.com (Status: 404, Attempt: 3)\n",
      "2025-03-09 16:12:58,289 - INFO - Attempting to scrape https://example4.com\n",
      "2025-03-09 16:12:58,289 - INFO - Attempting to scrape https://example4.com\n",
      "2025-03-09 16:12:58,294 - ERROR - Failed to scrape https://example4.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:58,294 - ERROR - Failed to scrape https://example4.com (Status: 503, Attempt: 1)\n",
      "2025-03-09 16:12:59,308 - INFO - Attempting to scrape https://example4.com\n",
      "2025-03-09 16:12:59,308 - INFO - Attempting to scrape https://example4.com\n",
      "2025-03-09 16:12:59,314 - INFO - Successfully scraped https://example4.com\n",
      "2025-03-09 16:12:59,314 - INFO - Successfully scraped https://example4.com\n",
      "2025-03-09 16:12:59,326 - INFO - Attempting to scrape https://example5.com\n",
      "2025-03-09 16:12:59,326 - INFO - Attempting to scrape https://example5.com\n",
      "2025-03-09 16:12:59,333 - INFO - Successfully scraped https://example5.com\n",
      "2025-03-09 16:12:59,333 - INFO - Successfully scraped https://example5.com\n",
      "2025-03-09 16:12:59,345 - INFO - Saved 1 failed URLs to failed_urls.txt\n",
      "2025-03-09 16:12:59,345 - INFO - Saved 1 failed URLs to failed_urls.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Objective: Handling failed requests using logging\n",
    "\"\"\"\n",
    "# TODO:\n",
    "# 1. Create a function that loop through number and get the random response,\n",
    "# just like previous code but modify it as you like\n",
    "# 2. Handle stream log in the console and the error log in a file\n",
    "# 3. Provide a file that contains all of failed URL so you can retry again\n",
    "# 4. Automate the process (optional)\n",
    "\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "# Configure logging handlers\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler('error.log')\n",
    "file_handler.setLevel(logging.ERROR)\n",
    "\n",
    "# Create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Setup logger\n",
    "logger = logging.getLogger('ScrapeLogger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "def scrape_with_retry(urls: List[str], max_retries: int = 3) -> None:\n",
    "    failed_urls = []\n",
    "    \n",
    "    for url in urls:\n",
    "        success = False\n",
    "        retries = 0\n",
    "        \n",
    "        while not success and retries < max_retries:\n",
    "            logger.info(f\"Attempting to scrape {url}\")\n",
    "            response = random.choice([200, 404, 503])\n",
    "            \n",
    "            if response == 200:\n",
    "                logger.info(f\"Successfully scraped {url}\")\n",
    "                success = True\n",
    "            else:\n",
    "                retries += 1\n",
    "                logger.error(f\"Failed to scrape {url} (Status: {response}, Attempt: {retries})\")\n",
    "                time.sleep(1)  # Wait before retry\n",
    "        \n",
    "        if not success:\n",
    "            failed_urls.append(url)\n",
    "            \n",
    "    # Save failed URLs to file\n",
    "    if failed_urls:\n",
    "        with open('failed_urls.txt', 'w') as f:\n",
    "            for url in failed_urls:\n",
    "                f.write(f\"{url}\\n\")\n",
    "        logger.info(f\"Saved {len(failed_urls)} failed URLs to failed_urls.txt\")\n",
    "\n",
    "# Test URLs\n",
    "test_urls = [\n",
    "    \"https://example1.com\",\n",
    "    \"https://example2.com\",\n",
    "    \"https://example3.com\",\n",
    "    \"https://example4.com\",\n",
    "    \"https://example5.com\"\n",
    "]\n",
    "\n",
    "# Run scraper\n",
    "scrape_with_retry(test_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reflection**\n",
    "In what situation logging will help you a lot?\n",
    "\n",
    "(answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging is particularly helpful in several key situations:\n",
    "\n",
    "1. Debugging Complex Applications\n",
    "- Tracking program flow in multi-threaded applications\n",
    "- Understanding sequence of events leading to errors\n",
    "- Monitoring state changes in long-running processes\n",
    "2. Production Environment Monitoring\n",
    "- Tracking application performance\n",
    "- Identifying bottlenecks\n",
    "- Monitoring system health\n",
    "- Early detection of potential issues\n",
    "3. Web Scraping Projects\n",
    "- Tracking successful/failed requests\n",
    "- Monitoring rate limits\n",
    "- Recording data extraction progress\n",
    "- Managing retry mechanisms\n",
    "4. Distributed Systems\n",
    "- Tracing requests across multiple services\n",
    "- Debugging communication between components\n",
    "- Monitoring system synchronization\n",
    "5. Data Processing Pipelines\n",
    "- Tracking progress of long-running jobs\n",
    "- Monitoring data transformation steps\n",
    "- Recording processing errors\n",
    "- Validating data quality\n",
    "6. User Behavior Analysis\n",
    "- Recording user interactions\n",
    "- Tracking feature usage\n",
    "- Monitoring error patterns\n",
    "- Analyzing performance impact\n",
    "7. Automated Systems\n",
    "- Recording scheduled task execution\n",
    "- Monitoring background processes\n",
    "- Tracking automated decisions\n",
    "- Validating system responses\n",
    "8. Security Monitoring\n",
    "- Recording access attempts\n",
    "- Tracking unauthorized activities\n",
    "- Monitoring system changes\n",
    "- Logging security events\n",
    "The structured nature of logging, combined with different severity levels and persistent storage, makes it invaluable for maintaining and troubleshooting complex systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploration**\n",
    "Explore advanced log and monitoring tools like:\n",
    "- Loguru\n",
    "- Loggly\n",
    "- Datadog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an exploration of these advanced logging and monitoring tools:\n",
    "\n",
    "1. Loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-09 16:19:59.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting data processing\u001b[0m\n",
      "\u001b[32m2025-03-09 16:19:59.395\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m16\u001b[0m - \u001b[31m\u001b[1mProcessing failed: division by zero\u001b[0m\n",
      "\u001b[32m2025-03-09 16:19:59.397\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_data\u001b[0m:\u001b[36m17\u001b[0m - \u001b[32m\u001b[1mProcessing completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    "#Install extension Loguru\n",
    "# pip install loguru\n",
    "\n",
    "# Basic setup\n",
    "logger.add(\"app.log\", rotation=\"500 MB\", retention=\"10 days\")\n",
    "\n",
    "# Rich formatting and easy debugging\n",
    "@logger.catch\n",
    "def process_data():\n",
    "    logger.info(\"Starting data processing\")\n",
    "    try:\n",
    "        result = 1 / 0  # Intentional error\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")\n",
    "    logger.success(\"Processing completed\")\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Features:\n",
    "\n",
    "- Better formatting out of the box\n",
    "- Automatic exception catching\n",
    "- Log rotation and retention\n",
    "- Asynchronous logging\n",
    "- Structured logging support\n",
    "2. Loggly (Cloud-based logging service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'loggly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mloggly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhandlers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# pip install loggly\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Configure Loggly handler\u001b[39;00m\n\u001b[0;32m      7\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloggly_logger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'loggly'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import loggly.handlers\n",
    "\n",
    "# pip install loggly\n",
    "\n",
    "# Configure Loggly handler\n",
    "logger = logging.getLogger('loggly_logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Replace with your Loggly token\n",
    "handler = loggly.handlers.HTTPSHandler(\n",
    "    'YOUR_CUSTOMER_TOKEN.loggly.com/inputs/YOUR_TOKEN/',\n",
    "    'POST'\n",
    ")\n",
    "\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Log events\n",
    "logger.info('Application started')\n",
    "logger.error('An error occurred', extra={'custom_field': 'value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Features:\n",
    "\n",
    "- Centralized log management\n",
    "- Real-time log aggregation\n",
    "- Advanced search capabilities\n",
    "- Custom dashboards\n",
    "- Alert management\n",
    "3. Datadog (Full-scale monitoring platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datadog import initialize, statsd\n",
    "from ddtrace import patch_all\n",
    "\n",
    "# Initialize the Datadog agent\n",
    "initialize(\n",
    "    api_key='YOUR_API_KEY',\n",
    "    app_key='YOUR_APP_KEY'\n",
    ")\n",
    "\n",
    "# Enable automatic instrumentation\n",
    "patch_all()\n",
    "\n",
    "# Monitor metrics\n",
    "def process_request():\n",
    "    # Track timing\n",
    "    with statsd.timed('app.request_duration'):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "    # Track custom metrics\n",
    "    statsd.increment('app.request_count')\n",
    "    statsd.gauge('app.memory_usage', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Features:\n",
    "\n",
    "- Infrastructure monitoring\n",
    "- Application performance monitoring (APM)\n",
    "- Log management\n",
    "- Real-time metrics\n",
    "- Custom dashboards and alerts\n",
    "- Distributed tracing\n",
    "Comparison:\n",
    "\n",
    "1. Loguru: Best for local development and simple applications\n",
    "2. Loggly: Ideal for cloud-based log aggregation and analysis\n",
    "3. Datadog: Complete solution for large-scale applications needing comprehensive monitoring\n",
    "Choose based on:\n",
    "\n",
    "- Application scale\n",
    "- Budget constraints\n",
    "- Monitoring requirements\n",
    "- Infrastructure complexity\n",
    "- Team size and expertise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
